{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "upEJK8UUDnyn"
   },
   "source": [
    "> ### EEE4423: Deep Learning Lab\n",
    "\n",
    "# LAB \\#12: Sequence to Sequence Network with Attention Module\n",
    "## Machine Translation with Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-21T06:15:45.379511Z",
     "iopub.status.busy": "2022-05-21T06:15:45.379357Z",
     "iopub.status.idle": "2022-05-21T06:15:45.384522Z",
     "shell.execute_reply": "2022-05-21T06:15:45.383851Z",
     "shell.execute_reply.started": "2022-05-21T06:15:45.379491Z"
    },
    "id": "2JP0LD9nDnys",
    "outputId": "0c1a8aae-4843-435b-dbaa-9624d918ff03",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This code is written at 2022-05-21 06:15:45.380377\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "print(\"This code is written at \" + str(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mBvC_gonDnyu"
   },
   "source": [
    "In this project we will be teaching a neural network to translate from\n",
    "French to English.\n",
    "*************************************************************\n",
    "::\n",
    "\n",
    "    [(>): input, (=): target, (<): output]\n",
    "\n",
    "    > il est en train de peindre un tableau .\n",
    "    = he is painting a picture .\n",
    "    < he is painting a picture .\n",
    "\n",
    "    > pourquoi ne pas essayer ce vin delicieux ?\n",
    "    = why not try that delicious wine ?\n",
    "    < why not try that delicious wine ?\n",
    "\n",
    "    > elle n est pas poete mais romanciere .\n",
    "    = she is not a poet but a novelist .\n",
    "    < she not not a poet but a novelist .\n",
    "\n",
    "    > vous etes trop maigre .\n",
    "    = you re too skinny .\n",
    "    < you re all alone .\n",
    "\n",
    "...\n",
    "*************************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-21T06:15:45.385422Z",
     "iopub.status.busy": "2022-05-21T06:15:45.385272Z",
     "iopub.status.idle": "2022-05-21T06:15:46.334405Z",
     "shell.execute_reply": "2022-05-21T06:15:46.333087Z",
     "shell.execute_reply.started": "2022-05-21T06:15:45.385404Z"
    },
    "id": "nLSIcUoZDnyu",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6v0s8XW7Dnyv"
   },
   "source": [
    "### 1. Prepare data\n",
    "\n",
    "The data for this project is a set of many thousands of English to French translation pairs. Download the data from <https://download.pytorch.org/tutorial/data.zip>. The file is a tab separated list of translation pairs:\n",
    "\n",
    "\n",
    "    I am cold.    J'ai froid.\n",
    "    \n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1K3W2RxeTKih5IiT5PcIyWNZSwMqtYSGZ\"  onerror=\"this.style.display='none'\" style=\"width: 600px;\"/><br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-21T06:15:46.336521Z",
     "iopub.status.busy": "2022-05-21T06:15:46.336113Z",
     "iopub.status.idle": "2022-05-21T06:15:49.453542Z",
     "shell.execute_reply": "2022-05-21T06:15:49.452583Z",
     "shell.execute_reply.started": "2022-05-21T06:15:46.336498Z"
    },
    "id": "Xn2soUCGDnyv",
    "outputId": "a76ed812-6017-4ceb-a52c-2fd70a6b1377",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 135842 sentence pairs\n",
      "Trimmed to 10599 sentence pairs\n",
      "Counted words: fra = 4345 eng = 2803\n",
      "['je suis tres contente de te revoir .', 'i m very glad to see you again .']\n"
     ]
    }
   ],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "MAX_LENGTH = 10\n",
    "eng_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s \",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \")\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "            \n",
    "def filterPair(p):\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "        len(p[1].split(' ')) < MAX_LENGTH and \\\n",
    "        p[1].startswith(eng_prefixes)\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]\n",
    "\n",
    "# Turn a Unicode string to plain ASCII, thanks to https://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join( c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s\n",
    "\n",
    "def readLangs(lang1, lang2, reverse=False):\n",
    "    # Read the file and split into lines\n",
    "    lines = open('../dataset/lab12/%s-%s.txt' % (lang1, lang2), encoding='utf-8').\\\n",
    "        read().strip().split('\\n')\n",
    "\n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
    "\n",
    "    # Reverse pairs, make Lang instances\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "def prepareData(lang1, lang2, reverse=False):\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\", input_lang.name, '=', input_lang.n_words, output_lang.name, '=', output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "input_lang, output_lang, pairs = prepareData('eng', 'fra', True)\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-21T06:15:49.454701Z",
     "iopub.status.busy": "2022-05-21T06:15:49.454439Z",
     "iopub.status.idle": "2022-05-21T06:15:49.459448Z",
     "shell.execute_reply": "2022-05-21T06:15:49.458805Z",
     "shell.execute_reply.started": "2022-05-21T06:15:49.454683Z"
    },
    "id": "A8yVC59WDnyw",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m4qXkB3DDnyw"
   },
   "source": [
    "### 2. Build the Seq2Seq model [5 points]\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1kKXrIIxi0t-Nm5HfzOukqjzEp7yEXEpV\"  onerror=\"this.style.display='none'\" /><br><br>\n",
    "\n",
    "[sequence to sequence network](https://arxiv.org/abs/1409.3215) is a model in which two\n",
    "recurrent neural networks work together to transform one sequence to\n",
    "another. An encoder network condenses an input sequence into a single vector,\n",
    "and a decoder network unfolds that vector into a new sequence.\n",
    "\n",
    "Unlike sequence prediction with a single RNN, where every input\n",
    "corresponds to an output, the seq2seq model frees us from sequence\n",
    "length and order, which makes it ideal for translation between two\n",
    "languages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hDTwMr5TDnyy",
    "tags": []
   },
   "source": [
    "#### Encoder\n",
    "The encoder of a seq2seq network is a RNN that outputs some value for every word from the input sentence. For every input word the encoder outputs a vector and a hidden state, and uses the hidden state for the next input word.  \n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1PyKBEVl5jwQfB0I0P2kG8nTGQVZQdZEM\"  onerror=\"this.style.display='none'\" /><br><br>\n",
    "\n",
    "#### GRU\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1467jVFRYbw1DYvVKeSyzGWLRmtlqpy8z\"  onerror=\"this.style.display='none'\" style=\"width: 700px;\"/><br><br>\n",
    "The GRU operates using a reset gate (r) and an update gate (z). The candidate state is created by using the previous hidden state and the current input. It is the reset gate that determines how the previous hidden state affects the candidate state. The newly created candidate state and the previous hidden state create a new hidden state, in which the update gate plays a role in balancing the two.\n",
    "\n",
    "#### LSTM vs GRU\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1lzGTsIYvPWKNF-XaTevMaaZvjfgp9G35\"  onerror=\"this.style.display='none'\" style=\"width: 600px;\"/><br><br>\n",
    "\n",
    "| <center>LSTM</center> | <center>GRU</center>  |\n",
    "|:--------|--------|\n",
    "| LSTM has 3 gates (forget, input, output) | GRU has 2 gates (reset, update) |\n",
    "| There is an internal memory (cell state) | There is no cell state and only hidden state exists |\n",
    "| When making output, another non-linearity is applied | There is no additional non-linearity when making output  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-21T06:15:49.460288Z",
     "iopub.status.busy": "2022-05-21T06:15:49.460165Z",
     "iopub.status.idle": "2022-05-21T06:15:52.895741Z",
     "shell.execute_reply": "2022-05-21T06:15:52.893899Z",
     "shell.execute_reply.started": "2022-05-21T06:15:49.460274Z"
    },
    "id": "PpL3bajyDnyy",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 2 points\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.embedding = nn.Embedding(input_dim, hidden_dim)\n",
    "\n",
    "        # gru\n",
    "        # The size of input is (batch_size, seq_dim, hidden_dim)\n",
    "        #############\n",
    "        # z = sigma(xU + hn_W) -> uz, wz / Sigmoid\n",
    "        # r = sigma(xU + hn_W) -> ur, wr / Sigmoid\n",
    "        # h = tanh(xU + (hn_*r)W) -> uh wh / Tanh\n",
    "        # hn = (1-z)*h + z+hn_\n",
    "        self.uz = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.wz = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.ur = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.wr = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.uh = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.wh = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.tanh = nn.Tanh()\n",
    "        #############\n",
    "\n",
    "    def forward(self, input, hn):\n",
    "        #############\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        x = embedded\n",
    "        z = self.sigmoid(self.uz(x) + self.wz(hn))\n",
    "        r = self.sigmoid(self.ur(x) + self.wr(hn))\n",
    "        h = self.tanh(self.uh(x) + self.wh(hn.mul(r)))\n",
    "        output = h.mul((1-z)) + hn.mul(z)\n",
    "        hn = output\n",
    "        #############\n",
    "        return output, hn\n",
    "\n",
    "    def initHidden(self):\n",
    "        # The size of h0 should be (layer_dim, batch_size, hidden_dim)\n",
    "        #############\n",
    "        h0 = torch.zeros(1, 1, self.hidden_dim).cuda()\n",
    "        #############\n",
    "        return h0\n",
    "    \n",
    "hidden_dim = 256\n",
    "encoder = EncoderRNN(input_lang.n_words, hidden_dim).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lGjPPHD7Dnyy"
   },
   "source": [
    "#### Decoder\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1Rm_LlpEolCvPuzPWEFOZ-zdTfsgMbtu-\"  onerror=\"this.style.display='none'\" /><br><br>\n",
    "\n",
    "If only the context vector is passed betweeen the encoder and decoder, that single vector carries the burden of encoding the entire sentence. Attention allows the decoder network to \"focus\" on a specific part of\n",
    "the encoder's outputs for every step and thus help the decoder choose the right output words. \n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=18hsS8PAA7I3QaN9oOebfnMGAMhR-6EID\"  onerror=\"this.style.display='none'\" style=\"width: 170px;\"/><br><br>\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1F1Y92uLvGaI6s-ygyNKNox4ZGiZmTZ3g\"  onerror=\"this.style.display='none'\" style=\"width: 170px;\"/><br><br>\n",
    "\n",
    "The attention weights are calculated using an another feed-forward layer which inputs the decoder's input and hidden state. And the calculated attention weight is multiplied to the corresponding hidden state of the encoder, respectively. Note that to actually create and train this layer we have to choose a maximum sentence length. Sentences of the maximum length will use all the attention weights, while shorter sentences will only use the first few.\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1JEE23gtJf4XciJUXLt2R9lZtpRn8mYCN\"  onerror=\"this.style.display='none'\" /><br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-21T06:15:52.900465Z",
     "iopub.status.busy": "2022-05-21T06:15:52.900134Z",
     "iopub.status.idle": "2022-05-21T06:15:52.940218Z",
     "shell.execute_reply": "2022-05-21T06:15:52.939295Z",
     "shell.execute_reply.started": "2022-05-21T06:15:52.900427Z"
    },
    "id": "NfWr22rFDnyz",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 3 points\n",
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_dim, output_dim, dropout_p=0.1):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.dropout_p = dropout_p\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_dim, self.hidden_dim)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        \n",
    "        # attention\n",
    "        # Note that the column of the attention weights is MAX_LENGTH\n",
    "        # Note that concatenation is used when \"attn\" and \"attn_combine\" are created\n",
    "        #############\n",
    "        self.attn = nn.Linear(self.hidden_dim * 2, MAX_LENGTH)\n",
    "        self.attn_combine = nn.Linear(self.hidden_dim * 2, self.hidden_dim)\n",
    "        #############\n",
    "        \n",
    "        # gru\n",
    "        # The size of input is (batch_size, seq_dim, hidden_dim)\n",
    "        #############\n",
    "        self.uz = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.wz = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.ur = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.wr = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.uh = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.wh = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.tanh = nn.Tanh()\n",
    "        #############\n",
    "        \n",
    "        self.out = nn.Linear(self.hidden_dim, self.output_dim)\n",
    "\n",
    "    def forward(self, input, hn, encoder_outputs):\n",
    "        # print(input.size(), hn.size(), encoder_outputs.size())\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "        \n",
    "        # attention\n",
    "        # All specifications of the operations are described in the above figure (e.g. use ReLU)\n",
    "        # bmm is a operation which performs a batch matrix-matrix product\n",
    "        #############\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hn[0]), 1)), dim=1)\n",
    "        attn_applied = torch.bmm(\n",
    "            attn_weights.unsqueeze(0), encoder_outputs.unsqueeze(0))\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "        #############\n",
    "        \n",
    "        # gru\n",
    "        #############\n",
    "        x = F.relu(output)\n",
    "        z = self.sigmoid(self.uz(x) + self.wz(hn))\n",
    "        r = self.sigmoid(self.ur(x) + self.wr(hn))\n",
    "        h = self.tanh(self.uh(x) + self.wh(hn.mul(r)))\n",
    "        output = h.mul((1-z)) + hn.mul(z)\n",
    "        #############\n",
    "        \n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        \n",
    "        return output, hn\n",
    "\n",
    "    def initHidden(self):\n",
    "        # The size of h0 should be (layer_dim, batch_size, hidden_dim)\n",
    "        #############\n",
    "        h0 = (torch.zeros(1, 1, self.hidden_dim)).cuda()\n",
    "        #############\n",
    "        return h0\n",
    "    \n",
    "decoder = AttnDecoderRNN(hidden_dim, output_lang.n_words, dropout_p=0.1).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kg2utJ-1Dnyz"
   },
   "source": [
    "### 3. Loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-21T06:15:52.966346Z",
     "iopub.status.busy": "2022-05-21T06:15:52.966116Z",
     "iopub.status.idle": "2022-05-21T06:15:52.970428Z",
     "shell.execute_reply": "2022-05-21T06:15:52.969637Z",
     "shell.execute_reply.started": "2022-05-21T06:15:52.966332Z"
    },
    "id": "KE-npWCODnyz"
   },
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()\n",
    "\n",
    "learning_rate=0.01\n",
    "encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IMr9RcDBDnyz"
   },
   "source": [
    "### 4. Write the evaluation code [2 points]\n",
    "\n",
    "- Using the trained model, display the translated output given input sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-21T06:15:52.991548Z",
     "iopub.status.busy": "2022-05-21T06:15:52.991435Z",
     "iopub.status.idle": "2022-05-21T06:15:52.997865Z",
     "shell.execute_reply": "2022-05-21T06:15:52.997282Z",
     "shell.execute_reply.started": "2022-05-21T06:15:52.991536Z"
    },
    "id": "UO3UzfFCDnyz"
   },
   "outputs": [],
   "source": [
    "def evaluate(sentence):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        \n",
    "        #############\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "        #############\n",
    "        \n",
    "        encoder_outputs = torch.zeros(MAX_LENGTH, encoder.hidden_dim, device=device)\n",
    "        \n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "        decoded_words = []\n",
    "        \n",
    "        #############\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "            encoder_outputs[ei] = encoder_outputs[0, 0]\n",
    "        #############\n",
    "        decoder_hidden = encoder_hidden\n",
    "        \n",
    "        for di in range(output_length):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(output_lang.index2word[topi.item()])\n",
    "            \n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return decoded_words\n",
    "    \n",
    "def evaluateRandomly():\n",
    "    pair = random.choice(pairs)\n",
    "    print('>', pair[0])\n",
    "    print('=', pair[1])\n",
    "    output_words = evaluate(pair[0])\n",
    "    output_sentence = ' '.join(output_words)\n",
    "    print('<', output_sentence)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K7wNENhXDnyz"
   },
   "source": [
    "### 5 . Write the code to train the model [3 points]\n",
    "\n",
    "- During training, use the `Teacher forcing` concept in addition to a naive approach.\n",
    "    - In other words, instead of using the decoder's guess as the next input, the real target outputs are also used sometimes. This shows faster convergence.\n",
    "- Plot the training loss curve.\n",
    "- Show the result using $evaluateRandomly()$ function. Below is an example.\n",
    "*************************************************************\n",
    "    > il est en train de peindre un tableau . (input)\n",
    "    = he is painting a picture . (target)\n",
    "    < he is painting a picture . (output)\n",
    "*************************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-21T06:15:53.018384Z",
     "iopub.status.busy": "2022-05-21T06:15:53.018154Z",
     "iopub.status.idle": "2022-05-21T06:49:22.636468Z",
     "shell.execute_reply": "2022-05-21T06:49:22.635568Z",
     "shell.execute_reply.started": "2022-05-21T06:15:53.018367Z"
    },
    "id": "9x83bK3GDnyz",
    "outputId": "34533830-f22b-4080-bc51-582d4993b8e2",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************* iter1000 *************************\n",
      "loss 25.9656\n",
      "> tu es deprime n est ce pas ?\n",
      "= you re depressed aren t you ?\n",
      "< you re not . <EOS>\n",
      "\n",
      "************************* iter2000 *************************\n",
      "loss 30.1123\n",
      "> vous etes tirees d affaire .\n",
      "= you re off the hook .\n",
      "< you re very . <EOS>\n",
      "\n",
      "************************* iter3000 *************************\n",
      "loss 13.9155\n",
      "> vous etes un genie .\n",
      "= you are a genius .\n",
      "< you re very . <EOS>\n",
      "\n",
      "************************* iter4000 *************************\n",
      "loss 12.1639\n",
      "> vous etes en premier .\n",
      "= you re first .\n",
      "< you re all . <EOS>\n",
      "\n",
      "************************* iter5000 *************************\n",
      "loss 11.8591\n",
      "> je suis trop fatigue pour conduire .\n",
      "= i m too tired to drive .\n",
      "< i m not tired to .\n",
      "\n",
      "************************* iter6000 *************************\n",
      "loss 16.2879\n",
      "> ils jouent aux echecs .\n",
      "= they are playing chess .\n",
      "< they re going to . .\n",
      "\n",
      "************************* iter7000 *************************\n",
      "loss 15.9178\n",
      "> je vais y songer .\n",
      "= i m going to sleep on it .\n",
      "< i m going to . <EOS>\n",
      "\n",
      "************************* iter8000 *************************\n",
      "loss 26.6692\n",
      "> je suis terriblement fatiguee .\n",
      "= i m awfully tired .\n",
      "< i m still . <EOS>\n",
      "\n",
      "************************* iter9000 *************************\n",
      "loss 10.3784\n",
      "> nous faisons la diete .\n",
      "= we re fasting .\n",
      "< we re going . <EOS>\n",
      "\n",
      "************************* iter10000 *************************\n",
      "loss 8.9022\n",
      "> je suis ici pour affaires .\n",
      "= i am here on business .\n",
      "< i m here for <EOS>\n",
      "\n",
      "************************* iter11000 *************************\n",
      "loss 13.7725\n",
      "> elle trouve toujours a redire aux autres .\n",
      "= she is always finding fault with other people .\n",
      "< she is always in to . <EOS>\n",
      "\n",
      "************************* iter12000 *************************\n",
      "loss 8.3448\n",
      "> ils ne vont pas aider tom .\n",
      "= they aren t going to help tom .\n",
      "< they re not going to\n",
      "\n",
      "************************* iter13000 *************************\n",
      "loss 2.9684\n",
      "> vous etes la meilleure .\n",
      "= you re the greatest .\n",
      "< you re the . <EOS>\n",
      "\n",
      "************************* iter14000 *************************\n",
      "loss 12.9631\n",
      "> elles sont juste pour toi .\n",
      "= they re just for you .\n",
      "< they re just for you re just for\n",
      "\n",
      "************************* iter15000 *************************\n",
      "loss 18.4646\n",
      "> ce sont des animaux .\n",
      "= they re animals .\n",
      "< they re out . <EOS>\n",
      "\n",
      "************************* iter16000 *************************\n",
      "loss 22.9032\n",
      "> nous sommes desormais des gens completement differents .\n",
      "= we re totally different people now .\n",
      "< we re looking . <EOS>\n",
      "\n",
      "************************* iter17000 *************************\n",
      "loss 12.1714\n",
      "> c est quelqu un de bien .\n",
      "= he s a good person .\n",
      "< he is a <EOS>\n",
      "\n",
      "************************* iter18000 *************************\n",
      "loss 6.7292\n",
      "> je suis degoute et decu .\n",
      "= i m disgusted and disappointed .\n",
      "< i am a and <EOS>\n",
      "\n",
      "************************* iter19000 *************************\n",
      "loss 9.5359\n",
      "> il est prevu que je dejeune avec lui .\n",
      "= i m scheduled to have lunch with him .\n",
      "< he is <EOS>\n",
      "\n",
      "************************* iter20000 *************************\n",
      "loss 18.5447\n",
      "> c est mon frere pas mon pere .\n",
      "= he is my brother not father .\n",
      "< he is my father my father my father my father\n",
      "\n",
      "************************* iter21000 *************************\n",
      "loss 8.6200\n",
      "> il fait une tete de plus que moi .\n",
      "= he is a head taller than me .\n",
      "< he s just a <EOS>\n",
      "\n",
      "************************* iter22000 *************************\n",
      "loss 15.2402\n",
      "> il est malade .\n",
      "= he is sick .\n",
      "< he s sick . <EOS>\n",
      "\n",
      "************************* iter23000 *************************\n",
      "loss 10.1228\n",
      "> nous nous retirons .\n",
      "= we re pulling out .\n",
      "< we re getting . . .\n",
      "\n",
      "************************* iter24000 *************************\n",
      "loss 24.4855\n",
      "> nous ne sommes pas tous enseignants .\n",
      "= we re not all teachers .\n",
      "< we re not all . . .\n",
      "\n",
      "************************* iter25000 *************************\n",
      "loss 3.5171\n",
      "> nous sommes toutes pretes .\n",
      "= we are all set .\n",
      "< we re all . <EOS>\n",
      "\n",
      "************************* iter26000 *************************\n",
      "loss 7.1223\n",
      "> tu me mens .\n",
      "= you re lying to me .\n",
      "< you re the . <EOS>\n",
      "\n",
      "************************* iter27000 *************************\n",
      "loss 7.2082\n",
      "> vous n etes pas facile a trouver .\n",
      "= you re not easy to find .\n",
      "< you re not supposed to . <EOS>\n",
      "\n",
      "************************* iter28000 *************************\n",
      "loss 39.9939\n",
      "> je ne suis pas un locuteur natif .\n",
      "= i m not a native speaker .\n",
      "< i m not a <EOS>\n",
      "\n",
      "************************* iter29000 *************************\n",
      "loss 4.9805\n",
      "> je suis certaine d avoir ferme le gaz .\n",
      "= i m sure i turned off the gas .\n",
      "< i m sure <EOS>\n",
      "\n",
      "************************* iter30000 *************************\n",
      "loss 2.0184\n",
      "> je me sens confuse .\n",
      "= i m feeling confused .\n",
      "< i m feeling . . .\n",
      "\n",
      "************************* iter31000 *************************\n",
      "loss 2.0025\n",
      "> tu es fascinante .\n",
      "= you re fascinating .\n",
      "< you re fascinating . <EOS>\n",
      "\n",
      "************************* iter32000 *************************\n",
      "loss 6.1740\n",
      "> il s amuse en jouant aux jeux videos .\n",
      "= he is amusing himself by playing video games .\n",
      "< he is playing at . . .\n",
      "\n",
      "************************* iter33000 *************************\n",
      "loss 14.4207\n",
      "> vous etes fort attirants .\n",
      "= you re very attractive .\n",
      "< you re very attractive . <EOS>\n",
      "\n",
      "************************* iter34000 *************************\n",
      "loss 1.9059\n",
      "> je suis au restaurant .\n",
      "= i m at the restaurant .\n",
      "< i m at the . <EOS>\n",
      "\n",
      "************************* iter35000 *************************\n",
      "loss 11.7045\n",
      "> je suis ton amie .\n",
      "= i m your friend .\n",
      "< i m your friend . <EOS>\n",
      "\n",
      "************************* iter36000 *************************\n",
      "loss 5.4777\n",
      "> je n en ai pas termine .\n",
      "= i m not done .\n",
      "< i m not done .\n",
      "\n",
      "************************* iter37000 *************************\n",
      "loss 2.8737\n",
      "> vous etes celui qui a plante cet arbre .\n",
      "= you re the one who planted that tree .\n",
      "< you re the one who planted . <EOS>\n",
      "\n",
      "************************* iter38000 *************************\n",
      "loss 6.0973\n",
      "> tu gaspilles du temps .\n",
      "= you re wasting time .\n",
      "< you re wasting time time time\n",
      "\n",
      "************************* iter39000 *************************\n",
      "loss 14.1797\n",
      "> desole .\n",
      "= i m sorry .\n",
      "< i m sorry . <EOS>\n",
      "\n",
      "************************* iter40000 *************************\n",
      "loss 2.9999\n",
      "> je perds patience avec vous .\n",
      "= i am losing my patience with you .\n",
      "< i am on you with\n",
      "\n",
      "************************* iter41000 *************************\n",
      "loss 10.7980\n",
      "> elle n est pas confiante en l avenir .\n",
      "= she s not confident about the future .\n",
      "< she s not about about about about about\n",
      "\n",
      "************************* iter42000 *************************\n",
      "loss 14.9113\n",
      "> elle attend un heureux evenement .\n",
      "= she s pregnant .\n",
      "< she s s s s s s s s\n",
      "\n",
      "************************* iter43000 *************************\n",
      "loss 0.5803\n",
      "> on nous fait chanter .\n",
      "= we re being blackmailed .\n",
      "< we re being . .\n",
      "\n",
      "************************* iter44000 *************************\n",
      "loss 1.3592\n",
      "> tu es fort attirante .\n",
      "= you re very attractive .\n",
      "< you re very attractive . <EOS>\n",
      "\n",
      "************************* iter45000 *************************\n",
      "loss 2.0334\n",
      "> je ne suis pas en train de rever .\n",
      "= i m not dreaming .\n",
      "< i m not . <EOS>\n",
      "\n",
      "************************* iter46000 *************************\n",
      "loss 4.2500\n",
      "> nous sommes freres .\n",
      "= we re brothers .\n",
      "< we are brothers . <EOS>\n",
      "\n",
      "************************* iter47000 *************************\n",
      "loss 13.5401\n",
      "> je vais epargner plus d argent .\n",
      "= i m going to save more money .\n",
      "< i m going to need . <EOS>\n",
      "\n",
      "************************* iter48000 *************************\n",
      "loss 3.4231\n",
      "> je suis desolee d avoir rate ton anniversaire .\n",
      "= i m sorry i missed your birthday .\n",
      "< i m sorry i m\n",
      "\n",
      "************************* iter49000 *************************\n",
      "loss 16.1067\n",
      "> il est mon meilleur ami .\n",
      "= he is my best friend .\n",
      "< he s my best friend . <EOS>\n",
      "\n",
      "************************* iter50000 *************************\n",
      "loss 14.2039\n",
      "> tu es voyant .\n",
      "= you re psychic .\n",
      "< you re psychic . <EOS>\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f661b5ceb50>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_iters = 50000\n",
    "print_every = 1000\n",
    "plot_every =100\n",
    "\n",
    "plot_losses = []\n",
    "print_loss_total = 0  # Reset every print_every\n",
    "plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "training_pairs = [tensorsFromPair(random.choice(pairs)) for i in range(n_iters)]\n",
    "\n",
    "for iter in range(1, n_iters+1):\n",
    "    # Load data\n",
    "    training_pair = training_pairs[iter-1]\n",
    "    input_tensor = training_pair[0]\n",
    "    target_tensor = training_pair[1]\n",
    "    \n",
    "    # Clear gradients w.r.t. parameters\n",
    "    #############\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    #############\n",
    "    \n",
    "    # Forward pass\n",
    "    #############\n",
    "    loss = 0\n",
    "    input_length = input_tensor.size(0)\n",
    "    output_length = target_tensor.size(0)\n",
    "    \n",
    "    target_length = target_tensor.size(0)\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "    encoder_outputs = torch.zeros(MAX_LENGTH, encoder.hidden_dim, device=device)\n",
    "    #############\n",
    "    \n",
    "    #############\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_outputs[0,0]\n",
    "    #############\n",
    "    decoder_hidden = encoder_hidden\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "    \n",
    "    #############\n",
    "    for di in range(output_length):\n",
    "        decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "        loss += criterion(decoder_output, target_tensor[di])\n",
    "        topv, topi = decoder_output.topk(1)\n",
    "        decoder_input = topi.squeeze().detach()\n",
    "        \n",
    "        if decoder_input.item() == EOS_token:\n",
    "            break\n",
    "    #############\n",
    "\n",
    "    # Backward pass\n",
    "    #############\n",
    "    loss.backward()\n",
    "    #############\n",
    "\n",
    "    # Updating parameters\n",
    "    #############\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    #############\n",
    "    \n",
    "    print_loss_total += loss.item() / target_length\n",
    "    plot_loss_total += loss.item() / target_length\n",
    "\n",
    "    if iter % print_every == 0:\n",
    "        print('*'*25, 'iter%d'%iter, '*'*25)\n",
    "        print('loss %.4f'%loss)\n",
    "        print_loss_avg = print_loss_total / print_every\n",
    "        print_loss_total = 0\n",
    "        evaluateRandomly()\n",
    "\n",
    "    if iter % plot_every == 0:\n",
    "        plot_loss_avg = plot_loss_total / plot_every\n",
    "        plot_losses.append(plot_loss_avg)\n",
    "        plot_loss_total = 0\n",
    "\n",
    "#################################################\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(plot_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rrccVZ1gDny0",
    "tags": []
   },
   "source": [
    "### *References*\n",
    "[1] [practical pytorch](https://github.com/spro/practical-pytorch)(https://github.com/spro/practical-pytorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "불어와 영어의 번역 작업을 LSTM의 변형인 GRU로 RNN을 구성하여 Seq2Seq model에 기반하여 encoder와 decoder를 만들었다.\n",
    "또한 Seq2Seq의 고정된 크기의 contex vector에 의한 정보 손실을 막기 위해 attention mechanism을 사용하여 model을 완성하였다.\n",
    "문장의 단어가 차례로 encoder로 들어가 hidden state를 만들어내고, 이 hidden state에 weight를 주어 decoder에서 번역 작업을 수행한다.\n",
    "decoder의 처음 input은 SOS_token을 사용하여 tranlation을 진행하며 EOS_token이 출력으로 되었을 때, 번역작업을 종료한다.\n",
    "\n",
    "49000 번째 epoch에서 프랑스어 \"il est mon meilleur ami\"를 넣었을 때, 실제 구글 번역은 \"he is my best friend\"로 예측값이 정확한 것을 확인할 수 있다.\n",
    "Seq2Seq, attention의 몇 년전 모델도 현재의 번역과 동일하게 높은 정확성을 보여주어 현재의 기계번역이 어떻게 사용될 수 있을지 기대가된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "EEE4423_lab11_Seq2Seq.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "torch1.9.0-py3.8-cuda11.1",
   "language": "python",
   "name": "torch1.9.0-py3.8-cuda11.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
