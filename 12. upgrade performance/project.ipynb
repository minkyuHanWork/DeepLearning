{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be564ffe-7c6a-4983-a927-c53f79fe530b",
   "metadata": {
    "id": "be564ffe-7c6a-4983-a927-c53f79fe530b"
   },
   "source": [
    "#### > ### EEE4423: Deep Learning Lab\n",
    "\n",
    "# Final Project: Long-tail Visual Recognition for Image Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93d88e6d-f649-4fbd-a02c-c6116f64055c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-22T15:29:49.165338Z",
     "iopub.status.busy": "2022-06-22T15:29:49.165126Z",
     "iopub.status.idle": "2022-06-22T15:29:51.320289Z",
     "shell.execute_reply": "2022-06-22T15:29:51.318874Z",
     "shell.execute_reply.started": "2022-06-22T15:29:49.165303Z"
    },
    "id": "93d88e6d-f649-4fbd-a02c-c6116f64055c",
    "outputId": "940208e1-9408-4062-be96-8c8cd923558c",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /home/jovyan_venv/.venv/torch1.9.0-py3.8-cuda11.1/lib/python3.8/site-packages (22.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ce94ced-8782-4566-9672-05bf3cb2bd87",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-22T15:29:51.322773Z",
     "iopub.status.busy": "2022-06-22T15:29:51.322374Z",
     "iopub.status.idle": "2022-06-22T15:29:53.342431Z",
     "shell.execute_reply": "2022-06-22T15:29:53.341020Z",
     "shell.execute_reply.started": "2022-06-22T15:29:51.322719Z"
    },
    "id": "9ce94ced-8782-4566-9672-05bf3cb2bd87",
    "outputId": "58123a26-c832-440a-b744-8b76afe64e3d",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: optuna in /home/jovyan_venv/.venv/torch1.9.0-py3.8-cuda11.1/lib/python3.8/site-packages (2.10.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from optuna) (21.3)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from optuna) (1.22.2)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.8/dist-packages (from optuna) (6.0)\n",
      "Requirement already satisfied: tqdm in /home/jovyan_venv/.venv/torch1.9.0-py3.8-cuda11.1/lib/python3.8/site-packages (from optuna) (4.62.3)\n",
      "Requirement already satisfied: scipy!=1.4.0 in /usr/local/lib/python3.8/dist-packages (from optuna) (1.8.0)\n",
      "Requirement already satisfied: cliff in /home/jovyan_venv/.venv/torch1.9.0-py3.8-cuda11.1/lib/python3.8/site-packages (from optuna) (3.10.1)\n",
      "Requirement already satisfied: colorlog in /home/jovyan_venv/.venv/torch1.9.0-py3.8-cuda11.1/lib/python3.8/site-packages (from optuna) (6.6.0)\n",
      "Requirement already satisfied: alembic in /home/jovyan_venv/.venv/torch1.9.0-py3.8-cuda11.1/lib/python3.8/site-packages (from optuna) (1.8.0)\n",
      "Requirement already satisfied: sqlalchemy>=1.1.0 in /home/jovyan_venv/.venv/torch1.9.0-py3.8-cuda11.1/lib/python3.8/site-packages (from optuna) (1.4.37)\n",
      "Requirement already satisfied: cmaes>=0.8.2 in /home/jovyan_venv/.venv/torch1.9.0-py3.8-cuda11.1/lib/python3.8/site-packages (from optuna) (0.8.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->optuna) (3.0.7)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/jovyan_venv/.venv/torch1.9.0-py3.8-cuda11.1/lib/python3.8/site-packages (from sqlalchemy>=1.1.0->optuna) (1.1.2)\n",
      "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.8/dist-packages (from alembic->optuna) (5.4.0)\n",
      "Requirement already satisfied: importlib-metadata in /home/jovyan_venv/.venv/torch1.9.0-py3.8-cuda11.1/lib/python3.8/site-packages (from alembic->optuna) (4.10.1)\n",
      "Requirement already satisfied: Mako in /home/jovyan_venv/.venv/torch1.9.0-py3.8-cuda11.1/lib/python3.8/site-packages (from alembic->optuna) (1.2.0)\n",
      "Requirement already satisfied: PrettyTable>=0.7.2 in /home/jovyan_venv/.venv/torch1.9.0-py3.8-cuda11.1/lib/python3.8/site-packages (from cliff->optuna) (3.3.0)\n",
      "Requirement already satisfied: autopage>=0.4.0 in /home/jovyan_venv/.venv/torch1.9.0-py3.8-cuda11.1/lib/python3.8/site-packages (from cliff->optuna) (0.5.1)\n",
      "Requirement already satisfied: cmd2>=1.0.0 in /home/jovyan_venv/.venv/torch1.9.0-py3.8-cuda11.1/lib/python3.8/site-packages (from cliff->optuna) (2.4.1)\n",
      "Requirement already satisfied: stevedore>=2.0.1 in /home/jovyan_venv/.venv/torch1.9.0-py3.8-cuda11.1/lib/python3.8/site-packages (from cliff->optuna) (3.5.0)\n",
      "Requirement already satisfied: pbr!=2.1.0,>=2.0.0 in /home/jovyan_venv/.venv/torch1.9.0-py3.8-cuda11.1/lib/python3.8/site-packages (from cliff->optuna) (5.9.0)\n",
      "Requirement already satisfied: wcwidth>=0.1.7 in /usr/local/lib/python3.8/dist-packages (from cmd2>=1.0.0->cliff->optuna) (0.2.5)\n",
      "Requirement already satisfied: attrs>=16.3.0 in /usr/local/lib/python3.8/dist-packages (from cmd2>=1.0.0->cliff->optuna) (21.4.0)\n",
      "Requirement already satisfied: pyperclip>=1.6 in /home/jovyan_venv/.venv/torch1.9.0-py3.8-cuda11.1/lib/python3.8/site-packages (from cmd2>=1.0.0->cliff->optuna) (1.8.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata->alembic->optuna) (3.7.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.8/dist-packages (from Mako->alembic->optuna) (2.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0d7d3b2-32a4-4808-9476-edc89d829705",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-22T15:29:53.378580Z",
     "iopub.status.busy": "2022-06-22T15:29:53.378364Z",
     "iopub.status.idle": "2022-06-22T15:29:55.978673Z",
     "shell.execute_reply": "2022-06-22T15:29:55.977385Z",
     "shell.execute_reply.started": "2022-06-22T15:29:53.378553Z"
    },
    "id": "a0d7d3b2-32a4-4808-9476-edc89d829705",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import optuna\n",
    "import joblib\n",
    "\n",
    "from misc.project.utils import resnet18, IMBALANCECIFAR10, IMBALANCECIFAR100, compute_accuracy, IMBALANCECIFAR10_V, IMBALANCECIFAR100_V\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c84926a-baea-4225-b4fe-0110aa07f0d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-22T15:29:55.990618Z",
     "iopub.status.busy": "2022-06-22T15:29:55.990296Z",
     "iopub.status.idle": "2022-06-22T15:29:55.998286Z",
     "shell.execute_reply": "2022-06-22T15:29:55.997270Z",
     "shell.execute_reply.started": "2022-06-22T15:29:55.990580Z"
    },
    "id": "5c84926a-baea-4225-b4fe-0110aa07f0d5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f062b33-7707-4f49-b7a1-9fabb5074aa8",
   "metadata": {
    "id": "2f062b33-7707-4f49-b7a1-9fabb5074aa8"
   },
   "source": [
    "# Find hyperparameter \"Weight Decay\"\n",
    "#### (Simple example)\n",
    "weight decay and accuracy are stored in a file 'WD'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "915a69f6-8d69-410b-a8be-8ffabc489777",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-22T15:30:25.841613Z",
     "iopub.status.busy": "2022-06-22T15:30:25.841128Z",
     "iopub.status.idle": "2022-06-22T15:30:25.848348Z",
     "shell.execute_reply": "2022-06-22T15:30:25.846986Z",
     "shell.execute_reply.started": "2022-06-22T15:30:25.841562Z"
    },
    "id": "915a69f6-8d69-410b-a8be-8ffabc489777",
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATASET = 'CIFAR10' #['CIFAR10', 'CIFAR100']\n",
    "IMB_TYPE = 'exp' #['exp', 'step']\n",
    "IMB_FACTOR = 0.01 #[0.1, 0.01]\n",
    "\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c68d2233-921f-43d6-bb1f-8bd119c4b491",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-22T15:30:27.591461Z",
     "iopub.status.busy": "2022-06-22T15:30:27.590889Z",
     "iopub.status.idle": "2022-06-22T15:30:27.602865Z",
     "shell.execute_reply": "2022-06-22T15:30:27.601752Z",
     "shell.execute_reply.started": "2022-06-22T15:30:27.591407Z"
    },
    "id": "c68d2233-921f-43d6-bb1f-8bd119c4b491",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# num_cls = 10, 100 (CIFAR10, CIFAR100)\n",
    "# val_num_each_cls = 1000, 100 (CIFAR10, CIFAR100)\n",
    "# samples_of_each_cls = 5000, 500 (CIFAR10, CIFAR100)\n",
    "\n",
    "def for_split_set(num_cls, val_num_each_cls, samples_of_each_cls):\n",
    "    val_idx_list = []\n",
    "    for i in range(num_cls):\n",
    "        for j in range(val_num_each_cls):\n",
    "            index = np.random.randint(i*0, (i+1)*samples_of_each_cls) # 0~4999, ..., 45000~49999(0~499, ..., 49500, 49999)의 숫자를 랜덤으로 하나 뽑는다.\n",
    "            while index in val_idx_list: # index가 이미 리스트에 있다면 다시 뽑는다.\n",
    "                index = np.random.randint(i*0, (i+1)*samples_of_each_cls)\n",
    "            val_idx_list.append(index)\n",
    "\n",
    "    train_idx = np.arange(50000)\n",
    "    train_idx_list = list(np.delete(train_idx, val_idx_list))\n",
    "\n",
    "    # Checking duplication\n",
    "    for i in val_idx_list:\n",
    "        if i in train_idx_list:\n",
    "            print(\"중복이 있습니다!\")\n",
    "    \n",
    "    return val_idx_list, train_idx_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01e4ce84-a317-4856-9bcc-fe80a16c33e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-22T15:30:30.590721Z",
     "iopub.status.busy": "2022-06-22T15:30:30.590251Z",
     "iopub.status.idle": "2022-06-22T15:30:30.606686Z",
     "shell.execute_reply": "2022-06-22T15:30:30.605522Z",
     "shell.execute_reply.started": "2022-06-22T15:30:30.590671Z"
    },
    "id": "01e4ce84-a317-4856-9bcc-fe80a16c33e5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_loaders():\n",
    "    if DATASET == 'CIFAR10':\n",
    "        val_idx, train_idx = for_split_set(10, 1000, 5000)\n",
    "        train_dataset = IMBALANCECIFAR10_V(root='../dataset/project', imb_type=IMB_TYPE, imb_factor=IMB_FACTOR, sub_idx=val_idx, train=True, download=True, transform=transform_train)\n",
    "        val_dataset = IMBALANCECIFAR10_V(root='../dataset/project', imb_factor=1, train=True, download=True, sub_idx=train_idx, transform=transform_test)\n",
    "    elif DATASET == 'CIFAR100':\n",
    "        val_idx, train_idx = for_split_set(100, 100, 500)\n",
    "        train_dataset = IMBALANCECIFAR100_V(root='../dataset/project', imb_type=IMB_TYPE, imb_factor=IMB_FACTOR, sub_idx=val_idx, train=True, download=True, transform=transform_train)\n",
    "        val_dataset = IMBALANCECIFAR100_V(root='../dataset/project', imb_factor=1, train=True, download=True, sub_idx=train_idx, transform=transform_test)\n",
    "\n",
    "    cls_num_list = train_dataset.get_cls_num_list()\n",
    "    print('cls num list(train_dataset):')\n",
    "    print(cls_num_list)\n",
    "\n",
    "    cls_num_list = val_dataset.get_cls_num_list()\n",
    "    print('cls num list(val_dataset):')\n",
    "    print(cls_num_list)\n",
    "    num_classes = len(cls_num_list)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
    "        num_workers=4, drop_last=True)\n",
    "\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset, batch_size=100, shuffle=False,\n",
    "        num_workers=4, )\n",
    "\n",
    "    return train_loader, val_loader, num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c7bd41a-75f3-4f71-ae1c-b7f778f6df50",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-22T15:30:31.617621Z",
     "iopub.status.busy": "2022-06-22T15:30:31.617165Z",
     "iopub.status.idle": "2022-06-22T15:30:31.627772Z",
     "shell.execute_reply": "2022-06-22T15:30:31.626416Z",
     "shell.execute_reply.started": "2022-06-22T15:30:31.617570Z"
    },
    "id": "2c7bd41a-75f3-4f71-ae1c-b7f778f6df50",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ResNet18(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ResNet18, self).__init__()\n",
    "        self.backbone = resnet18()\n",
    "        self.classifier = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        x = self.backbone(x)\n",
    "        x = F.adaptive_max_pool2d(x, 1)\n",
    "        x = x.view(batch_size, -1)\n",
    "        pred = self.classifier(x)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8d2a668f-dbd3-4fc2-aa66-66307a52ac23",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-22T15:29:40.879181Z",
     "iopub.status.busy": "2022-06-22T15:29:40.878708Z",
     "iopub.status.idle": "2022-06-22T15:29:40.892870Z",
     "shell.execute_reply": "2022-06-22T15:29:40.891569Z",
     "shell.execute_reply.started": "2022-06-22T15:29:40.879129Z"
    },
    "id": "8d2a668f-dbd3-4fc2-aa66-66307a52ac23",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(EPOCHS, model, train_loader, criterion, optimizer, scheduler):\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        loss_history = []\n",
    "        model.train()\n",
    "        for batch_index, data in enumerate(train_loader):\n",
    "            image, target = data\n",
    "            image, target = image.cuda(), target.cuda()\n",
    "\n",
    "            pred = model(image)\n",
    "            loss = criterion(pred, target)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_history.append(loss.item())\n",
    "\n",
    "        topk_acc, head_acc, tail_acc = compute_accuracy(train_loader, model)\n",
    "        loss_mean = np.mean(loss_history)\n",
    "        scheduler.step()\n",
    "\n",
    "        print('Epoch: [{:03d}] \\t Loss {:.4f} \\t Acc {:.2f} \\t AccHead {:.2f} \\t AccTail {:.2f}'.format(epoch+1, loss_mean, topk_acc[0], head_acc[0], tail_acc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31560645-1e26-40fa-b758-b37824df6acb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-18T12:24:51.859660Z",
     "iopub.status.busy": "2022-06-18T12:24:51.859433Z",
     "iopub.status.idle": "2022-06-18T12:24:51.863400Z",
     "shell.execute_reply": "2022-06-18T12:24:51.862834Z",
     "shell.execute_reply.started": "2022-06-18T12:24:51.859645Z"
    },
    "id": "31560645-1e26-40fa-b758-b37824df6acb",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test(model, test_loader):\n",
    "    model.eval()\n",
    "    topk_acc, head_acc, tail_acc = compute_accuracy(test_loader, model)\n",
    "    print('Acc {:.2f} \\t AccHead {:.2f} \\t AccTail {:.2f}'.format(topk_acc[0], head_acc[0], tail_acc[0]))\n",
    "\n",
    "    accuracy = topk_acc[0]\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0d539e-d7a8-40b0-98cc-02281efb3443",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-18T12:24:51.864234Z",
     "iopub.status.busy": "2022-06-18T12:24:51.864042Z",
     "iopub.status.idle": "2022-06-18T12:25:30.698549Z",
     "shell.execute_reply": "2022-06-18T12:25:30.697807Z",
     "shell.execute_reply.started": "2022-06-18T12:24:51.864219Z"
    },
    "id": "ff0d539e-d7a8-40b0-98cc-02281efb3443",
    "outputId": "40c01dad-1848-46fd-b76a-99a89efb0ade",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "cls num list(train_dataset):\n",
      "[4000, 2397, 1437, 861, 516, 309, 185, 111, 66, 40]\n",
      "cls num list(val_dataset):\n",
      "[1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000]\n"
     ]
    }
   ],
   "source": [
    "LR = 0.1\n",
    "MOMENTUM = 0.9\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "torch.manual_seed(0)\n",
    "train_loader, test_loader, num_classes = get_loaders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad323a59-ccd1-459d-b206-d5773d086de7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-18T12:25:30.804206Z",
     "iopub.status.busy": "2022-06-18T12:25:30.803987Z",
     "iopub.status.idle": "2022-06-18T12:25:30.808615Z",
     "shell.execute_reply": "2022-06-18T12:25:30.808062Z",
     "shell.execute_reply.started": "2022-06-18T12:25:30.804193Z"
    },
    "id": "ad323a59-ccd1-459d-b206-d5773d086de7",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model(trial):\n",
    "    cfg = {\n",
    "        'n_epoch': trial.suggest_categorical('n_epoch', [1,2,3]), # In the actual experiment, proceed to 90, 200\n",
    "        'weight_decay': trial.suggest_loguniform('weight_decay', 1e-5, 1e-1),\n",
    "    }\n",
    "\n",
    "    model = ResNet18(num_classes)\n",
    "    model = model.cuda()\n",
    "\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=LR, momentum=MOMENTUM, weight_decay=cfg['weight_decay'])\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=150, gamma=0.1)\n",
    "\n",
    "    train(cfg['n_epoch'], model, train_loader, criterion, optimizer, scheduler)\n",
    "    test_accuracy = test(model, test_loader)\n",
    "    \n",
    "    return test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0d60a0-f3fd-4f09-9a7e-22c068f8a1ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-18T12:27:17.956570Z",
     "iopub.status.busy": "2022-06-18T12:27:17.956223Z",
     "iopub.status.idle": "2022-06-18T12:28:27.038247Z",
     "shell.execute_reply": "2022-06-18T12:28:27.036885Z",
     "shell.execute_reply.started": "2022-06-18T12:27:17.956541Z"
    },
    "id": "0c0d60a0-f3fd-4f09-9a7e-22c068f8a1ab",
    "outputId": "fe95c2db-6223-4419-f536-d34879d6704f",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-06-18 12:27:17,959]\u001b[0m A new study created in memory with name: no-name-d1495dd3-a601-4aaf-8d05-a36bd83cb572\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [001] \t Loss 2.4846 \t Acc 37.19 \t AccHead 40.07 \t AccTail 0.00\n",
      "Epoch: [002] \t Loss 1.5100 \t Acc 50.69 \t AccHead 54.62 \t AccTail 0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-06-18 12:27:35,932]\u001b[0m Trial 0 finished with value: 19.018281936645508 and parameters: {'n_epoch': 2, 'weight_decay': 0.003762899404711573}. Best is trial 0 with value: 19.018281936645508.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc 19.02 \t AccHead 37.66 \t AccTail 0.00\n",
      "Epoch: [001] \t Loss 2.7323 \t Acc 39.62 \t AccHead 42.68 \t AccTail 0.00\n",
      "Epoch: [002] \t Loss 1.5564 \t Acc 54.60 \t AccHead 58.82 \t AccTail 0.00\n",
      "Epoch: [003] \t Loss 1.7739 \t Acc 56.29 \t AccHead 60.62 \t AccTail 0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-06-18 12:28:01,464]\u001b[0m Trial 1 finished with value: 22.593677520751953 and parameters: {'n_epoch': 3, 'weight_decay': 1.3695730368201831e-05}. Best is trial 1 with value: 22.593677520751953.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc 22.59 \t AccHead 44.74 \t AccTail 0.00\n",
      "Epoch: [001] \t Loss 2.4117 \t Acc 40.27 \t AccHead 43.38 \t AccTail 0.00\n",
      "Epoch: [002] \t Loss 1.3575 \t Acc 46.27 \t AccHead 49.83 \t AccTail 0.00\n",
      "Epoch: [003] \t Loss 1.3184 \t Acc 53.95 \t AccHead 58.11 \t AccTail 0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-06-18 12:28:27,023]\u001b[0m Trial 2 finished with value: 17.654783248901367 and parameters: {'n_epoch': 3, 'weight_decay': 0.030911012891264054}. Best is trial 1 with value: 22.593677520751953.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc 17.65 \t AccHead 34.96 \t AccTail 0.00\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    sampler = optuna.samplers.TPESampler()\n",
    "    study = optuna.create_study(sampler=sampler, direction='maximize')\n",
    "    study.optimize(func=train_model, n_trials=3) # 실제 실험에서는 10 or 12로 진행\n",
    "    joblib.dump(study, 'test_sampling.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e351fa36-4122-4a38-9ca1-35338720a3ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-18T12:28:27.065855Z",
     "iopub.status.busy": "2022-06-18T12:28:27.065611Z",
     "iopub.status.idle": "2022-06-18T12:28:27.083379Z",
     "shell.execute_reply": "2022-06-18T12:28:27.082802Z",
     "shell.execute_reply.started": "2022-06-18T12:28:27.065833Z"
    },
    "id": "e351fa36-4122-4a38-9ca1-35338720a3ba",
    "outputId": "94b80d5f-226a-4611-fb18-42ea8122a672",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number</th>\n",
       "      <th>value</th>\n",
       "      <th>duration</th>\n",
       "      <th>params_n_epoch</th>\n",
       "      <th>params_weight_decay</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>19.018282</td>\n",
       "      <td>0 days 00:00:17.970055</td>\n",
       "      <td>2</td>\n",
       "      <td>0.003763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>22.593678</td>\n",
       "      <td>0 days 00:00:25.529832</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>17.654783</td>\n",
       "      <td>0 days 00:00:25.556325</td>\n",
       "      <td>3</td>\n",
       "      <td>0.030911</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   number      value               duration  params_n_epoch  \\\n",
       "0       0  19.018282 0 days 00:00:17.970055               2   \n",
       "1       1  22.593678 0 days 00:00:25.529832               3   \n",
       "2       2  17.654783 0 days 00:00:25.556325               3   \n",
       "\n",
       "   params_weight_decay  \n",
       "0             0.003763  \n",
       "1             0.000014  \n",
       "2             0.030911  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study = joblib.load('test_sampling.pkl')\n",
    "df = study.trials_dataframe().drop(['state','datetime_start','datetime_complete'], axis=1)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8Xs6V9rHLL3v",
   "metadata": {
    "id": "8Xs6V9rHLL3v"
   },
   "source": [
    "## Optimal Weight decay by sampling\n",
    "\n",
    "#### Data representation\n",
    "*  DATASET : CIFAR10 = 10, CIFAR100 = 10\n",
    "*  IMB_TYPE : exp, step\n",
    "*  IMB_FACTOR : 0.1, 0.01\n",
    "\n",
    "(비슷한 수치()가 나왔을 때는 tail acc이 높은 것으로 선택)\n",
    "#### Best Weight decay\n",
    "* 10, exp, 0.1 = 0.000023 (90 epochs)\n",
    "* 10, exp, 0.01 = 0.000042 (90 epochs)\n",
    "* 10, step, 0.1 = 0.000052 (90 epochs)\n",
    "* 10, step, 0.01 = 0.000288 (200 epochs)\n",
    "* 100, exp, 0.1 = 0.001405 (200 epochs)\n",
    "* 100, exp, 0.01 = 0.001043 (200 epochs)\n",
    "* 100, step, 0.1 = 0.000479 (200 epochs)\n",
    "* 100, step, 0.01 = 0.000670 (200 epochs)\n",
    "\n",
    "##### If the integer units are the same, the higher tail accuracy is chosen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2iaEaOheKVLf",
   "metadata": {
    "id": "2iaEaOheKVLf"
   },
   "source": [
    "# Find $τ$ value for $τ$-normalization   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4835d560-6879-4137-a5a4-eb43af14556c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-22T15:30:37.776783Z",
     "iopub.status.busy": "2022-06-22T15:30:37.776326Z",
     "iopub.status.idle": "2022-06-22T15:30:37.788446Z",
     "shell.execute_reply": "2022-06-22T15:30:37.787305Z",
     "shell.execute_reply.started": "2022-06-22T15:30:37.776733Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# num_cls = 10, 100 (CIFAR10, CIFAR100)\n",
    "# val_num_each_cls = 1000, 100 (CIFAR10, CIFAR100)\n",
    "# samples_of_each_cls = 5000, 500 (CIFAR10, CIFAR100)\n",
    "\n",
    "def for_split_set(num_cls, val_num_each_cls, samples_of_each_cls):\n",
    "    val_idx_list = []\n",
    "    for i in range(num_cls):\n",
    "        for j in range(val_num_each_cls):\n",
    "            index = np.random.randint(i*0, (i+1)*samples_of_each_cls) # 0~4999, ..., 45000~49999(0~499, ..., 49500, 49999)의 숫자를 랜덤으로 하나 뽑는다.\n",
    "            while index in val_idx_list: # index가 이미 리스트에 있다면 다시 뽑는다.\n",
    "                index = np.random.randint(i*0, (i+1)*samples_of_each_cls)\n",
    "            val_idx_list.append(index)\n",
    "\n",
    "    train_idx = np.arange(50000)\n",
    "    train_idx_list = list(np.delete(train_idx, val_idx_list))\n",
    "\n",
    "    # Checking duplication\n",
    "    for i in val_idx_list:\n",
    "        if i in train_idx_list:\n",
    "            print(\"중복이 있습니다!\")\n",
    "    \n",
    "    return val_idx_list, train_idx_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "-eE-3C2JPQIL",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-22T15:30:39.590582Z",
     "iopub.status.busy": "2022-06-22T15:30:39.590016Z",
     "iopub.status.idle": "2022-06-22T15:30:39.605919Z",
     "shell.execute_reply": "2022-06-22T15:30:39.604956Z",
     "shell.execute_reply.started": "2022-06-22T15:30:39.590533Z"
    },
    "id": "-eE-3C2JPQIL",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_loaders():\n",
    "    if DATASET == 'CIFAR10':\n",
    "        val_idx, train_idx = for_split_set(10, 1000, 5000)\n",
    "        train_dataset = IMBALANCECIFAR10_V(root='../dataset/project', imb_type=IMB_TYPE, imb_factor=IMB_FACTOR, sub_idx=val_idx, train=True, download=True, transform=transform_train)\n",
    "        val_dataset = IMBALANCECIFAR10_V(root='../dataset/project', imb_factor=1, train=True, download=True, sub_idx=train_idx, transform=transform_test)\n",
    "    elif DATASET == 'CIFAR100':\n",
    "        val_idx, train_idx = for_split_set(100, 100, 500)\n",
    "        train_dataset = IMBALANCECIFAR100_V(root='../dataset/project', imb_type=IMB_TYPE, imb_factor=IMB_FACTOR, sub_idx=val_idx, train=True, download=True, transform=transform_train)\n",
    "        val_dataset = IMBALANCECIFAR100_V(root='../dataset/project', imb_factor=1, train=True, download=True, sub_idx=train_idx, transform=transform_test)\n",
    "\n",
    "    cls_num_list = train_dataset.get_cls_num_list()\n",
    "    print('cls num list(train_dataset):')\n",
    "    print(cls_num_list)\n",
    "\n",
    "    cls_num_list = val_dataset.get_cls_num_list()\n",
    "    print('cls num list(val_dataset):')\n",
    "    print(cls_num_list)\n",
    "    num_classes = len(cls_num_list)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
    "        num_workers=4, drop_last=True)\n",
    "\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset, batch_size=100, shuffle=False,\n",
    "        num_workers=4, )\n",
    "\n",
    "    return train_dataset, train_loader, val_loader, num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "118621e7-d9a5-4430-b538-336582c34f62",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-22T15:30:41.590588Z",
     "iopub.status.busy": "2022-06-22T15:30:41.590120Z",
     "iopub.status.idle": "2022-06-22T15:30:41.600673Z",
     "shell.execute_reply": "2022-06-22T15:30:41.599370Z",
     "shell.execute_reply.started": "2022-06-22T15:30:41.590536Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ResNet18(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ResNet18, self).__init__()\n",
    "        self.backbone = resnet18()\n",
    "        self.classifier = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        x = self.backbone(x)\n",
    "        x = F.adaptive_max_pool2d(x, 1)\n",
    "        x = x.view(batch_size, -1)\n",
    "        pred = self.classifier(x)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4-loqukQKZdK",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-22T15:30:43.158490Z",
     "iopub.status.busy": "2022-06-22T15:30:43.157783Z",
     "iopub.status.idle": "2022-06-22T15:30:43.173447Z",
     "shell.execute_reply": "2022-06-22T15:30:43.172226Z",
     "shell.execute_reply.started": "2022-06-22T15:30:43.158416Z"
    },
    "id": "4-loqukQKZdK",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(EPOCHS):\n",
    "    for epoch in range(EPOCHS):\n",
    "        loss_history = []\n",
    "        model.train()\n",
    "        for batch_index, data in enumerate(train_loader):\n",
    "            image, target = data\n",
    "            image, target = image.cuda(), target.cuda()\n",
    "\n",
    "            pred = model(image)\n",
    "            loss = criterion(pred, target)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_history.append(loss.item())\n",
    "\n",
    "        topk_acc, head_acc, tail_acc = compute_accuracy(train_loader, model)\n",
    "        loss_mean = np.mean(loss_history)\n",
    "        scheduler.step()\n",
    "\n",
    "        print('Epoch: [{:03d}] \\t Loss {:.4f} \\t Acc {:.2f} \\t AccHead {:.2f} \\t AccTail {:.2f}'.format(epoch+1, loss_mean, topk_acc[0], head_acc[0], tail_acc[0]))\n",
    "    \n",
    "    print(epoch)\n",
    "    torch.save({\n",
    "        'model': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'epoch': epoch},\n",
    "        osp.join(SAVE_DIR, 'ep{:03d}.pth'.format(EPOCHS))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "WULqnwRVKZfe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-22T15:30:44.103222Z",
     "iopub.status.busy": "2022-06-22T15:30:44.102756Z",
     "iopub.status.idle": "2022-06-22T15:30:44.110848Z",
     "shell.execute_reply": "2022-06-22T15:30:44.109673Z",
     "shell.execute_reply.started": "2022-06-22T15:30:44.103172Z"
    },
    "id": "WULqnwRVKZfe",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test():\n",
    "    model.eval()\n",
    "    topk_acc, head_acc, tail_acc = compute_accuracy(test_loader, model)\n",
    "    # head : training sample개수가 많은 집함, tail : training samplg이 적은 아이들 -> head의 성능저하를 최소화하고 tail의 성능을 올리는 것이 목표\n",
    "    print('Acc {:.2f} \\t AccHead {:.2f} \\t AccTail {:.2f}'.format(topk_acc[0], head_acc[0], tail_acc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "XqM9T-hwKZh0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-22T17:32:38.678208Z",
     "iopub.status.busy": "2022-06-22T17:32:38.677677Z",
     "iopub.status.idle": "2022-06-22T17:32:38.688106Z",
     "shell.execute_reply": "2022-06-22T17:32:38.687295Z",
     "shell.execute_reply.started": "2022-06-22T17:32:38.678172Z"
    },
    "id": "XqM9T-hwKZh0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def testEeahClass():\n",
    "    class_correct = list(0. for i in range(num_classes))\n",
    "    class_total = list(0. for i in range(num_classes))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            images, labels = data\n",
    "            images, labels = images.cuda(), labels.cuda()\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            c = (predicted == labels).squeeze()\n",
    "            \n",
    "            for i in range(len(labels)):  # batch size of test_loader\n",
    "                label = labels[i]\n",
    "                class_correct[label] += c[i].item()\n",
    "                class_total[label] += 1\n",
    "    \n",
    "    acc = []\n",
    "    for i in range(num_classes):\n",
    "        if class_total[i] == 0:\n",
    "            acc.append(0)\n",
    "            continue\n",
    "        acc.append(100 * class_correct[i] / class_total[i])\n",
    "    \n",
    "    for i in range(num_classes):\n",
    "        print('Accuracy of %5s : %2d %%' % (\n",
    "            train_dataset.classes[i], acc[i]))\n",
    "    \n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "nOXuVPG1Pmpg",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-22T15:31:04.642511Z",
     "iopub.status.busy": "2022-06-22T15:31:04.641756Z",
     "iopub.status.idle": "2022-06-22T15:31:04.648186Z",
     "shell.execute_reply": "2022-06-22T15:31:04.646830Z",
     "shell.execute_reply.started": "2022-06-22T15:31:04.642451Z"
    },
    "id": "nOXuVPG1Pmpg",
    "tags": []
   },
   "outputs": [],
   "source": [
    "tau_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "S2jxzUocKZkI",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-22T15:31:05.165681Z",
     "iopub.status.busy": "2022-06-22T15:31:05.165260Z",
     "iopub.status.idle": "2022-06-22T15:31:05.173589Z",
     "shell.execute_reply": "2022-06-22T15:31:05.172595Z",
     "shell.execute_reply.started": "2022-06-22T15:31:05.165633Z"
    },
    "id": "S2jxzUocKZkI",
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATASET = 'CIFAR10' #['CIFAR10', 'CIFAR100']\n",
    "IMB_TYPE = 'exp' #['exp', 'step']\n",
    "IMB_FACTOR = 0.1 #[0.1, 0.01]\n",
    "SAVE_DIR = 'logs/10-exp-0.1-find_tau' \n",
    "\n",
    "LR = 0.1\n",
    "BATCH_SIZE = 128\n",
    "MOMENTUM = 0.9\n",
    "WEIGHT_DECAY = 0.000023\n",
    "EPOCHS = 90\n",
    "\n",
    "os.makedirs(osp.join(SAVE_DIR), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3Hnfbw5pPbj6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-22T15:31:54.643900Z",
     "iopub.status.busy": "2022-06-22T15:31:54.643427Z",
     "iopub.status.idle": "2022-06-22T15:43:51.200486Z",
     "shell.execute_reply": "2022-06-22T15:43:51.199109Z",
     "shell.execute_reply.started": "2022-06-22T15:31:54.643845Z"
    },
    "id": "3Hnfbw5pPbj6",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "cls num list(train_dataset):\n",
      "[4000, 3097, 2397, 1856, 1437, 1113, 861, 667, 516, 400]\n",
      "cls num list(val_dataset):\n",
      "[1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000]\n",
      "Epoch: [001] \t Loss 2.4303 \t Acc 42.27 \t AccHead 53.95 \t AccTail 0.25\n",
      "Epoch: [002] \t Loss 1.7202 \t Acc 44.50 \t AccHead 55.38 \t AccTail 5.32\n",
      "Epoch: [003] \t Loss 1.5470 \t Acc 49.56 \t AccHead 58.89 \t AccTail 16.00\n",
      "Epoch: [004] \t Loss 1.4517 \t Acc 50.27 \t AccHead 61.86 \t AccTail 8.64\n",
      "Epoch: [005] \t Loss 1.3919 \t Acc 51.77 \t AccHead 59.71 \t AccTail 23.24\n",
      "Epoch: [006] \t Loss 1.3240 \t Acc 55.20 \t AccHead 63.75 \t AccTail 24.45\n",
      "Epoch: [007] \t Loss 1.2898 \t Acc 56.48 \t AccHead 65.19 \t AccTail 25.18\n",
      "Epoch: [008] \t Loss 1.2461 \t Acc 57.41 \t AccHead 67.46 \t AccTail 21.26\n",
      "Epoch: [009] \t Loss 1.2150 \t Acc 58.64 \t AccHead 65.31 \t AccTail 34.61\n",
      "Epoch: [010] \t Loss 1.1734 \t Acc 60.57 \t AccHead 67.40 \t AccTail 36.02\n",
      "Epoch: [011] \t Loss 1.1361 \t Acc 61.42 \t AccHead 71.82 \t AccTail 24.05\n",
      "Epoch: [012] \t Loss 1.0970 \t Acc 62.88 \t AccHead 71.60 \t AccTail 31.60\n",
      "Epoch: [013] \t Loss 1.0532 \t Acc 64.57 \t AccHead 72.58 \t AccTail 35.77\n",
      "Epoch: [014] \t Loss 1.0323 \t Acc 64.55 \t AccHead 72.72 \t AccTail 35.09\n",
      "Epoch: [015] \t Loss 0.9869 \t Acc 66.09 \t AccHead 73.45 \t AccTail 39.63\n",
      "Epoch: [016] \t Loss 0.9649 \t Acc 68.27 \t AccHead 74.04 \t AccTail 47.45\n",
      "Epoch: [017] \t Loss 0.9341 \t Acc 67.29 \t AccHead 74.72 \t AccTail 40.57\n",
      "Epoch: [018] \t Loss 0.9074 \t Acc 70.71 \t AccHead 75.88 \t AccTail 52.12\n",
      "Epoch: [019] \t Loss 0.8760 \t Acc 68.45 \t AccHead 73.97 \t AccTail 48.59\n",
      "Epoch: [020] \t Loss 0.8467 \t Acc 70.42 \t AccHead 76.88 \t AccTail 47.19\n",
      "Epoch: [021] \t Loss 0.8353 \t Acc 68.18 \t AccHead 72.59 \t AccTail 52.37\n",
      "Epoch: [022] \t Loss 0.8111 \t Acc 72.34 \t AccHead 76.68 \t AccTail 56.79\n",
      "Epoch: [023] \t Loss 0.7894 \t Acc 72.48 \t AccHead 78.78 \t AccTail 49.86\n",
      "Epoch: [024] \t Loss 0.7655 \t Acc 74.51 \t AccHead 80.09 \t AccTail 54.47\n",
      "Epoch: [025] \t Loss 0.7561 \t Acc 74.92 \t AccHead 77.88 \t AccTail 64.29\n",
      "Epoch: [026] \t Loss 0.7325 \t Acc 75.99 \t AccHead 80.68 \t AccTail 59.15\n",
      "Epoch: [027] \t Loss 0.7132 \t Acc 76.14 \t AccHead 81.64 \t AccTail 56.35\n",
      "Epoch: [028] \t Loss 0.6988 \t Acc 77.43 \t AccHead 81.83 \t AccTail 61.60\n",
      "Epoch: [029] \t Loss 0.6915 \t Acc 77.40 \t AccHead 81.56 \t AccTail 62.44\n",
      "Epoch: [030] \t Loss 0.6555 \t Acc 78.33 \t AccHead 81.98 \t AccTail 65.22\n",
      "Epoch: [031] \t Loss 0.6417 \t Acc 77.69 \t AccHead 80.99 \t AccTail 65.81\n",
      "Epoch: [032] \t Loss 0.6351 \t Acc 79.83 \t AccHead 83.58 \t AccTail 66.33\n",
      "Epoch: [033] \t Loss 0.6155 \t Acc 80.33 \t AccHead 83.65 \t AccTail 68.39\n",
      "Epoch: [034] \t Loss 0.6041 \t Acc 80.75 \t AccHead 84.36 \t AccTail 67.74\n",
      "Epoch: [035] \t Loss 0.5857 \t Acc 80.78 \t AccHead 84.14 \t AccTail 68.71\n",
      "Epoch: [036] \t Loss 0.5730 \t Acc 80.88 \t AccHead 85.08 \t AccTail 65.74\n",
      "Epoch: [037] \t Loss 0.5507 \t Acc 81.08 \t AccHead 84.53 \t AccTail 68.68\n",
      "Epoch: [038] \t Loss 0.5461 \t Acc 80.08 \t AccHead 86.92 \t AccTail 55.49\n",
      "Epoch: [039] \t Loss 0.5242 \t Acc 79.97 \t AccHead 85.11 \t AccTail 61.49\n",
      "Epoch: [040] \t Loss 0.5220 \t Acc 82.85 \t AccHead 84.50 \t AccTail 76.92\n",
      "Epoch: [041] \t Loss 0.5237 \t Acc 83.67 \t AccHead 86.58 \t AccTail 73.24\n",
      "Epoch: [042] \t Loss 0.5038 \t Acc 82.22 \t AccHead 85.40 \t AccTail 70.80\n",
      "Epoch: [043] \t Loss 0.4879 \t Acc 84.63 \t AccHead 88.10 \t AccTail 72.16\n",
      "Epoch: [044] \t Loss 0.4882 \t Acc 83.72 \t AccHead 86.81 \t AccTail 72.59\n",
      "Epoch: [045] \t Loss 0.4822 \t Acc 83.65 \t AccHead 87.24 \t AccTail 70.73\n",
      "Epoch: [046] \t Loss 0.4551 \t Acc 85.55 \t AccHead 89.26 \t AccTail 72.24\n",
      "Epoch: [047] \t Loss 0.4483 \t Acc 85.82 \t AccHead 89.25 \t AccTail 73.47\n",
      "Epoch: [048] \t Loss 0.4404 \t Acc 85.27 \t AccHead 87.74 \t AccTail 76.41\n",
      "Epoch: [049] \t Loss 0.4308 \t Acc 85.84 \t AccHead 88.74 \t AccTail 75.38\n",
      "Epoch: [050] \t Loss 0.4274 \t Acc 86.12 \t AccHead 87.82 \t AccTail 80.02\n",
      "Epoch: [051] \t Loss 0.4176 \t Acc 86.24 \t AccHead 90.56 \t AccTail 70.73\n",
      "Epoch: [052] \t Loss 0.4023 \t Acc 87.02 \t AccHead 89.48 \t AccTail 78.20\n",
      "Epoch: [053] \t Loss 0.3941 \t Acc 87.55 \t AccHead 91.07 \t AccTail 74.90\n",
      "Epoch: [054] \t Loss 0.3939 \t Acc 87.72 \t AccHead 91.09 \t AccTail 75.57\n",
      "Epoch: [055] \t Loss 0.3686 \t Acc 87.61 \t AccHead 89.69 \t AccTail 80.14\n",
      "Epoch: [056] \t Loss 0.3768 \t Acc 86.99 \t AccHead 90.53 \t AccTail 74.32\n",
      "Epoch: [057] \t Loss 0.3724 \t Acc 87.80 \t AccHead 90.09 \t AccTail 79.54\n",
      "Epoch: [058] \t Loss 0.3605 \t Acc 88.97 \t AccHead 90.51 \t AccTail 83.44\n",
      "Epoch: [059] \t Loss 0.3492 \t Acc 88.67 \t AccHead 90.59 \t AccTail 81.75\n",
      "Epoch: [060] \t Loss 0.3431 \t Acc 88.72 \t AccHead 90.25 \t AccTail 83.21\n",
      "Epoch: [061] \t Loss 0.3383 \t Acc 89.43 \t AccHead 91.70 \t AccTail 81.26\n",
      "Epoch: [062] \t Loss 0.3284 \t Acc 88.13 \t AccHead 89.59 \t AccTail 82.91\n",
      "Epoch: [063] \t Loss 0.3253 \t Acc 88.59 \t AccHead 90.43 \t AccTail 81.98\n",
      "Epoch: [064] \t Loss 0.3082 \t Acc 89.58 \t AccHead 90.26 \t AccTail 87.15\n",
      "Epoch: [065] \t Loss 0.3085 \t Acc 90.35 \t AccHead 91.71 \t AccTail 85.44\n",
      "Epoch: [066] \t Loss 0.2986 \t Acc 90.40 \t AccHead 92.88 \t AccTail 81.46\n",
      "Epoch: [067] \t Loss 0.3027 \t Acc 90.99 \t AccHead 92.12 \t AccTail 86.91\n",
      "Epoch: [068] \t Loss 0.3035 \t Acc 90.54 \t AccHead 92.44 \t AccTail 83.71\n",
      "Epoch: [069] \t Loss 0.2990 \t Acc 90.72 \t AccHead 91.64 \t AccTail 87.43\n",
      "Epoch: [070] \t Loss 0.2776 \t Acc 91.05 \t AccHead 92.00 \t AccTail 87.64\n",
      "Epoch: [071] \t Loss 0.2768 \t Acc 91.70 \t AccHead 93.12 \t AccTail 86.59\n",
      "Epoch: [072] \t Loss 0.2650 \t Acc 92.12 \t AccHead 93.34 \t AccTail 87.75\n",
      "Epoch: [073] \t Loss 0.2630 \t Acc 91.76 \t AccHead 93.27 \t AccTail 86.37\n",
      "Epoch: [074] \t Loss 0.2597 \t Acc 90.50 \t AccHead 92.47 \t AccTail 83.41\n",
      "Epoch: [075] \t Loss 0.2549 \t Acc 90.75 \t AccHead 92.27 \t AccTail 85.29\n",
      "Epoch: [076] \t Loss 0.2502 \t Acc 90.93 \t AccHead 91.57 \t AccTail 88.64\n",
      "Epoch: [077] \t Loss 0.2526 \t Acc 92.35 \t AccHead 93.66 \t AccTail 87.64\n",
      "Epoch: [078] \t Loss 0.2464 \t Acc 92.01 \t AccHead 92.76 \t AccTail 89.30\n",
      "Epoch: [079] \t Loss 0.2409 \t Acc 92.48 \t AccHead 94.23 \t AccTail 86.15\n",
      "Epoch: [080] \t Loss 0.2406 \t Acc 92.89 \t AccHead 94.21 \t AccTail 88.19\n",
      "Epoch: [081] \t Loss 0.2368 \t Acc 92.85 \t AccHead 95.17 \t AccTail 84.51\n",
      "Epoch: [082] \t Loss 0.2236 \t Acc 93.17 \t AccHead 95.24 \t AccTail 85.72\n",
      "Epoch: [083] \t Loss 0.2180 \t Acc 93.29 \t AccHead 95.06 \t AccTail 86.92\n",
      "Epoch: [084] \t Loss 0.2270 \t Acc 92.95 \t AccHead 93.63 \t AccTail 90.50\n",
      "Epoch: [085] \t Loss 0.2124 \t Acc 92.08 \t AccHead 92.73 \t AccTail 89.74\n",
      "Epoch: [086] \t Loss 0.2265 \t Acc 93.32 \t AccHead 94.56 \t AccTail 88.86\n",
      "Epoch: [087] \t Loss 0.2114 \t Acc 93.07 \t AccHead 94.16 \t AccTail 89.16\n",
      "Epoch: [088] \t Loss 0.2132 \t Acc 93.25 \t AccHead 94.41 \t AccTail 89.06\n",
      "Epoch: [089] \t Loss 0.2028 \t Acc 93.49 \t AccHead 94.64 \t AccTail 89.38\n",
      "Epoch: [090] \t Loss 0.1949 \t Acc 94.26 \t AccHead 94.25 \t AccTail 94.29\n",
      "89\n",
      "Finished Training\n",
      "Acc 67.98 \t AccHead 74.22 \t AccTail 61.60\n",
      "----------------------------\n",
      "get the top-1 accuracy for each of the classes.\n",
      "Accuracy of airplane : 86 %\n",
      "Accuracy of automobile : 92 %\n",
      "Accuracy of  bird : 70 %\n",
      "Accuracy of   cat : 55 %\n",
      "Accuracy of  deer : 65 %\n",
      "Accuracy of   dog : 58 %\n",
      "Accuracy of  frog : 64 %\n",
      "Accuracy of horse : 60 %\n",
      "Accuracy of  ship : 67 %\n",
      "Accuracy of truck : 56 %\n",
      "0.67859012345679\n"
     ]
    }
   ],
   "source": [
    "train_dataset, train_loader, test_loader, num_classes = get_loaders()\n",
    "model = ResNet18(num_classes)\n",
    "model = model.cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=150, gamma=0.1)\n",
    "\n",
    "# Let's build our model\n",
    "train(EPOCHS)\n",
    "print('Finished Training')\n",
    "test() \n",
    "print('----------------------------')\n",
    "print('get the top-1 accuracy for each of the classes.')\n",
    "\n",
    "avg_accu = testEeahClass()\n",
    "tau = np.mean(np.array(avg_accu)) / 100\n",
    "print(tau)\n",
    "tau_list.append(tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "yzudYD_oLP1T",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-22T15:43:51.221804Z",
     "iopub.status.busy": "2022-06-22T15:43:51.221427Z",
     "iopub.status.idle": "2022-06-22T15:43:51.228461Z",
     "shell.execute_reply": "2022-06-22T15:43:51.227551Z",
     "shell.execute_reply.started": "2022-06-22T15:43:51.221773Z"
    },
    "id": "yzudYD_oLP1T"
   },
   "outputs": [],
   "source": [
    "DATASET = 'CIFAR10' #['CIFAR10', 'CIFAR100']\n",
    "IMB_TYPE = 'exp' #['exp', 'step']\n",
    "IMB_FACTOR = 0.01 #[0.1, 0.01]\n",
    "SAVE_DIR = 'logs/10-exp-0.01-find_tau' \n",
    "\n",
    "LR = 0.1\n",
    "BATCH_SIZE = 128\n",
    "MOMENTUM = 0.9\n",
    "WEIGHT_DECAY = 0.000042\n",
    "EPOCHS = 90\n",
    "\n",
    "os.makedirs(osp.join(SAVE_DIR), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "JPQdko4UPcQG",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-22T15:43:51.230325Z",
     "iopub.status.busy": "2022-06-22T15:43:51.230055Z",
     "iopub.status.idle": "2022-06-22T15:52:25.501350Z",
     "shell.execute_reply": "2022-06-22T15:52:25.499979Z",
     "shell.execute_reply.started": "2022-06-22T15:43:51.230298Z"
    },
    "id": "JPQdko4UPcQG",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "cls num list(train_dataset):\n",
      "[4000, 2397, 1437, 861, 516, 309, 185, 111, 66, 40]\n",
      "cls num list(val_dataset):\n",
      "[1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000]\n",
      "Epoch: [001] \t Loss 2.9861 \t Acc 44.00 \t AccHead 47.40 \t AccTail 0.00\n",
      "Epoch: [002] \t Loss 1.8592 \t Acc 47.37 \t AccHead 51.03 \t AccTail 0.00\n",
      "Epoch: [003] \t Loss 1.4960 \t Acc 43.14 \t AccHead 46.47 \t AccTail 0.00\n",
      "Epoch: [004] \t Loss 1.3264 \t Acc 57.01 \t AccHead 61.41 \t AccTail 0.00\n",
      "Epoch: [005] \t Loss 1.2236 \t Acc 58.21 \t AccHead 62.72 \t AccTail 0.00\n",
      "Epoch: [006] \t Loss 1.1949 \t Acc 60.91 \t AccHead 65.61 \t AccTail 0.00\n",
      "Epoch: [007] \t Loss 1.1515 \t Acc 61.92 \t AccHead 66.69 \t AccTail 0.00\n",
      "Epoch: [008] \t Loss 1.1090 \t Acc 63.38 \t AccHead 68.27 \t AccTail 0.00\n",
      "Epoch: [009] \t Loss 1.0910 \t Acc 62.07 \t AccHead 66.87 \t AccTail 0.00\n",
      "Epoch: [010] \t Loss 1.0654 \t Acc 65.97 \t AccHead 71.02 \t AccTail 0.71\n",
      "Epoch: [011] \t Loss 1.0377 \t Acc 66.21 \t AccHead 71.34 \t AccTail 0.00\n",
      "Epoch: [012] \t Loss 0.9917 \t Acc 65.80 \t AccHead 70.71 \t AccTail 1.99\n",
      "Epoch: [013] \t Loss 0.9671 \t Acc 68.00 \t AccHead 73.17 \t AccTail 1.13\n",
      "Epoch: [014] \t Loss 0.9704 \t Acc 67.30 \t AccHead 72.49 \t AccTail 0.00\n",
      "Epoch: [015] \t Loss 0.9454 \t Acc 67.72 \t AccHead 72.93 \t AccTail 0.00\n",
      "Epoch: [016] \t Loss 0.9331 \t Acc 70.12 \t AccHead 75.45 \t AccTail 1.41\n",
      "Epoch: [017] \t Loss 0.9051 \t Acc 69.24 \t AccHead 74.25 \t AccTail 4.25\n",
      "Epoch: [018] \t Loss 0.8727 \t Acc 71.56 \t AccHead 76.58 \t AccTail 6.65\n",
      "Epoch: [019] \t Loss 0.8793 \t Acc 71.94 \t AccHead 77.42 \t AccTail 0.57\n",
      "Epoch: [020] \t Loss 0.8494 \t Acc 72.02 \t AccHead 77.12 \t AccTail 5.82\n",
      "Epoch: [021] \t Loss 0.8371 \t Acc 71.72 \t AccHead 77.11 \t AccTail 1.84\n",
      "Epoch: [022] \t Loss 0.8034 \t Acc 72.43 \t AccHead 77.62 \t AccTail 4.97\n",
      "Epoch: [023] \t Loss 0.8140 \t Acc 73.10 \t AccHead 78.22 \t AccTail 6.53\n",
      "Epoch: [024] \t Loss 0.8082 \t Acc 70.15 \t AccHead 75.39 \t AccTail 2.40\n",
      "Epoch: [025] \t Loss 0.7798 \t Acc 74.15 \t AccHead 79.36 \t AccTail 6.13\n",
      "Epoch: [026] \t Loss 0.7672 \t Acc 72.56 \t AccHead 77.80 \t AccTail 4.55\n",
      "Epoch: [027] \t Loss 0.7563 \t Acc 75.07 \t AccHead 79.34 \t AccTail 19.69\n",
      "Epoch: [028] \t Loss 0.7413 \t Acc 74.86 \t AccHead 79.63 \t AccTail 13.03\n",
      "Epoch: [029] \t Loss 0.7244 \t Acc 75.98 \t AccHead 80.83 \t AccTail 13.30\n",
      "Epoch: [030] \t Loss 0.7209 \t Acc 75.66 \t AccHead 80.82 \t AccTail 8.91\n",
      "Epoch: [031] \t Loss 0.7029 \t Acc 75.95 \t AccHead 80.15 \t AccTail 21.75\n",
      "Epoch: [032] \t Loss 0.6879 \t Acc 75.96 \t AccHead 80.94 \t AccTail 11.60\n",
      "Epoch: [033] \t Loss 0.6869 \t Acc 77.20 \t AccHead 81.61 \t AccTail 20.11\n",
      "Epoch: [034] \t Loss 0.6762 \t Acc 77.66 \t AccHead 83.13 \t AccTail 7.05\n",
      "Epoch: [035] \t Loss 0.6518 \t Acc 79.28 \t AccHead 83.90 \t AccTail 19.29\n",
      "Epoch: [036] \t Loss 0.6447 \t Acc 79.22 \t AccHead 83.92 \t AccTail 18.27\n",
      "Epoch: [037] \t Loss 0.6319 \t Acc 79.19 \t AccHead 83.89 \t AccTail 18.50\n",
      "Epoch: [038] \t Loss 0.6204 \t Acc 80.57 \t AccHead 85.04 \t AccTail 22.85\n",
      "Epoch: [039] \t Loss 0.6180 \t Acc 80.55 \t AccHead 84.88 \t AccTail 24.47\n",
      "Epoch: [040] \t Loss 0.6022 \t Acc 80.34 \t AccHead 84.50 \t AccTail 26.35\n",
      "Epoch: [041] \t Loss 0.5958 \t Acc 79.61 \t AccHead 83.91 \t AccTail 24.12\n",
      "Epoch: [042] \t Loss 0.5909 \t Acc 80.93 \t AccHead 84.98 \t AccTail 28.33\n",
      "Epoch: [043] \t Loss 0.5704 \t Acc 78.83 \t AccHead 82.98 \t AccTail 25.25\n",
      "Epoch: [044] \t Loss 0.5690 \t Acc 82.65 \t AccHead 86.45 \t AccTail 33.43\n",
      "Epoch: [045] \t Loss 0.5713 \t Acc 79.03 \t AccHead 82.60 \t AccTail 33.00\n",
      "Epoch: [046] \t Loss 0.5555 \t Acc 82.89 \t AccHead 86.71 \t AccTail 33.14\n",
      "Epoch: [047] \t Loss 0.5485 \t Acc 81.57 \t AccHead 85.56 \t AccTail 29.83\n",
      "Epoch: [048] \t Loss 0.5215 \t Acc 82.06 \t AccHead 85.81 \t AccTail 33.52\n",
      "Epoch: [049] \t Loss 0.5221 \t Acc 83.53 \t AccHead 87.15 \t AccTail 36.42\n",
      "Epoch: [050] \t Loss 0.5136 \t Acc 83.74 \t AccHead 86.60 \t AccTail 46.75\n",
      "Epoch: [051] \t Loss 0.5042 \t Acc 83.69 \t AccHead 86.75 \t AccTail 43.89\n",
      "Epoch: [052] \t Loss 0.4921 \t Acc 84.29 \t AccHead 88.33 \t AccTail 32.11\n",
      "Epoch: [053] \t Loss 0.4806 \t Acc 83.64 \t AccHead 87.06 \t AccTail 39.38\n",
      "Epoch: [054] \t Loss 0.4789 \t Acc 84.11 \t AccHead 86.44 \t AccTail 53.70\n",
      "Epoch: [055] \t Loss 0.4636 \t Acc 83.81 \t AccHead 86.97 \t AccTail 42.70\n",
      "Epoch: [056] \t Loss 0.4611 \t Acc 85.12 \t AccHead 88.48 \t AccTail 41.42\n",
      "Epoch: [057] \t Loss 0.4521 \t Acc 84.16 \t AccHead 87.42 \t AccTail 41.76\n",
      "Epoch: [058] \t Loss 0.4596 \t Acc 85.82 \t AccHead 89.07 \t AccTail 43.79\n",
      "Epoch: [059] \t Loss 0.4314 \t Acc 81.06 \t AccHead 84.24 \t AccTail 40.00\n",
      "Epoch: [060] \t Loss 0.4425 \t Acc 86.04 \t AccHead 90.05 \t AccTail 34.09\n",
      "Epoch: [061] \t Loss 0.4253 \t Acc 86.20 \t AccHead 88.91 \t AccTail 51.06\n",
      "Epoch: [062] \t Loss 0.4144 \t Acc 86.17 \t AccHead 88.74 \t AccTail 52.83\n",
      "Epoch: [063] \t Loss 0.4216 \t Acc 86.31 \t AccHead 88.53 \t AccTail 57.57\n",
      "Epoch: [064] \t Loss 0.4039 \t Acc 86.68 \t AccHead 89.84 \t AccTail 45.84\n",
      "Epoch: [065] \t Loss 0.3934 \t Acc 86.10 \t AccHead 88.61 \t AccTail 53.67\n",
      "Epoch: [066] \t Loss 0.4069 \t Acc 87.86 \t AccHead 90.13 \t AccTail 58.53\n",
      "Epoch: [067] \t Loss 0.3874 \t Acc 87.74 \t AccHead 90.21 \t AccTail 55.87\n",
      "Epoch: [068] \t Loss 0.3765 \t Acc 87.08 \t AccHead 89.42 \t AccTail 56.68\n",
      "Epoch: [069] \t Loss 0.3927 \t Acc 88.22 \t AccHead 90.99 \t AccTail 52.27\n",
      "Epoch: [070] \t Loss 0.3691 \t Acc 87.41 \t AccHead 89.52 \t AccTail 60.11\n",
      "Epoch: [071] \t Loss 0.3591 \t Acc 87.31 \t AccHead 89.41 \t AccTail 60.06\n",
      "Epoch: [072] \t Loss 0.3574 \t Acc 88.09 \t AccHead 90.39 \t AccTail 58.27\n",
      "Epoch: [073] \t Loss 0.3405 \t Acc 88.33 \t AccHead 90.50 \t AccTail 60.37\n",
      "Epoch: [074] \t Loss 0.3410 \t Acc 89.37 \t AccHead 90.78 \t AccTail 71.09\n",
      "Epoch: [075] \t Loss 0.3330 \t Acc 88.40 \t AccHead 90.35 \t AccTail 63.28\n",
      "Epoch: [076] \t Loss 0.3304 \t Acc 89.21 \t AccHead 91.13 \t AccTail 64.45\n",
      "Epoch: [077] \t Loss 0.3246 \t Acc 88.98 \t AccHead 91.19 \t AccTail 60.28\n",
      "Epoch: [078] \t Loss 0.3262 \t Acc 88.85 \t AccHead 91.41 \t AccTail 55.60\n",
      "Epoch: [079] \t Loss 0.3050 \t Acc 89.94 \t AccHead 91.75 \t AccTail 66.43\n",
      "Epoch: [080] \t Loss 0.3112 \t Acc 90.07 \t AccHead 91.31 \t AccTail 74.01\n",
      "Epoch: [081] \t Loss 0.3174 \t Acc 90.08 \t AccHead 91.84 \t AccTail 67.33\n",
      "Epoch: [082] \t Loss 0.2982 \t Acc 90.06 \t AccHead 91.56 \t AccTail 70.62\n",
      "Epoch: [083] \t Loss 0.2911 \t Acc 91.50 \t AccHead 93.06 \t AccTail 71.29\n",
      "Epoch: [084] \t Loss 0.2830 \t Acc 89.79 \t AccHead 91.92 \t AccTail 62.29\n",
      "Epoch: [085] \t Loss 0.2854 \t Acc 89.62 \t AccHead 91.41 \t AccTail 66.43\n",
      "Epoch: [086] \t Loss 0.2723 \t Acc 91.21 \t AccHead 92.83 \t AccTail 70.38\n",
      "Epoch: [087] \t Loss 0.2761 \t Acc 91.13 \t AccHead 92.58 \t AccTail 72.26\n",
      "Epoch: [088] \t Loss 0.2758 \t Acc 91.82 \t AccHead 93.31 \t AccTail 72.44\n",
      "Epoch: [089] \t Loss 0.2621 \t Acc 91.36 \t AccHead 92.49 \t AccTail 76.63\n",
      "Epoch: [090] \t Loss 0.2608 \t Acc 91.79 \t AccHead 93.24 \t AccTail 73.06\n",
      "89\n",
      "Finished Training\n",
      "Acc 50.14 \t AccHead 72.46 \t AccTail 27.28\n",
      "----------------------------\n",
      "get the top-1 accuracy for each of the classes.\n",
      "Accuracy of airplane : 89 %\n",
      "Accuracy of automobile : 87 %\n",
      "Accuracy of  bird : 64 %\n",
      "Accuracy of   cat : 61 %\n",
      "Accuracy of  deer : 58 %\n",
      "Accuracy of   dog : 32 %\n",
      "Accuracy of  frog : 32 %\n",
      "Accuracy of horse : 42 %\n",
      "Accuracy of  ship : 18 %\n",
      "Accuracy of truck :  9 %\n",
      "0.49658367346938775\n"
     ]
    }
   ],
   "source": [
    "train_dataset, train_loader, test_loader, num_classes = get_loaders()\n",
    "model = ResNet18(num_classes)\n",
    "model = model.cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=150, gamma=0.1)\n",
    "\n",
    "# Let's build our model\n",
    "train(EPOCHS)\n",
    "print('Finished Training')\n",
    "test() \n",
    "print('----------------------------')\n",
    "print('get the top-1 accuracy for each of the classes.')\n",
    "avg_accu = testEeahClass()\n",
    "tau = np.mean(np.array(avg_accu)) / 100\n",
    "print(tau)\n",
    "tau_list.append(tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "JeoD_0gtLQBi",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-22T15:52:25.505564Z",
     "iopub.status.busy": "2022-06-22T15:52:25.504765Z",
     "iopub.status.idle": "2022-06-22T15:52:25.514166Z",
     "shell.execute_reply": "2022-06-22T15:52:25.513356Z",
     "shell.execute_reply.started": "2022-06-22T15:52:25.505494Z"
    },
    "id": "JeoD_0gtLQBi"
   },
   "outputs": [],
   "source": [
    "DATASET = 'CIFAR10' #['CIFAR10', 'CIFAR100']\n",
    "IMB_TYPE = 'step' #['exp', 'step']\n",
    "IMB_FACTOR = 0.1 #[0.1, 0.01]\n",
    "SAVE_DIR = 'logs/10-step-0.1-find_tau' \n",
    "\n",
    "LR = 0.1\n",
    "BATCH_SIZE = 128\n",
    "MOMENTUM = 0.9\n",
    "WEIGHT_DECAY = 0.000052\n",
    "EPOCHS = 90\n",
    "\n",
    "os.makedirs(osp.join(SAVE_DIR), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "XbXYNNtrPdBW",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-22T15:52:25.515652Z",
     "iopub.status.busy": "2022-06-22T15:52:25.515384Z",
     "iopub.status.idle": "2022-06-22T16:06:42.736356Z",
     "shell.execute_reply": "2022-06-22T16:06:42.734996Z",
     "shell.execute_reply.started": "2022-06-22T15:52:25.515619Z"
    },
    "id": "XbXYNNtrPdBW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "cls num list(train_dataset):\n",
      "[4000, 4000, 4000, 4000, 4000, 400, 400, 400, 400, 400]\n",
      "cls num list(val_dataset):\n",
      "[1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000]\n",
      "Epoch: [001] \t Loss 2.3790 \t Acc 32.01 \t AccHead 35.05 \t AccTail 0.42\n",
      "Epoch: [002] \t Loss 1.6655 \t Acc 46.33 \t AccHead 50.79 \t AccTail 0.00\n",
      "Epoch: [003] \t Loss 1.4529 \t Acc 51.42 \t AccHead 56.37 \t AccTail 0.00\n",
      "Epoch: [004] \t Loss 1.3541 \t Acc 52.02 \t AccHead 57.03 \t AccTail 0.00\n",
      "Epoch: [005] \t Loss 1.3000 \t Acc 55.58 \t AccHead 60.94 \t AccTail 0.00\n",
      "Epoch: [006] \t Loss 1.2533 \t Acc 57.78 \t AccHead 63.30 \t AccTail 0.42\n",
      "Epoch: [007] \t Loss 1.1986 \t Acc 58.65 \t AccHead 64.26 \t AccTail 0.47\n",
      "Epoch: [008] \t Loss 1.1696 \t Acc 61.08 \t AccHead 66.90 \t AccTail 0.62\n",
      "Epoch: [009] \t Loss 1.1185 \t Acc 62.07 \t AccHead 67.92 \t AccTail 1.30\n",
      "Epoch: [010] \t Loss 1.0778 \t Acc 65.24 \t AccHead 71.43 \t AccTail 0.78\n",
      "Epoch: [011] \t Loss 1.0408 \t Acc 63.99 \t AccHead 70.00 \t AccTail 1.61\n",
      "Epoch: [012] \t Loss 1.0136 \t Acc 66.09 \t AccHead 71.95 \t AccTail 5.20\n",
      "Epoch: [013] \t Loss 0.9677 \t Acc 68.74 \t AccHead 74.88 \t AccTail 4.99\n",
      "Epoch: [014] \t Loss 0.9370 \t Acc 68.17 \t AccHead 74.01 \t AccTail 7.59\n",
      "Epoch: [015] \t Loss 0.9078 \t Acc 69.93 \t AccHead 75.36 \t AccTail 13.61\n",
      "Epoch: [016] \t Loss 0.8814 \t Acc 72.47 \t AccHead 78.32 \t AccTail 11.66\n",
      "Epoch: [017] \t Loss 0.8517 \t Acc 71.73 \t AccHead 77.27 \t AccTail 14.25\n",
      "Epoch: [018] \t Loss 0.8399 \t Acc 73.21 \t AccHead 78.33 \t AccTail 20.11\n",
      "Epoch: [019] \t Loss 0.8147 \t Acc 73.62 \t AccHead 78.55 \t AccTail 22.60\n",
      "Epoch: [020] \t Loss 0.7980 \t Acc 72.66 \t AccHead 77.56 \t AccTail 21.68\n",
      "Epoch: [021] \t Loss 0.7760 \t Acc 75.02 \t AccHead 79.30 \t AccTail 30.61\n",
      "Epoch: [022] \t Loss 0.7531 \t Acc 75.51 \t AccHead 80.02 \t AccTail 28.64\n",
      "Epoch: [023] \t Loss 0.7458 \t Acc 75.73 \t AccHead 79.98 \t AccTail 31.64\n",
      "Epoch: [024] \t Loss 0.7295 \t Acc 76.09 \t AccHead 81.02 \t AccTail 24.96\n",
      "Epoch: [025] \t Loss 0.7147 \t Acc 78.63 \t AccHead 83.64 \t AccTail 26.63\n",
      "Epoch: [026] \t Loss 0.6951 \t Acc 77.09 \t AccHead 81.24 \t AccTail 34.11\n",
      "Epoch: [027] \t Loss 0.6785 \t Acc 78.04 \t AccHead 82.53 \t AccTail 31.41\n",
      "Epoch: [028] \t Loss 0.6644 \t Acc 77.93 \t AccHead 82.12 \t AccTail 34.44\n",
      "Epoch: [029] \t Loss 0.6486 \t Acc 78.73 \t AccHead 83.43 \t AccTail 29.97\n",
      "Epoch: [030] \t Loss 0.6441 \t Acc 78.58 \t AccHead 82.69 \t AccTail 35.86\n",
      "Epoch: [031] \t Loss 0.6348 \t Acc 79.71 \t AccHead 84.38 \t AccTail 31.11\n",
      "Epoch: [032] \t Loss 0.6170 \t Acc 80.07 \t AccHead 84.45 \t AccTail 34.63\n",
      "Epoch: [033] \t Loss 0.5991 \t Acc 81.13 \t AccHead 85.39 \t AccTail 36.87\n",
      "Epoch: [034] \t Loss 0.5894 \t Acc 81.12 \t AccHead 85.04 \t AccTail 40.42\n",
      "Epoch: [035] \t Loss 0.5883 \t Acc 80.75 \t AccHead 85.01 \t AccTail 36.51\n",
      "Epoch: [036] \t Loss 0.5721 \t Acc 81.99 \t AccHead 85.63 \t AccTail 44.12\n",
      "Epoch: [037] \t Loss 0.5637 \t Acc 80.76 \t AccHead 84.39 \t AccTail 43.06\n",
      "Epoch: [038] \t Loss 0.5540 \t Acc 81.48 \t AccHead 85.31 \t AccTail 41.71\n",
      "Epoch: [039] \t Loss 0.5502 \t Acc 82.68 \t AccHead 86.77 \t AccTail 40.23\n",
      "Epoch: [040] \t Loss 0.5392 \t Acc 83.24 \t AccHead 86.80 \t AccTail 46.25\n",
      "Epoch: [041] \t Loss 0.5265 \t Acc 83.46 \t AccHead 87.05 \t AccTail 46.15\n",
      "Epoch: [042] \t Loss 0.5232 \t Acc 83.26 \t AccHead 87.72 \t AccTail 36.97\n",
      "Epoch: [043] \t Loss 0.5176 \t Acc 84.93 \t AccHead 88.46 \t AccTail 48.26\n",
      "Epoch: [044] \t Loss 0.5013 \t Acc 83.70 \t AccHead 87.40 \t AccTail 45.38\n",
      "Epoch: [045] \t Loss 0.5009 \t Acc 84.69 \t AccHead 87.68 \t AccTail 53.77\n",
      "Epoch: [046] \t Loss 0.4879 \t Acc 84.73 \t AccHead 87.91 \t AccTail 51.69\n",
      "Epoch: [047] \t Loss 0.4853 \t Acc 84.25 \t AccHead 87.54 \t AccTail 50.05\n",
      "Epoch: [048] \t Loss 0.4727 \t Acc 83.45 \t AccHead 86.74 \t AccTail 49.30\n",
      "Epoch: [049] \t Loss 0.4761 \t Acc 84.03 \t AccHead 87.30 \t AccTail 50.08\n",
      "Epoch: [050] \t Loss 0.4576 \t Acc 84.41 \t AccHead 87.67 \t AccTail 50.65\n",
      "Epoch: [051] \t Loss 0.4493 \t Acc 85.38 \t AccHead 88.25 \t AccTail 55.56\n",
      "Epoch: [052] \t Loss 0.4441 \t Acc 85.97 \t AccHead 89.12 \t AccTail 53.15\n",
      "Epoch: [053] \t Loss 0.4389 \t Acc 83.13 \t AccHead 85.67 \t AccTail 56.68\n",
      "Epoch: [054] \t Loss 0.4343 \t Acc 86.36 \t AccHead 89.12 \t AccTail 57.63\n",
      "Epoch: [055] \t Loss 0.4286 \t Acc 85.86 \t AccHead 88.53 \t AccTail 58.21\n",
      "Epoch: [056] \t Loss 0.4127 \t Acc 86.57 \t AccHead 89.61 \t AccTail 54.99\n",
      "Epoch: [057] \t Loss 0.4128 \t Acc 84.61 \t AccHead 87.33 \t AccTail 56.35\n",
      "Epoch: [058] \t Loss 0.4093 \t Acc 86.79 \t AccHead 89.65 \t AccTail 57.09\n",
      "Epoch: [059] \t Loss 0.3991 \t Acc 86.50 \t AccHead 89.64 \t AccTail 53.92\n",
      "Epoch: [060] \t Loss 0.3958 \t Acc 87.93 \t AccHead 89.67 \t AccTail 69.89\n",
      "Epoch: [061] \t Loss 0.3987 \t Acc 86.86 \t AccHead 89.91 \t AccTail 55.25\n",
      "Epoch: [062] \t Loss 0.3889 \t Acc 87.79 \t AccHead 90.37 \t AccTail 61.09\n",
      "Epoch: [063] \t Loss 0.3740 \t Acc 87.40 \t AccHead 90.07 \t AccTail 59.75\n",
      "Epoch: [064] \t Loss 0.3851 \t Acc 87.94 \t AccHead 90.28 \t AccTail 63.56\n",
      "Epoch: [065] \t Loss 0.3739 \t Acc 88.46 \t AccHead 90.41 \t AccTail 68.17\n",
      "Epoch: [066] \t Loss 0.3633 \t Acc 88.16 \t AccHead 90.67 \t AccTail 62.13\n",
      "Epoch: [067] \t Loss 0.3621 \t Acc 88.04 \t AccHead 90.87 \t AccTail 58.68\n",
      "Epoch: [068] \t Loss 0.3585 \t Acc 87.33 \t AccHead 89.04 \t AccTail 69.56\n",
      "Epoch: [069] \t Loss 0.3535 \t Acc 88.88 \t AccHead 91.77 \t AccTail 58.89\n",
      "Epoch: [070] \t Loss 0.3627 \t Acc 87.82 \t AccHead 91.15 \t AccTail 53.23\n",
      "Epoch: [071] \t Loss 0.3479 \t Acc 89.59 \t AccHead 91.88 \t AccTail 65.82\n",
      "Epoch: [072] \t Loss 0.3480 \t Acc 89.43 \t AccHead 91.82 \t AccTail 64.66\n",
      "Epoch: [073] \t Loss 0.3411 \t Acc 89.38 \t AccHead 91.53 \t AccTail 67.07\n",
      "Epoch: [074] \t Loss 0.3368 \t Acc 89.14 \t AccHead 90.92 \t AccTail 70.67\n",
      "Epoch: [075] \t Loss 0.3290 \t Acc 88.72 \t AccHead 91.11 \t AccTail 64.02\n",
      "Epoch: [076] \t Loss 0.3211 \t Acc 88.34 \t AccHead 90.40 \t AccTail 66.89\n",
      "Epoch: [077] \t Loss 0.3238 \t Acc 90.07 \t AccHead 92.49 \t AccTail 64.95\n",
      "Epoch: [078] \t Loss 0.3102 \t Acc 89.82 \t AccHead 91.88 \t AccTail 68.37\n",
      "Epoch: [079] \t Loss 0.3105 \t Acc 89.24 \t AccHead 91.25 \t AccTail 68.31\n",
      "Epoch: [080] \t Loss 0.3163 \t Acc 89.88 \t AccHead 92.57 \t AccTail 61.99\n",
      "Epoch: [081] \t Loss 0.3097 \t Acc 90.14 \t AccHead 91.67 \t AccTail 74.23\n",
      "Epoch: [082] \t Loss 0.2985 \t Acc 89.95 \t AccHead 91.70 \t AccTail 71.81\n",
      "Epoch: [083] \t Loss 0.2993 \t Acc 90.72 \t AccHead 92.56 \t AccTail 71.58\n",
      "Epoch: [084] \t Loss 0.2914 \t Acc 90.07 \t AccHead 91.94 \t AccTail 70.68\n",
      "Epoch: [085] \t Loss 0.2852 \t Acc 89.09 \t AccHead 90.57 \t AccTail 73.73\n",
      "Epoch: [086] \t Loss 0.2889 \t Acc 90.74 \t AccHead 92.82 \t AccTail 69.13\n",
      "Epoch: [087] \t Loss 0.2945 \t Acc 90.47 \t AccHead 92.39 \t AccTail 70.58\n",
      "Epoch: [088] \t Loss 0.2829 \t Acc 91.17 \t AccHead 93.42 \t AccTail 67.71\n",
      "Epoch: [089] \t Loss 0.2787 \t Acc 91.99 \t AccHead 93.99 \t AccTail 71.24\n",
      "Epoch: [090] \t Loss 0.2734 \t Acc 91.26 \t AccHead 93.30 \t AccTail 70.08\n",
      "89\n",
      "Finished Training\n",
      "Acc 60.88 \t AccHead 81.36 \t AccTail 39.80\n",
      "----------------------------\n",
      "get the top-1 accuracy for each of the classes.\n",
      "Accuracy of airplane : 90 %\n",
      "Accuracy of automobile : 89 %\n",
      "Accuracy of  bird : 77 %\n",
      "Accuracy of   cat : 70 %\n",
      "Accuracy of  deer : 79 %\n",
      "Accuracy of   dog : 15 %\n",
      "Accuracy of  frog : 29 %\n",
      "Accuracy of horse : 48 %\n",
      "Accuracy of  ship : 48 %\n",
      "Accuracy of truck : 60 %\n",
      "0.6087934655775963\n"
     ]
    }
   ],
   "source": [
    "train_dataset, train_loader, test_loader, num_classes = get_loaders()\n",
    "model = ResNet18(num_classes)\n",
    "model = model.cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=150, gamma=0.1)\n",
    "\n",
    "# Let's build our model\n",
    "train(EPOCHS)\n",
    "print('Finished Training')\n",
    "test() \n",
    "print('----------------------------')\n",
    "print('get the top-1 accuracy for each of the classes.')\n",
    "avg_accu = testEeahClass()\n",
    "tau = np.mean(np.array(avg_accu)) / 100\n",
    "print(tau)\n",
    "tau_list.append(tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5o1czesULQTw",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-22T16:06:42.739595Z",
     "iopub.status.busy": "2022-06-22T16:06:42.739133Z",
     "iopub.status.idle": "2022-06-22T16:06:42.748390Z",
     "shell.execute_reply": "2022-06-22T16:06:42.747695Z",
     "shell.execute_reply.started": "2022-06-22T16:06:42.739537Z"
    },
    "id": "5o1czesULQTw"
   },
   "outputs": [],
   "source": [
    "DATASET = 'CIFAR10' #['CIFAR10', 'CIFAR100']\n",
    "IMB_TYPE = 'step' #['exp', 'step']\n",
    "IMB_FACTOR = 0.01 #[0.1, 0.01]\n",
    "SAVE_DIR = 'logs/10-step-0.01-find_tau' \n",
    "\n",
    "LR = 0.1\n",
    "BATCH_SIZE = 128\n",
    "MOMENTUM = 0.9\n",
    "WEIGHT_DECAY = 0.000288\n",
    "EPOCHS = 200\n",
    "\n",
    "os.makedirs(osp.join(SAVE_DIR), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "YILwKmiPPdcp",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-22T16:06:42.750003Z",
     "iopub.status.busy": "2022-06-22T16:06:42.749588Z",
     "iopub.status.idle": "2022-06-22T16:35:18.626660Z",
     "shell.execute_reply": "2022-06-22T16:35:18.626031Z",
     "shell.execute_reply.started": "2022-06-22T16:06:42.749968Z"
    },
    "id": "YILwKmiPPdcp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "cls num list(train_dataset):\n",
      "[4000, 4000, 4000, 4000, 4000, 40, 40, 40, 40, 40]\n",
      "cls num list(val_dataset):\n",
      "[1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000]\n",
      "Epoch: [001] \t Loss 2.1632 \t Acc 37.66 \t AccHead 37.96 \t AccTail 0.00\n",
      "Epoch: [002] \t Loss 1.3774 \t Acc 52.71 \t AccHead 53.14 \t AccTail 0.00\n",
      "Epoch: [003] \t Loss 1.1608 \t Acc 57.45 \t AccHead 57.92 \t AccTail 0.00\n",
      "Epoch: [004] \t Loss 1.0665 \t Acc 56.93 \t AccHead 57.38 \t AccTail 0.00\n",
      "Epoch: [005] \t Loss 1.0026 \t Acc 61.13 \t AccHead 61.62 \t AccTail 0.00\n",
      "Epoch: [006] \t Loss 0.9510 \t Acc 60.30 \t AccHead 60.79 \t AccTail 0.00\n",
      "Epoch: [007] \t Loss 0.9024 \t Acc 68.34 \t AccHead 68.89 \t AccTail 0.00\n",
      "Epoch: [008] \t Loss 0.8420 \t Acc 69.78 \t AccHead 70.35 \t AccTail 0.00\n",
      "Epoch: [009] \t Loss 0.8027 \t Acc 72.70 \t AccHead 73.29 \t AccTail 0.00\n",
      "Epoch: [010] \t Loss 0.7523 \t Acc 73.50 \t AccHead 74.09 \t AccTail 0.00\n",
      "Epoch: [011] \t Loss 0.7227 \t Acc 75.05 \t AccHead 75.66 \t AccTail 0.00\n",
      "Epoch: [012] \t Loss 0.6847 \t Acc 77.58 \t AccHead 78.21 \t AccTail 0.00\n",
      "Epoch: [013] \t Loss 0.6661 \t Acc 76.97 \t AccHead 77.60 \t AccTail 0.00\n",
      "Epoch: [014] \t Loss 0.6425 \t Acc 77.64 \t AccHead 78.27 \t AccTail 0.00\n",
      "Epoch: [015] \t Loss 0.6292 \t Acc 77.62 \t AccHead 78.24 \t AccTail 0.00\n",
      "Epoch: [016] \t Loss 0.6054 \t Acc 79.90 \t AccHead 80.54 \t AccTail 0.00\n",
      "Epoch: [017] \t Loss 0.5856 \t Acc 80.57 \t AccHead 81.23 \t AccTail 0.00\n",
      "Epoch: [018] \t Loss 0.5738 \t Acc 80.21 \t AccHead 80.86 \t AccTail 0.00\n",
      "Epoch: [019] \t Loss 0.5523 \t Acc 79.10 \t AccHead 79.74 \t AccTail 0.00\n",
      "Epoch: [020] \t Loss 0.5525 \t Acc 81.96 \t AccHead 82.62 \t AccTail 0.00\n",
      "Epoch: [021] \t Loss 0.5395 \t Acc 81.65 \t AccHead 82.32 \t AccTail 0.00\n",
      "Epoch: [022] \t Loss 0.5431 \t Acc 81.24 \t AccHead 81.89 \t AccTail 0.00\n",
      "Epoch: [023] \t Loss 0.5128 \t Acc 80.85 \t AccHead 81.51 \t AccTail 0.00\n",
      "Epoch: [024] \t Loss 0.5074 \t Acc 81.70 \t AccHead 82.36 \t AccTail 0.00\n",
      "Epoch: [025] \t Loss 0.4923 \t Acc 81.85 \t AccHead 82.51 \t AccTail 0.00\n",
      "Epoch: [026] \t Loss 0.4922 \t Acc 80.46 \t AccHead 81.12 \t AccTail 0.00\n",
      "Epoch: [027] \t Loss 0.4803 \t Acc 80.52 \t AccHead 81.18 \t AccTail 0.00\n",
      "Epoch: [028] \t Loss 0.4806 \t Acc 83.13 \t AccHead 83.81 \t AccTail 0.00\n",
      "Epoch: [029] \t Loss 0.4720 \t Acc 84.49 \t AccHead 85.18 \t AccTail 0.00\n",
      "Epoch: [030] \t Loss 0.4643 \t Acc 82.88 \t AccHead 83.56 \t AccTail 0.00\n",
      "Epoch: [031] \t Loss 0.4572 \t Acc 84.34 \t AccHead 85.03 \t AccTail 0.00\n",
      "Epoch: [032] \t Loss 0.4528 \t Acc 79.23 \t AccHead 79.88 \t AccTail 0.00\n",
      "Epoch: [033] \t Loss 0.4454 \t Acc 83.98 \t AccHead 84.66 \t AccTail 0.00\n",
      "Epoch: [034] \t Loss 0.4417 \t Acc 86.00 \t AccHead 86.70 \t AccTail 0.00\n",
      "Epoch: [035] \t Loss 0.4392 \t Acc 84.36 \t AccHead 85.04 \t AccTail 0.00\n",
      "Epoch: [036] \t Loss 0.4248 \t Acc 86.60 \t AccHead 87.30 \t AccTail 0.00\n",
      "Epoch: [037] \t Loss 0.4308 \t Acc 86.14 \t AccHead 86.78 \t AccTail 5.06\n",
      "Epoch: [038] \t Loss 0.4162 \t Acc 85.46 \t AccHead 86.14 \t AccTail 1.85\n",
      "Epoch: [039] \t Loss 0.4213 \t Acc 85.73 \t AccHead 86.42 \t AccTail 0.00\n",
      "Epoch: [040] \t Loss 0.4190 \t Acc 84.16 \t AccHead 84.84 \t AccTail 0.00\n",
      "Epoch: [041] \t Loss 0.4078 \t Acc 86.16 \t AccHead 86.81 \t AccTail 4.97\n",
      "Epoch: [042] \t Loss 0.4154 \t Acc 86.31 \t AccHead 86.98 \t AccTail 3.70\n",
      "Epoch: [043] \t Loss 0.4047 \t Acc 86.72 \t AccHead 87.41 \t AccTail 1.85\n",
      "Epoch: [044] \t Loss 0.4051 \t Acc 87.08 \t AccHead 87.73 \t AccTail 7.41\n",
      "Epoch: [045] \t Loss 0.4046 \t Acc 83.88 \t AccHead 84.53 \t AccTail 3.73\n",
      "Epoch: [046] \t Loss 0.4003 \t Acc 84.57 \t AccHead 85.19 \t AccTail 7.50\n",
      "Epoch: [047] \t Loss 0.3924 \t Acc 86.35 \t AccHead 86.95 \t AccTail 11.73\n",
      "Epoch: [048] \t Loss 0.3853 \t Acc 84.50 \t AccHead 85.09 \t AccTail 12.96\n",
      "Epoch: [049] \t Loss 0.3895 \t Acc 85.67 \t AccHead 86.28 \t AccTail 11.18\n",
      "Epoch: [050] \t Loss 0.3842 \t Acc 88.42 \t AccHead 88.93 \t AccTail 24.22\n",
      "Epoch: [051] \t Loss 0.3704 \t Acc 87.72 \t AccHead 88.27 \t AccTail 20.37\n",
      "Epoch: [052] \t Loss 0.3773 \t Acc 87.23 \t AccHead 87.81 \t AccTail 14.91\n",
      "Epoch: [053] \t Loss 0.3804 \t Acc 87.51 \t AccHead 88.11 \t AccTail 13.66\n",
      "Epoch: [054] \t Loss 0.3728 \t Acc 87.90 \t AccHead 88.46 \t AccTail 19.14\n",
      "Epoch: [055] \t Loss 0.3698 \t Acc 86.82 \t AccHead 87.42 \t AccTail 12.96\n",
      "Epoch: [056] \t Loss 0.3698 \t Acc 88.27 \t AccHead 88.80 \t AccTail 22.36\n",
      "Epoch: [057] \t Loss 0.3620 \t Acc 87.65 \t AccHead 88.15 \t AccTail 26.09\n",
      "Epoch: [058] \t Loss 0.3620 \t Acc 89.01 \t AccHead 89.61 \t AccTail 14.91\n",
      "Epoch: [059] \t Loss 0.3576 \t Acc 88.42 \t AccHead 89.03 \t AccTail 13.04\n",
      "Epoch: [060] \t Loss 0.3555 \t Acc 88.43 \t AccHead 88.94 \t AccTail 25.93\n",
      "Epoch: [061] \t Loss 0.3554 \t Acc 89.03 \t AccHead 89.53 \t AccTail 27.95\n",
      "Epoch: [062] \t Loss 0.3503 \t Acc 88.74 \t AccHead 89.21 \t AccTail 30.86\n",
      "Epoch: [063] \t Loss 0.3513 \t Acc 88.97 \t AccHead 89.49 \t AccTail 24.84\n",
      "Epoch: [064] \t Loss 0.3371 \t Acc 88.57 \t AccHead 88.89 \t AccTail 49.07\n",
      "Epoch: [065] \t Loss 0.3447 \t Acc 88.79 \t AccHead 89.32 \t AccTail 23.46\n",
      "Epoch: [066] \t Loss 0.3391 \t Acc 89.01 \t AccHead 89.45 \t AccTail 35.80\n",
      "Epoch: [067] \t Loss 0.3439 \t Acc 86.71 \t AccHead 87.13 \t AccTail 34.38\n",
      "Epoch: [068] \t Loss 0.3432 \t Acc 89.00 \t AccHead 89.62 \t AccTail 12.35\n",
      "Epoch: [069] \t Loss 0.3456 \t Acc 88.38 \t AccHead 88.83 \t AccTail 32.30\n",
      "Epoch: [070] \t Loss 0.3384 \t Acc 89.40 \t AccHead 89.90 \t AccTail 27.95\n",
      "Epoch: [071] \t Loss 0.3422 \t Acc 88.36 \t AccHead 88.86 \t AccTail 25.62\n",
      "Epoch: [072] \t Loss 0.3401 \t Acc 88.36 \t AccHead 88.87 \t AccTail 24.22\n",
      "Epoch: [073] \t Loss 0.3320 \t Acc 88.34 \t AccHead 88.74 \t AccTail 39.51\n",
      "Epoch: [074] \t Loss 0.3448 \t Acc 87.66 \t AccHead 88.06 \t AccTail 38.89\n",
      "Epoch: [075] \t Loss 0.3329 \t Acc 89.02 \t AccHead 89.52 \t AccTail 27.78\n",
      "Epoch: [076] \t Loss 0.3307 \t Acc 88.41 \t AccHead 88.75 \t AccTail 46.91\n",
      "Epoch: [077] \t Loss 0.3234 \t Acc 87.72 \t AccHead 88.08 \t AccTail 43.48\n",
      "Epoch: [078] \t Loss 0.3265 \t Acc 87.14 \t AccHead 87.54 \t AccTail 37.27\n",
      "Epoch: [079] \t Loss 0.3333 \t Acc 87.74 \t AccHead 88.13 \t AccTail 39.51\n",
      "Epoch: [080] \t Loss 0.3261 \t Acc 89.32 \t AccHead 89.76 \t AccTail 35.19\n",
      "Epoch: [081] \t Loss 0.3222 \t Acc 88.41 \t AccHead 88.91 \t AccTail 26.54\n",
      "Epoch: [082] \t Loss 0.3165 \t Acc 89.88 \t AccHead 90.31 \t AccTail 36.02\n",
      "Epoch: [083] \t Loss 0.3241 \t Acc 88.91 \t AccHead 89.21 \t AccTail 52.47\n",
      "Epoch: [084] \t Loss 0.3304 \t Acc 83.62 \t AccHead 84.02 \t AccTail 33.75\n",
      "Epoch: [085] \t Loss 0.3274 \t Acc 89.99 \t AccHead 90.45 \t AccTail 33.54\n",
      "Epoch: [086] \t Loss 0.3269 \t Acc 86.83 \t AccHead 87.16 \t AccTail 46.30\n",
      "Epoch: [087] \t Loss 0.3085 \t Acc 89.31 \t AccHead 89.79 \t AccTail 29.81\n",
      "Epoch: [088] \t Loss 0.3289 \t Acc 88.26 \t AccHead 88.76 \t AccTail 25.93\n",
      "Epoch: [089] \t Loss 0.3191 \t Acc 89.34 \t AccHead 89.83 \t AccTail 28.57\n",
      "Epoch: [090] \t Loss 0.3155 \t Acc 88.15 \t AccHead 88.50 \t AccTail 44.10\n",
      "Epoch: [091] \t Loss 0.3173 \t Acc 88.56 \t AccHead 88.89 \t AccTail 48.45\n",
      "Epoch: [092] \t Loss 0.3079 \t Acc 89.24 \t AccHead 89.75 \t AccTail 27.16\n",
      "Epoch: [093] \t Loss 0.3066 \t Acc 89.58 \t AccHead 89.89 \t AccTail 51.55\n",
      "Epoch: [094] \t Loss 0.3122 \t Acc 89.42 \t AccHead 89.87 \t AccTail 34.57\n",
      "Epoch: [095] \t Loss 0.3099 \t Acc 89.74 \t AccHead 90.08 \t AccTail 47.20\n",
      "Epoch: [096] \t Loss 0.3136 \t Acc 88.15 \t AccHead 88.49 \t AccTail 46.30\n",
      "Epoch: [097] \t Loss 0.3137 \t Acc 87.55 \t AccHead 88.00 \t AccTail 31.88\n",
      "Epoch: [098] \t Loss 0.3053 \t Acc 87.17 \t AccHead 87.56 \t AccTail 38.27\n",
      "Epoch: [099] \t Loss 0.3152 \t Acc 88.82 \t AccHead 89.34 \t AccTail 25.31\n",
      "Epoch: [100] \t Loss 0.3009 \t Acc 90.62 \t AccHead 91.02 \t AccTail 42.59\n",
      "Epoch: [101] \t Loss 0.3067 \t Acc 90.81 \t AccHead 91.12 \t AccTail 52.80\n",
      "Epoch: [102] \t Loss 0.3088 \t Acc 88.89 \t AccHead 89.17 \t AccTail 54.66\n",
      "Epoch: [103] \t Loss 0.2990 \t Acc 89.89 \t AccHead 90.14 \t AccTail 59.88\n",
      "Epoch: [104] \t Loss 0.3056 \t Acc 86.53 \t AccHead 86.94 \t AccTail 35.80\n",
      "Epoch: [105] \t Loss 0.3147 \t Acc 90.11 \t AccHead 90.50 \t AccTail 42.59\n",
      "Epoch: [106] \t Loss 0.3077 \t Acc 87.57 \t AccHead 88.01 \t AccTail 32.92\n",
      "Epoch: [107] \t Loss 0.3048 \t Acc 89.88 \t AccHead 90.28 \t AccTail 40.74\n",
      "Epoch: [108] \t Loss 0.3050 \t Acc 89.80 \t AccHead 90.19 \t AccTail 41.98\n",
      "Epoch: [109] \t Loss 0.2975 \t Acc 89.07 \t AccHead 89.47 \t AccTail 39.75\n",
      "Epoch: [110] \t Loss 0.3162 \t Acc 89.76 \t AccHead 90.09 \t AccTail 49.38\n",
      "Epoch: [111] \t Loss 0.3136 \t Acc 88.33 \t AccHead 88.81 \t AccTail 29.63\n",
      "Epoch: [112] \t Loss 0.2959 \t Acc 90.04 \t AccHead 90.42 \t AccTail 43.21\n",
      "Epoch: [113] \t Loss 0.3114 \t Acc 89.17 \t AccHead 89.61 \t AccTail 35.40\n",
      "Epoch: [114] \t Loss 0.2986 \t Acc 88.29 \t AccHead 88.66 \t AccTail 42.59\n",
      "Epoch: [115] \t Loss 0.3001 \t Acc 89.88 \t AccHead 90.22 \t AccTail 48.15\n",
      "Epoch: [116] \t Loss 0.2925 \t Acc 87.67 \t AccHead 88.06 \t AccTail 39.38\n",
      "Epoch: [117] \t Loss 0.2956 \t Acc 90.07 \t AccHead 90.42 \t AccTail 46.91\n",
      "Epoch: [118] \t Loss 0.3030 \t Acc 90.94 \t AccHead 91.30 \t AccTail 46.91\n",
      "Epoch: [119] \t Loss 0.3040 \t Acc 84.52 \t AccHead 84.97 \t AccTail 29.63\n",
      "Epoch: [120] \t Loss 0.2981 \t Acc 89.04 \t AccHead 89.35 \t AccTail 50.00\n",
      "Epoch: [121] \t Loss 0.2955 \t Acc 90.48 \t AccHead 90.87 \t AccTail 41.98\n",
      "Epoch: [122] \t Loss 0.2889 \t Acc 89.55 \t AccHead 89.92 \t AccTail 43.12\n",
      "Epoch: [123] \t Loss 0.2967 \t Acc 88.08 \t AccHead 88.38 \t AccTail 50.31\n",
      "Epoch: [124] \t Loss 0.3109 \t Acc 89.44 \t AccHead 89.81 \t AccTail 44.44\n",
      "Epoch: [125] \t Loss 0.2911 \t Acc 88.63 \t AccHead 89.04 \t AccTail 37.89\n",
      "Epoch: [126] \t Loss 0.2888 \t Acc 90.96 \t AccHead 91.31 \t AccTail 47.83\n",
      "Epoch: [127] \t Loss 0.2907 \t Acc 89.11 \t AccHead 89.47 \t AccTail 44.44\n",
      "Epoch: [128] \t Loss 0.2870 \t Acc 90.34 \t AccHead 90.72 \t AccTail 43.83\n",
      "Epoch: [129] \t Loss 0.2977 \t Acc 89.06 \t AccHead 89.47 \t AccTail 37.89\n",
      "Epoch: [130] \t Loss 0.2921 \t Acc 90.57 \t AccHead 90.85 \t AccTail 55.56\n",
      "Epoch: [131] \t Loss 0.2897 \t Acc 90.62 \t AccHead 90.98 \t AccTail 45.34\n",
      "Epoch: [132] \t Loss 0.2874 \t Acc 90.76 \t AccHead 91.03 \t AccTail 58.02\n",
      "Epoch: [133] \t Loss 0.2959 \t Acc 87.86 \t AccHead 88.15 \t AccTail 51.85\n",
      "Epoch: [134] \t Loss 0.2887 \t Acc 90.51 \t AccHead 90.91 \t AccTail 40.99\n",
      "Epoch: [135] \t Loss 0.2904 \t Acc 89.51 \t AccHead 89.85 \t AccTail 47.83\n",
      "Epoch: [136] \t Loss 0.2859 \t Acc 91.13 \t AccHead 91.42 \t AccTail 55.28\n",
      "Epoch: [137] \t Loss 0.2757 \t Acc 89.03 \t AccHead 89.46 \t AccTail 36.42\n",
      "Epoch: [138] \t Loss 0.2912 \t Acc 90.19 \t AccHead 90.59 \t AccTail 40.37\n",
      "Epoch: [139] \t Loss 0.2797 \t Acc 88.94 \t AccHead 89.25 \t AccTail 50.00\n",
      "Epoch: [140] \t Loss 0.2890 \t Acc 86.46 \t AccHead 86.62 \t AccTail 66.46\n",
      "Epoch: [141] \t Loss 0.2865 \t Acc 91.39 \t AccHead 91.74 \t AccTail 48.15\n",
      "Epoch: [142] \t Loss 0.2908 \t Acc 89.96 \t AccHead 90.45 \t AccTail 29.63\n",
      "Epoch: [143] \t Loss 0.2893 \t Acc 90.67 \t AccHead 91.03 \t AccTail 46.58\n",
      "Epoch: [144] \t Loss 0.2802 \t Acc 89.96 \t AccHead 90.31 \t AccTail 46.91\n",
      "Epoch: [145] \t Loss 0.2893 \t Acc 88.65 \t AccHead 89.14 \t AccTail 29.01\n",
      "Epoch: [146] \t Loss 0.2837 \t Acc 90.76 \t AccHead 91.10 \t AccTail 49.38\n",
      "Epoch: [147] \t Loss 0.2941 \t Acc 87.00 \t AccHead 87.40 \t AccTail 37.27\n",
      "Epoch: [148] \t Loss 0.2824 \t Acc 89.97 \t AccHead 90.26 \t AccTail 54.32\n",
      "Epoch: [149] \t Loss 0.2920 \t Acc 85.63 \t AccHead 86.00 \t AccTail 40.12\n",
      "Epoch: [150] \t Loss 0.2895 \t Acc 90.59 \t AccHead 90.90 \t AccTail 51.55\n",
      "Epoch: [151] \t Loss 0.1878 \t Acc 95.36 \t AccHead 95.54 \t AccTail 73.29\n",
      "Epoch: [152] \t Loss 0.1440 \t Acc 96.33 \t AccHead 96.52 \t AccTail 72.84\n",
      "Epoch: [153] \t Loss 0.1268 \t Acc 96.62 \t AccHead 96.79 \t AccTail 75.78\n",
      "Epoch: [154] \t Loss 0.1178 \t Acc 97.14 \t AccHead 97.28 \t AccTail 80.86\n",
      "Epoch: [155] \t Loss 0.1051 \t Acc 97.20 \t AccHead 97.29 \t AccTail 86.34\n",
      "Epoch: [156] \t Loss 0.1034 \t Acc 97.34 \t AccHead 97.46 \t AccTail 83.23\n",
      "Epoch: [157] \t Loss 0.0958 \t Acc 97.49 \t AccHead 97.57 \t AccTail 86.88\n",
      "Epoch: [158] \t Loss 0.0920 \t Acc 97.74 \t AccHead 97.83 \t AccTail 85.80\n",
      "Epoch: [159] \t Loss 0.0854 \t Acc 97.69 \t AccHead 97.79 \t AccTail 85.71\n",
      "Epoch: [160] \t Loss 0.0849 \t Acc 97.97 \t AccHead 98.04 \t AccTail 88.82\n",
      "Epoch: [161] \t Loss 0.0820 \t Acc 98.07 \t AccHead 98.11 \t AccTail 93.83\n",
      "Epoch: [162] \t Loss 0.0769 \t Acc 98.19 \t AccHead 98.29 \t AccTail 86.34\n",
      "Epoch: [163] \t Loss 0.0747 \t Acc 98.01 \t AccHead 98.08 \t AccTail 89.51\n",
      "Epoch: [164] \t Loss 0.0716 \t Acc 98.25 \t AccHead 98.31 \t AccTail 91.36\n",
      "Epoch: [165] \t Loss 0.0670 \t Acc 98.41 \t AccHead 98.47 \t AccTail 91.30\n",
      "Epoch: [166] \t Loss 0.0646 \t Acc 98.48 \t AccHead 98.54 \t AccTail 90.74\n",
      "Epoch: [167] \t Loss 0.0652 \t Acc 98.51 \t AccHead 98.55 \t AccTail 93.21\n",
      "Epoch: [168] \t Loss 0.0615 \t Acc 98.50 \t AccHead 98.56 \t AccTail 91.36\n",
      "Epoch: [169] \t Loss 0.0579 \t Acc 98.55 \t AccHead 98.61 \t AccTail 91.36\n",
      "Epoch: [170] \t Loss 0.0613 \t Acc 98.43 \t AccHead 98.48 \t AccTail 92.55\n",
      "Epoch: [171] \t Loss 0.0541 \t Acc 98.63 \t AccHead 98.67 \t AccTail 93.79\n",
      "Epoch: [172] \t Loss 0.0580 \t Acc 98.75 \t AccHead 98.81 \t AccTail 91.98\n",
      "Epoch: [173] \t Loss 0.0570 \t Acc 98.70 \t AccHead 98.74 \t AccTail 93.79\n",
      "Epoch: [174] \t Loss 0.0532 \t Acc 98.66 \t AccHead 98.70 \t AccTail 94.44\n",
      "Epoch: [175] \t Loss 0.0501 \t Acc 98.96 \t AccHead 99.00 \t AccTail 93.83\n",
      "Epoch: [176] \t Loss 0.0472 \t Acc 98.78 \t AccHead 98.82 \t AccTail 93.75\n",
      "Epoch: [177] \t Loss 0.0491 \t Acc 98.85 \t AccHead 98.91 \t AccTail 90.74\n",
      "Epoch: [178] \t Loss 0.0504 \t Acc 98.88 \t AccHead 98.95 \t AccTail 90.06\n",
      "Epoch: [179] \t Loss 0.0475 \t Acc 98.67 \t AccHead 98.71 \t AccTail 93.83\n",
      "Epoch: [180] \t Loss 0.0488 \t Acc 98.94 \t AccHead 98.98 \t AccTail 93.21\n",
      "Epoch: [181] \t Loss 0.0458 \t Acc 98.94 \t AccHead 98.97 \t AccTail 94.44\n",
      "Epoch: [182] \t Loss 0.0442 \t Acc 98.81 \t AccHead 98.84 \t AccTail 95.06\n",
      "Epoch: [183] \t Loss 0.0456 \t Acc 99.15 \t AccHead 99.19 \t AccTail 95.03\n",
      "Epoch: [184] \t Loss 0.0364 \t Acc 99.09 \t AccHead 99.13 \t AccTail 94.44\n",
      "Epoch: [185] \t Loss 0.0383 \t Acc 98.98 \t AccHead 99.02 \t AccTail 94.38\n",
      "Epoch: [186] \t Loss 0.0434 \t Acc 98.84 \t AccHead 98.89 \t AccTail 93.21\n",
      "Epoch: [187] \t Loss 0.0438 \t Acc 99.11 \t AccHead 99.14 \t AccTail 95.06\n",
      "Epoch: [188] \t Loss 0.0379 \t Acc 99.13 \t AccHead 99.16 \t AccTail 95.68\n",
      "Epoch: [189] \t Loss 0.0357 \t Acc 99.06 \t AccHead 99.08 \t AccTail 97.53\n",
      "Epoch: [190] \t Loss 0.0380 \t Acc 99.13 \t AccHead 99.16 \t AccTail 95.06\n",
      "Epoch: [191] \t Loss 0.0375 \t Acc 99.12 \t AccHead 99.15 \t AccTail 96.30\n",
      "Epoch: [192] \t Loss 0.0372 \t Acc 99.05 \t AccHead 99.10 \t AccTail 93.21\n",
      "Epoch: [193] \t Loss 0.0391 \t Acc 98.98 \t AccHead 99.02 \t AccTail 93.79\n",
      "Epoch: [194] \t Loss 0.0369 \t Acc 99.16 \t AccHead 99.17 \t AccTail 98.15\n",
      "Epoch: [195] \t Loss 0.0378 \t Acc 99.31 \t AccHead 99.33 \t AccTail 96.30\n",
      "Epoch: [196] \t Loss 0.0364 \t Acc 99.16 \t AccHead 99.20 \t AccTail 95.06\n",
      "Epoch: [197] \t Loss 0.0328 \t Acc 99.29 \t AccHead 99.31 \t AccTail 96.27\n",
      "Epoch: [198] \t Loss 0.0306 \t Acc 99.14 \t AccHead 99.18 \t AccTail 95.06\n",
      "Epoch: [199] \t Loss 0.0371 \t Acc 99.03 \t AccHead 99.07 \t AccTail 95.06\n",
      "Epoch: [200] \t Loss 0.0354 \t Acc 99.15 \t AccHead 99.19 \t AccTail 94.44\n",
      "199\n",
      "Finished Training\n",
      "Acc 44.72 \t AccHead 87.40 \t AccTail 1.32\n",
      "----------------------------\n",
      "get the top-1 accuracy for each of the classes.\n",
      "Accuracy of airplane : 93 %\n",
      "Accuracy of automobile : 95 %\n",
      "Accuracy of  bird : 83 %\n",
      "Accuracy of   cat : 79 %\n",
      "Accuracy of  deer : 84 %\n",
      "Accuracy of   dog :  1 %\n",
      "Accuracy of  frog :  2 %\n",
      "Accuracy of horse :  1 %\n",
      "Accuracy of  ship :  1 %\n",
      "Accuracy of truck :  0 %\n",
      "0.44349999999999995\n"
     ]
    }
   ],
   "source": [
    "train_dataset, train_loader, test_loader, num_classes = get_loaders()\n",
    "model = ResNet18(num_classes)\n",
    "model = model.cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=150, gamma=0.1)\n",
    "\n",
    "# Let's build our model\n",
    "train(EPOCHS)\n",
    "print('Finished Training')\n",
    "test() \n",
    "print('----------------------------')\n",
    "print('get the top-1 accuracy for each of the classes.')\n",
    "avg_accu = testEeahClass()\n",
    "tau = np.mean(np.array(avg_accu)) / 100\n",
    "print(tau)\n",
    "tau_list.append(tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "IenEd8-hLQds",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-22T16:35:18.628401Z",
     "iopub.status.busy": "2022-06-22T16:35:18.628165Z",
     "iopub.status.idle": "2022-06-22T16:35:18.634323Z",
     "shell.execute_reply": "2022-06-22T16:35:18.633682Z",
     "shell.execute_reply.started": "2022-06-22T16:35:18.628371Z"
    },
    "id": "IenEd8-hLQds",
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATASET = 'CIFAR100' #['CIFAR10', 'CIFAR100']\n",
    "IMB_TYPE = 'exp' #['exp', 'step']\n",
    "IMB_FACTOR = 0.1 #[0.1, 0.01]\n",
    "SAVE_DIR = 'logs/100-exp-0.1-find_tau' \n",
    "\n",
    "LR = 0.1\n",
    "BATCH_SIZE = 128\n",
    "MOMENTUM = 0.9\n",
    "WEIGHT_DECAY = 0.001405\n",
    "EPOCHS = 200\n",
    "\n",
    "os.makedirs(osp.join(SAVE_DIR), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "MyM5shyEPd-S",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2022-06-22T16:35:18.636868Z",
     "iopub.status.busy": "2022-06-22T16:35:18.636582Z",
     "iopub.status.idle": "2022-06-22T16:59:03.088995Z",
     "shell.execute_reply": "2022-06-22T16:59:03.087273Z",
     "shell.execute_reply.started": "2022-06-22T16:35:18.636842Z"
    },
    "id": "MyM5shyEPd-S",
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "cls num list(train_dataset):\n",
      "[400, 390, 381, 373, 364, 356, 347, 339, 332, 324, 316, 309, 302, 295, 288, 282, 275, 269, 263, 257, 251, 245, 239, 234, 228, 223, 218, 213, 208, 203, 199, 194, 190, 185, 181, 177, 173, 169, 165, 161, 157, 154, 150, 147, 143, 140, 137, 134, 130, 127, 125, 122, 119, 116, 113, 111, 108, 106, 103, 101, 99, 96, 94, 92, 90, 88, 86, 84, 82, 80, 78, 76, 74, 73, 71, 69, 68, 66, 65, 63, 62, 60, 59, 58, 56, 55, 54, 52, 51, 50, 49, 48, 47, 45, 44, 43, 42, 41, 40, 40]\n",
      "cls num list(val_dataset):\n",
      "[100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100]\n",
      "Epoch: [001] \t Loss 4.5675 \t Acc 8.93 \t AccHead 11.42 \t AccTail 0.89\n",
      "Epoch: [002] \t Loss 3.9315 \t Acc 11.67 \t AccHead 15.21 \t AccTail 0.24\n",
      "Epoch: [003] \t Loss 3.7504 \t Acc 15.29 \t AccHead 18.79 \t AccTail 3.98\n",
      "Epoch: [004] \t Loss 3.6358 \t Acc 16.05 \t AccHead 20.19 \t AccTail 2.71\n",
      "Epoch: [005] \t Loss 3.5141 \t Acc 19.25 \t AccHead 23.39 \t AccTail 5.88\n",
      "Epoch: [006] \t Loss 3.4110 \t Acc 20.93 \t AccHead 25.49 \t AccTail 6.20\n",
      "Epoch: [007] \t Loss 3.3195 \t Acc 21.88 \t AccHead 26.17 \t AccTail 7.99\n",
      "Epoch: [008] \t Loss 3.2448 \t Acc 23.14 \t AccHead 28.42 \t AccTail 6.09\n",
      "Epoch: [009] \t Loss 3.1890 \t Acc 24.81 \t AccHead 29.72 \t AccTail 8.96\n",
      "Epoch: [010] \t Loss 3.1314 \t Acc 24.55 \t AccHead 28.69 \t AccTail 11.16\n",
      "Epoch: [011] \t Loss 3.0667 \t Acc 24.08 \t AccHead 28.84 \t AccTail 8.72\n",
      "Epoch: [012] \t Loss 3.0175 \t Acc 28.46 \t AccHead 34.14 \t AccTail 10.13\n",
      "Epoch: [013] \t Loss 2.9657 \t Acc 28.00 \t AccHead 32.63 \t AccTail 13.05\n",
      "Epoch: [014] \t Loss 2.9358 \t Acc 26.83 \t AccHead 30.92 \t AccTail 13.63\n",
      "Epoch: [015] \t Loss 2.8995 \t Acc 26.73 \t AccHead 31.40 \t AccTail 11.65\n",
      "Epoch: [016] \t Loss 2.8835 \t Acc 29.79 \t AccHead 35.32 \t AccTail 11.94\n",
      "Epoch: [017] \t Loss 2.8459 \t Acc 30.56 \t AccHead 35.67 \t AccTail 14.06\n",
      "Epoch: [018] \t Loss 2.8111 \t Acc 31.46 \t AccHead 36.63 \t AccTail 14.74\n",
      "Epoch: [019] \t Loss 2.7793 \t Acc 29.66 \t AccHead 34.33 \t AccTail 14.60\n",
      "Epoch: [020] \t Loss 2.7701 \t Acc 31.98 \t AccHead 36.80 \t AccTail 16.43\n",
      "Epoch: [021] \t Loss 2.7219 \t Acc 32.96 \t AccHead 38.74 \t AccTail 14.32\n",
      "Epoch: [022] \t Loss 2.7169 \t Acc 32.11 \t AccHead 37.80 \t AccTail 13.75\n",
      "Epoch: [023] \t Loss 2.7091 \t Acc 32.91 \t AccHead 38.60 \t AccTail 14.54\n",
      "Epoch: [024] \t Loss 2.6769 \t Acc 33.62 \t AccHead 38.69 \t AccTail 17.24\n",
      "Epoch: [025] \t Loss 2.6562 \t Acc 29.83 \t AccHead 34.91 \t AccTail 13.42\n",
      "Epoch: [026] \t Loss 2.6316 \t Acc 34.07 \t AccHead 39.08 \t AccTail 17.92\n",
      "Epoch: [027] \t Loss 2.6329 \t Acc 34.15 \t AccHead 38.56 \t AccTail 19.93\n",
      "Epoch: [028] \t Loss 2.6421 \t Acc 31.99 \t AccHead 38.26 \t AccTail 11.77\n",
      "Epoch: [029] \t Loss 2.6004 \t Acc 33.88 \t AccHead 38.60 \t AccTail 18.62\n",
      "Epoch: [030] \t Loss 2.5950 \t Acc 33.75 \t AccHead 39.46 \t AccTail 15.31\n",
      "Epoch: [031] \t Loss 2.5713 \t Acc 34.86 \t AccHead 40.11 \t AccTail 17.89\n",
      "Epoch: [032] \t Loss 2.5793 \t Acc 34.44 \t AccHead 41.02 \t AccTail 13.19\n",
      "Epoch: [033] \t Loss 2.5495 \t Acc 37.08 \t AccHead 42.98 \t AccTail 18.02\n",
      "Epoch: [034] \t Loss 2.5593 \t Acc 35.11 \t AccHead 39.95 \t AccTail 19.46\n",
      "Epoch: [035] \t Loss 2.5433 \t Acc 36.42 \t AccHead 42.29 \t AccTail 17.49\n",
      "Epoch: [036] \t Loss 2.5427 \t Acc 31.01 \t AccHead 34.91 \t AccTail 18.40\n",
      "Epoch: [037] \t Loss 2.5195 \t Acc 33.02 \t AccHead 38.22 \t AccTail 16.23\n",
      "Epoch: [038] \t Loss 2.5149 \t Acc 34.34 \t AccHead 38.60 \t AccTail 20.59\n",
      "Epoch: [039] \t Loss 2.5070 \t Acc 36.32 \t AccHead 42.14 \t AccTail 17.52\n",
      "Epoch: [040] \t Loss 2.5023 \t Acc 38.89 \t AccHead 44.46 \t AccTail 20.86\n",
      "Epoch: [041] \t Loss 2.5013 \t Acc 36.21 \t AccHead 41.44 \t AccTail 19.32\n",
      "Epoch: [042] \t Loss 2.4935 \t Acc 35.35 \t AccHead 40.57 \t AccTail 18.49\n",
      "Epoch: [043] \t Loss 2.4852 \t Acc 32.02 \t AccHead 36.56 \t AccTail 17.36\n",
      "Epoch: [044] \t Loss 2.4637 \t Acc 34.87 \t AccHead 39.83 \t AccTail 18.87\n",
      "Epoch: [045] \t Loss 2.4617 \t Acc 37.12 \t AccHead 42.13 \t AccTail 20.93\n",
      "Epoch: [046] \t Loss 2.4501 \t Acc 35.35 \t AccHead 39.14 \t AccTail 23.15\n",
      "Epoch: [047] \t Loss 2.4527 \t Acc 36.02 \t AccHead 41.24 \t AccTail 19.15\n",
      "Epoch: [048] \t Loss 2.4363 \t Acc 37.56 \t AccHead 42.72 \t AccTail 20.89\n",
      "Epoch: [049] \t Loss 2.4392 \t Acc 38.42 \t AccHead 44.02 \t AccTail 20.34\n",
      "Epoch: [050] \t Loss 2.4357 \t Acc 38.01 \t AccHead 43.99 \t AccTail 18.69\n",
      "Epoch: [051] \t Loss 2.4217 \t Acc 39.73 \t AccHead 45.23 \t AccTail 21.98\n",
      "Epoch: [052] \t Loss 2.4483 \t Acc 36.85 \t AccHead 43.07 \t AccTail 16.76\n",
      "Epoch: [053] \t Loss 2.4042 \t Acc 37.10 \t AccHead 42.18 \t AccTail 20.72\n",
      "Epoch: [054] \t Loss 2.4194 \t Acc 32.04 \t AccHead 37.26 \t AccTail 15.19\n",
      "Epoch: [055] \t Loss 2.4138 \t Acc 38.41 \t AccHead 44.58 \t AccTail 18.47\n",
      "Epoch: [056] \t Loss 2.4015 \t Acc 40.23 \t AccHead 46.80 \t AccTail 19.05\n",
      "Epoch: [057] \t Loss 2.3953 \t Acc 40.08 \t AccHead 45.77 \t AccTail 21.70\n",
      "Epoch: [058] \t Loss 2.4007 \t Acc 34.72 \t AccHead 39.01 \t AccTail 20.89\n",
      "Epoch: [059] \t Loss 2.3835 \t Acc 36.82 \t AccHead 41.71 \t AccTail 21.03\n",
      "Epoch: [060] \t Loss 2.3773 \t Acc 38.88 \t AccHead 44.29 \t AccTail 21.42\n",
      "Epoch: [061] \t Loss 2.3732 \t Acc 36.89 \t AccHead 41.46 \t AccTail 22.11\n",
      "Epoch: [062] \t Loss 2.3965 \t Acc 38.35 \t AccHead 44.53 \t AccTail 18.39\n",
      "Epoch: [063] \t Loss 2.3514 \t Acc 37.09 \t AccHead 42.05 \t AccTail 21.07\n",
      "Epoch: [064] \t Loss 2.3706 \t Acc 37.78 \t AccHead 42.95 \t AccTail 21.07\n",
      "Epoch: [065] \t Loss 2.3585 \t Acc 38.31 \t AccHead 43.63 \t AccTail 21.11\n",
      "Epoch: [066] \t Loss 2.3399 \t Acc 38.13 \t AccHead 43.73 \t AccTail 20.04\n",
      "Epoch: [067] \t Loss 2.3590 \t Acc 34.87 \t AccHead 39.93 \t AccTail 18.54\n",
      "Epoch: [068] \t Loss 2.3552 \t Acc 39.06 \t AccHead 45.47 \t AccTail 18.35\n",
      "Epoch: [069] \t Loss 2.3451 \t Acc 39.98 \t AccHead 45.17 \t AccTail 23.21\n",
      "Epoch: [070] \t Loss 2.3381 \t Acc 36.53 \t AccHead 42.01 \t AccTail 18.84\n",
      "Epoch: [071] \t Loss 2.3373 \t Acc 38.89 \t AccHead 44.46 \t AccTail 20.91\n",
      "Epoch: [072] \t Loss 2.3363 \t Acc 37.69 \t AccHead 42.73 \t AccTail 21.42\n",
      "Epoch: [073] \t Loss 2.3371 \t Acc 41.49 \t AccHead 47.27 \t AccTail 22.85\n",
      "Epoch: [074] \t Loss 2.3412 \t Acc 38.72 \t AccHead 44.35 \t AccTail 20.53\n",
      "Epoch: [075] \t Loss 2.3206 \t Acc 38.81 \t AccHead 43.80 \t AccTail 22.72\n",
      "Epoch: [076] \t Loss 2.3193 \t Acc 38.96 \t AccHead 42.67 \t AccTail 27.00\n",
      "Epoch: [077] \t Loss 2.3411 \t Acc 40.69 \t AccHead 46.88 \t AccTail 20.70\n",
      "Epoch: [078] \t Loss 2.3082 \t Acc 38.79 \t AccHead 44.23 \t AccTail 21.24\n",
      "Epoch: [079] \t Loss 2.3262 \t Acc 37.97 \t AccHead 44.63 \t AccTail 16.49\n",
      "Epoch: [080] \t Loss 2.3206 \t Acc 40.71 \t AccHead 46.56 \t AccTail 21.82\n",
      "Epoch: [081] \t Loss 2.3226 \t Acc 39.43 \t AccHead 45.28 \t AccTail 20.56\n",
      "Epoch: [082] \t Loss 2.3172 \t Acc 39.65 \t AccHead 44.84 \t AccTail 22.87\n",
      "Epoch: [083] \t Loss 2.3228 \t Acc 36.27 \t AccHead 40.17 \t AccTail 23.67\n",
      "Epoch: [084] \t Loss 2.3094 \t Acc 38.51 \t AccHead 44.24 \t AccTail 20.02\n",
      "Epoch: [085] \t Loss 2.3139 \t Acc 35.60 \t AccHead 40.45 \t AccTail 19.92\n",
      "Epoch: [086] \t Loss 2.3267 \t Acc 41.27 \t AccHead 46.43 \t AccTail 24.58\n",
      "Epoch: [087] \t Loss 2.3081 \t Acc 38.27 \t AccHead 43.10 \t AccTail 22.66\n",
      "Epoch: [088] \t Loss 2.3050 \t Acc 39.69 \t AccHead 44.70 \t AccTail 23.52\n",
      "Epoch: [089] \t Loss 2.3222 \t Acc 40.06 \t AccHead 45.42 \t AccTail 22.74\n",
      "Epoch: [090] \t Loss 2.2960 \t Acc 41.52 \t AccHead 46.22 \t AccTail 26.33\n",
      "Epoch: [091] \t Loss 2.2898 \t Acc 39.60 \t AccHead 45.32 \t AccTail 21.15\n",
      "Epoch: [092] \t Loss 2.3030 \t Acc 39.16 \t AccHead 45.31 \t AccTail 19.30\n",
      "Epoch: [093] \t Loss 2.2877 \t Acc 41.16 \t AccHead 46.64 \t AccTail 23.50\n",
      "Epoch: [094] \t Loss 2.2721 \t Acc 39.68 \t AccHead 44.29 \t AccTail 24.78\n",
      "Epoch: [095] \t Loss 2.2876 \t Acc 38.88 \t AccHead 45.24 \t AccTail 18.36\n",
      "Epoch: [096] \t Loss 2.3005 \t Acc 41.73 \t AccHead 46.76 \t AccTail 25.49\n",
      "Epoch: [097] \t Loss 2.2826 \t Acc 38.79 \t AccHead 42.98 \t AccTail 25.28\n",
      "Epoch: [098] \t Loss 2.2816 \t Acc 39.93 \t AccHead 45.60 \t AccTail 21.63\n",
      "Epoch: [099] \t Loss 2.2720 \t Acc 41.78 \t AccHead 47.49 \t AccTail 23.38\n",
      "Epoch: [100] \t Loss 2.2959 \t Acc 40.22 \t AccHead 45.74 \t AccTail 22.34\n",
      "Epoch: [101] \t Loss 2.2779 \t Acc 39.13 \t AccHead 43.37 \t AccTail 25.44\n",
      "Epoch: [102] \t Loss 2.2751 \t Acc 40.76 \t AccHead 45.76 \t AccTail 24.61\n",
      "Epoch: [103] \t Loss 2.2671 \t Acc 37.60 \t AccHead 42.78 \t AccTail 20.86\n",
      "Epoch: [104] \t Loss 2.2803 \t Acc 40.08 \t AccHead 45.84 \t AccTail 21.46\n",
      "Epoch: [105] \t Loss 2.2910 \t Acc 40.87 \t AccHead 46.96 \t AccTail 21.24\n",
      "Epoch: [106] \t Loss 2.2735 \t Acc 39.92 \t AccHead 44.60 \t AccTail 24.81\n",
      "Epoch: [107] \t Loss 2.2655 \t Acc 39.15 \t AccHead 43.23 \t AccTail 26.01\n",
      "Epoch: [108] \t Loss 2.2757 \t Acc 35.18 \t AccHead 38.84 \t AccTail 23.38\n",
      "Epoch: [109] \t Loss 2.2597 \t Acc 38.09 \t AccHead 43.50 \t AccTail 20.63\n",
      "Epoch: [110] \t Loss 2.3050 \t Acc 40.20 \t AccHead 46.80 \t AccTail 18.91\n",
      "Epoch: [111] \t Loss 2.2665 \t Acc 39.37 \t AccHead 44.93 \t AccTail 21.43\n",
      "Epoch: [112] \t Loss 2.2636 \t Acc 34.16 \t AccHead 37.65 \t AccTail 22.88\n",
      "Epoch: [113] \t Loss 2.2665 \t Acc 41.32 \t AccHead 46.89 \t AccTail 23.34\n",
      "Epoch: [114] \t Loss 2.2533 \t Acc 39.22 \t AccHead 43.79 \t AccTail 24.46\n",
      "Epoch: [115] \t Loss 2.2626 \t Acc 39.71 \t AccHead 44.65 \t AccTail 23.77\n",
      "Epoch: [116] \t Loss 2.2762 \t Acc 40.82 \t AccHead 46.17 \t AccTail 23.56\n",
      "Epoch: [117] \t Loss 2.2711 \t Acc 41.50 \t AccHead 47.24 \t AccTail 23.00\n",
      "Epoch: [118] \t Loss 2.2711 \t Acc 41.26 \t AccHead 46.29 \t AccTail 25.00\n",
      "Epoch: [119] \t Loss 2.2522 \t Acc 41.73 \t AccHead 47.10 \t AccTail 24.41\n",
      "Epoch: [120] \t Loss 2.2323 \t Acc 42.00 \t AccHead 48.15 \t AccTail 22.13\n",
      "Epoch: [121] \t Loss 2.2616 \t Acc 39.43 \t AccHead 44.31 \t AccTail 23.67\n",
      "Epoch: [122] \t Loss 2.2556 \t Acc 40.87 \t AccHead 46.26 \t AccTail 23.46\n",
      "Epoch: [123] \t Loss 2.2615 \t Acc 39.52 \t AccHead 43.97 \t AccTail 25.13\n",
      "Epoch: [124] \t Loss 2.2623 \t Acc 40.09 \t AccHead 44.25 \t AccTail 26.67\n",
      "Epoch: [125] \t Loss 2.2554 \t Acc 42.01 \t AccHead 47.56 \t AccTail 24.13\n",
      "Epoch: [126] \t Loss 2.2600 \t Acc 41.14 \t AccHead 46.70 \t AccTail 23.23\n",
      "Epoch: [127] \t Loss 2.2480 \t Acc 41.95 \t AccHead 47.90 \t AccTail 22.74\n",
      "Epoch: [128] \t Loss 2.2396 \t Acc 41.22 \t AccHead 46.88 \t AccTail 22.98\n",
      "Epoch: [129] \t Loss 2.2515 \t Acc 38.06 \t AccHead 42.35 \t AccTail 24.19\n",
      "Epoch: [130] \t Loss 2.2410 \t Acc 43.38 \t AccHead 49.05 \t AccTail 25.09\n",
      "Epoch: [131] \t Loss 2.2467 \t Acc 39.97 \t AccHead 44.08 \t AccTail 26.68\n",
      "Epoch: [132] \t Loss 2.2328 \t Acc 40.94 \t AccHead 47.13 \t AccTail 20.96\n",
      "Epoch: [133] \t Loss 2.2424 \t Acc 41.46 \t AccHead 46.37 \t AccTail 25.58\n",
      "Epoch: [134] \t Loss 2.2715 \t Acc 40.38 \t AccHead 45.62 \t AccTail 23.42\n",
      "Epoch: [135] \t Loss 2.2408 \t Acc 41.64 \t AccHead 47.25 \t AccTail 23.55\n",
      "Epoch: [136] \t Loss 2.2382 \t Acc 38.31 \t AccHead 42.90 \t AccTail 23.48\n",
      "Epoch: [137] \t Loss 2.2445 \t Acc 42.04 \t AccHead 46.98 \t AccTail 26.10\n",
      "Epoch: [138] \t Loss 2.2202 \t Acc 40.81 \t AccHead 46.91 \t AccTail 21.13\n",
      "Epoch: [139] \t Loss 2.2387 \t Acc 42.11 \t AccHead 47.51 \t AccTail 24.67\n",
      "Epoch: [140] \t Loss 2.2423 \t Acc 40.66 \t AccHead 46.44 \t AccTail 22.02\n",
      "Epoch: [141] \t Loss 2.2468 \t Acc 38.95 \t AccHead 43.15 \t AccTail 25.36\n",
      "Epoch: [142] \t Loss 2.2360 \t Acc 43.12 \t AccHead 47.97 \t AccTail 27.43\n",
      "Epoch: [143] \t Loss 2.2340 \t Acc 37.73 \t AccHead 42.67 \t AccTail 21.79\n",
      "Epoch: [144] \t Loss 2.2460 \t Acc 43.10 \t AccHead 48.64 \t AccTail 25.20\n",
      "Epoch: [145] \t Loss 2.2470 \t Acc 38.62 \t AccHead 42.90 \t AccTail 24.79\n",
      "Epoch: [146] \t Loss 2.2288 \t Acc 38.07 \t AccHead 42.81 \t AccTail 22.74\n",
      "Epoch: [147] \t Loss 2.2494 \t Acc 39.65 \t AccHead 44.56 \t AccTail 23.77\n",
      "Epoch: [148] \t Loss 2.2311 \t Acc 42.05 \t AccHead 47.60 \t AccTail 24.13\n",
      "Epoch: [149] \t Loss 2.2309 \t Acc 41.21 \t AccHead 46.38 \t AccTail 24.53\n",
      "Epoch: [150] \t Loss 2.2523 \t Acc 41.83 \t AccHead 47.74 \t AccTail 22.72\n",
      "Epoch: [151] \t Loss 1.7393 \t Acc 61.19 \t AccHead 67.44 \t AccTail 40.98\n",
      "Epoch: [152] \t Loss 1.4662 \t Acc 65.38 \t AccHead 70.80 \t AccTail 47.89\n",
      "Epoch: [153] \t Loss 1.3339 \t Acc 67.99 \t AccHead 73.26 \t AccTail 50.97\n",
      "Epoch: [154] \t Loss 1.2232 \t Acc 70.66 \t AccHead 75.58 \t AccTail 54.75\n",
      "Epoch: [155] \t Loss 1.1667 \t Acc 73.09 \t AccHead 78.32 \t AccTail 56.20\n",
      "Epoch: [156] \t Loss 1.0792 \t Acc 74.13 \t AccHead 78.49 \t AccTail 60.06\n",
      "Epoch: [157] \t Loss 1.0166 \t Acc 76.34 \t AccHead 80.44 \t AccTail 63.08\n",
      "Epoch: [158] \t Loss 0.9503 \t Acc 78.36 \t AccHead 82.41 \t AccTail 65.30\n",
      "Epoch: [159] \t Loss 0.8971 \t Acc 79.60 \t AccHead 83.26 \t AccTail 67.78\n",
      "Epoch: [160] \t Loss 0.8416 \t Acc 80.92 \t AccHead 84.48 \t AccTail 69.41\n",
      "Epoch: [161] \t Loss 0.7949 \t Acc 81.42 \t AccHead 85.24 \t AccTail 69.10\n",
      "Epoch: [162] \t Loss 0.7538 \t Acc 83.41 \t AccHead 86.78 \t AccTail 72.50\n",
      "Epoch: [163] \t Loss 0.7024 \t Acc 84.96 \t AccHead 87.59 \t AccTail 76.49\n",
      "Epoch: [164] \t Loss 0.6551 \t Acc 84.94 \t AccHead 87.33 \t AccTail 77.22\n",
      "Epoch: [165] \t Loss 0.6263 \t Acc 85.05 \t AccHead 87.90 \t AccTail 75.84\n",
      "Epoch: [166] \t Loss 0.6020 \t Acc 87.54 \t AccHead 89.66 \t AccTail 80.69\n",
      "Epoch: [167] \t Loss 0.5825 \t Acc 88.08 \t AccHead 90.05 \t AccTail 81.70\n",
      "Epoch: [168] \t Loss 0.5437 \t Acc 88.83 \t AccHead 90.25 \t AccTail 84.21\n",
      "Epoch: [169] \t Loss 0.5084 \t Acc 87.42 \t AccHead 89.44 \t AccTail 80.88\n",
      "Epoch: [170] \t Loss 0.5048 \t Acc 89.24 \t AccHead 90.91 \t AccTail 83.86\n",
      "Epoch: [171] \t Loss 0.5006 \t Acc 88.96 \t AccHead 90.36 \t AccTail 84.43\n",
      "Epoch: [172] \t Loss 0.4820 \t Acc 88.79 \t AccHead 90.08 \t AccTail 84.65\n",
      "Epoch: [173] \t Loss 0.4409 \t Acc 90.30 \t AccHead 91.40 \t AccTail 86.74\n",
      "Epoch: [174] \t Loss 0.4518 \t Acc 90.74 \t AccHead 92.04 \t AccTail 86.54\n",
      "Epoch: [175] \t Loss 0.4364 \t Acc 91.04 \t AccHead 92.46 \t AccTail 86.46\n",
      "Epoch: [176] \t Loss 0.4259 \t Acc 91.02 \t AccHead 92.46 \t AccTail 86.39\n",
      "Epoch: [177] \t Loss 0.4102 \t Acc 91.60 \t AccHead 92.56 \t AccTail 88.52\n",
      "Epoch: [178] \t Loss 0.4232 \t Acc 90.55 \t AccHead 91.27 \t AccTail 88.25\n",
      "Epoch: [179] \t Loss 0.4039 \t Acc 90.97 \t AccHead 92.04 \t AccTail 87.52\n",
      "Epoch: [180] \t Loss 0.3998 \t Acc 90.68 \t AccHead 91.56 \t AccTail 87.82\n",
      "Epoch: [181] \t Loss 0.3728 \t Acc 91.49 \t AccHead 92.30 \t AccTail 88.87\n",
      "Epoch: [182] \t Loss 0.3628 \t Acc 91.77 \t AccHead 92.42 \t AccTail 89.68\n",
      "Epoch: [183] \t Loss 0.3610 \t Acc 91.87 \t AccHead 92.75 \t AccTail 89.00\n",
      "Epoch: [184] \t Loss 0.3559 \t Acc 92.90 \t AccHead 93.63 \t AccTail 90.55\n",
      "Epoch: [185] \t Loss 0.3614 \t Acc 92.37 \t AccHead 92.81 \t AccTail 90.92\n",
      "Epoch: [186] \t Loss 0.3579 \t Acc 92.44 \t AccHead 93.40 \t AccTail 89.33\n",
      "Epoch: [187] \t Loss 0.3437 \t Acc 93.40 \t AccHead 93.88 \t AccTail 91.85\n",
      "Epoch: [188] \t Loss 0.3466 \t Acc 92.13 \t AccHead 93.09 \t AccTail 89.04\n",
      "Epoch: [189] \t Loss 0.3463 \t Acc 93.11 \t AccHead 93.68 \t AccTail 91.27\n",
      "Epoch: [190] \t Loss 0.3597 \t Acc 92.72 \t AccHead 93.41 \t AccTail 90.47\n",
      "Epoch: [191] \t Loss 0.3347 \t Acc 93.10 \t AccHead 93.58 \t AccTail 91.54\n",
      "Epoch: [192] \t Loss 0.3224 \t Acc 92.83 \t AccHead 93.53 \t AccTail 90.55\n",
      "Epoch: [193] \t Loss 0.3225 \t Acc 92.44 \t AccHead 92.67 \t AccTail 91.68\n",
      "Epoch: [194] \t Loss 0.3414 \t Acc 91.78 \t AccHead 92.07 \t AccTail 90.85\n",
      "Epoch: [195] \t Loss 0.3412 \t Acc 93.03 \t AccHead 93.47 \t AccTail 91.61\n",
      "Epoch: [196] \t Loss 0.3208 \t Acc 92.05 \t AccHead 92.60 \t AccTail 90.28\n",
      "Epoch: [197] \t Loss 0.3273 \t Acc 92.26 \t AccHead 93.01 \t AccTail 89.85\n",
      "Epoch: [198] \t Loss 0.2955 \t Acc 94.38 \t AccHead 94.96 \t AccTail 92.53\n",
      "Epoch: [199] \t Loss 0.2923 \t Acc 93.71 \t AccHead 93.85 \t AccTail 93.23\n",
      "Epoch: [200] \t Loss 0.3029 \t Acc 93.58 \t AccHead 93.98 \t AccTail 92.31\n",
      "199\n",
      "Finished Training\n",
      "Acc 9.66 \t AccHead 18.81 \t AccTail 0.56\n",
      "----------------------------\n",
      "get the top-1 accuracy for each of the classes.\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [28]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m----------------------------\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mget the top-1 accuracy for each of the classes.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m avg_accu \u001b[38;5;241m=\u001b[39m \u001b[43mtestEeahClass\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m tau \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(np\u001b[38;5;241m.\u001b[39marray(avg_accu)) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(tau)\n",
      "Input \u001b[0;32mIn [16]\u001b[0m, in \u001b[0;36mtestEeahClass\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m acc \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_classes):\n\u001b[0;32m---> 20\u001b[0m     acc\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mclass_correct\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mclass_total\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_classes):\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAccuracy of \u001b[39m\u001b[38;5;132;01m%5s\u001b[39;00m\u001b[38;5;124m : \u001b[39m\u001b[38;5;132;01m%2d\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%%\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (\n\u001b[1;32m     24\u001b[0m         train_dataset\u001b[38;5;241m.\u001b[39mclasses[i], acc[i]))\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: float division by zero"
     ]
    }
   ],
   "source": [
    "train_dataset, train_loader, test_loader, num_classes = get_loaders()\n",
    "model = ResNet18(num_classes)\n",
    "model = model.cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=150, gamma=0.1)\n",
    "\n",
    "# Let's build our model\n",
    "train(EPOCHS)\n",
    "print('Finished Training')\n",
    "test() \n",
    "print('----------------------------')\n",
    "print('get the top-1 accuracy for each of the classes.')\n",
    "avg_accu = testEeahClass()\n",
    "tau = np.mean(np.array(avg_accu)) / 100\n",
    "print(tau)\n",
    "tau_list.append(tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fb8a28ec-2859-4eb7-801e-2ab32f55c5dd",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2022-06-22T18:17:05.590536Z",
     "iopub.status.busy": "2022-06-22T18:17:05.589992Z",
     "iopub.status.idle": "2022-06-22T18:17:07.893135Z",
     "shell.execute_reply": "2022-06-22T18:17:07.891579Z",
     "shell.execute_reply.started": "2022-06-22T18:17:05.590485Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of apple :  1 %\n",
      "Accuracy of aquarium_fish :  4 %\n",
      "Accuracy of  baby :  5 %\n",
      "Accuracy of  bear :  7 %\n",
      "Accuracy of beaver :  5 %\n",
      "Accuracy of   bed :  3 %\n",
      "Accuracy of   bee :  4 %\n",
      "Accuracy of beetle :  5 %\n",
      "Accuracy of bicycle :  9 %\n",
      "Accuracy of bottle :  9 %\n",
      "Accuracy of  bowl :  4 %\n",
      "Accuracy of   boy :  5 %\n",
      "Accuracy of bridge :  6 %\n",
      "Accuracy of   bus :  6 %\n",
      "Accuracy of butterfly :  5 %\n",
      "Accuracy of camel :  4 %\n",
      "Accuracy of   can :  3 %\n",
      "Accuracy of castle :  4 %\n",
      "Accuracy of caterpillar :  3 %\n",
      "Accuracy of cattle :  4 %\n",
      "Accuracy of chair :  5 %\n",
      "Accuracy of chimpanzee :  2 %\n",
      "Accuracy of clock :  3 %\n",
      "Accuracy of cloud :  1 %\n",
      "Accuracy of cockroach :  7 %\n",
      "Accuracy of couch :  6 %\n",
      "Accuracy of  crab :  4 %\n",
      "Accuracy of crocodile :  1 %\n",
      "Accuracy of   cup :  4 %\n",
      "Accuracy of dinosaur :  3 %\n",
      "Accuracy of dolphin :  1 %\n",
      "Accuracy of elephant :  0 %\n",
      "Accuracy of flatfish :  1 %\n",
      "Accuracy of forest :  0 %\n",
      "Accuracy of   fox :  1 %\n",
      "Accuracy of  girl :  2 %\n",
      "Accuracy of hamster :  1 %\n",
      "Accuracy of house :  1 %\n",
      "Accuracy of kangaroo :  1 %\n",
      "Accuracy of keyboard :  2 %\n",
      "Accuracy of  lamp :  2 %\n",
      "Accuracy of lawn_mower :  2 %\n",
      "Accuracy of leopard :  1 %\n",
      "Accuracy of  lion :  2 %\n",
      "Accuracy of lizard :  1 %\n",
      "Accuracy of lobster :  0 %\n",
      "Accuracy of   man :  0 %\n",
      "Accuracy of maple_tree :  0 %\n",
      "Accuracy of motorcycle :  1 %\n",
      "Accuracy of mountain :  0 %\n",
      "Accuracy of mouse :  4 %\n",
      "Accuracy of mushroom :  0 %\n",
      "Accuracy of oak_tree :  1 %\n",
      "Accuracy of orange :  1 %\n",
      "Accuracy of orchid :  0 %\n",
      "Accuracy of otter :  1 %\n",
      "Accuracy of palm_tree :  0 %\n",
      "Accuracy of  pear :  0 %\n",
      "Accuracy of pickup_truck :  0 %\n",
      "Accuracy of pine_tree :  1 %\n",
      "Accuracy of plain :  0 %\n",
      "Accuracy of plate :  1 %\n",
      "Accuracy of poppy :  1 %\n",
      "Accuracy of porcupine :  0 %\n",
      "Accuracy of possum :  0 %\n",
      "Accuracy of rabbit :  1 %\n",
      "Accuracy of raccoon :  1 %\n",
      "Accuracy of   ray :  0 %\n",
      "Accuracy of  road :  0 %\n",
      "Accuracy of rocket :  0 %\n",
      "Accuracy of  rose :  1 %\n",
      "Accuracy of   sea :  0 %\n",
      "Accuracy of  seal :  2 %\n",
      "Accuracy of shark :  0 %\n",
      "Accuracy of shrew :  0 %\n",
      "Accuracy of skunk :  1 %\n",
      "Accuracy of skyscraper :  0 %\n",
      "Accuracy of snail :  1 %\n",
      "Accuracy of snake :  0 %\n",
      "Accuracy of spider :  0 %\n",
      "Accuracy of squirrel :  0 %\n",
      "Accuracy of streetcar :  0 %\n",
      "Accuracy of sunflower :  0 %\n",
      "Accuracy of sweet_pepper :  0 %\n",
      "Accuracy of table :  0 %\n",
      "Accuracy of  tank :  0 %\n",
      "Accuracy of telephone :  0 %\n",
      "Accuracy of television :  0 %\n",
      "Accuracy of tiger :  1 %\n",
      "Accuracy of tractor :  2 %\n",
      "Accuracy of train :  1 %\n",
      "Accuracy of trout :  1 %\n",
      "Accuracy of tulip :  1 %\n",
      "Accuracy of turtle :  0 %\n",
      "Accuracy of wardrobe :  0 %\n",
      "Accuracy of whale :  1 %\n",
      "Accuracy of willow_tree :  0 %\n",
      "Accuracy of  wolf :  0 %\n",
      "Accuracy of woman :  0 %\n",
      "Accuracy of  worm :  0 %\n",
      "0.0175\n"
     ]
    }
   ],
   "source": [
    "avg_accu = testEeahClass()\n",
    "tau = np.mean(np.array(avg_accu)) / 100\n",
    "print(tau)\n",
    "tau_list.append(tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "QAbwdq_dLQhz",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-22T18:18:28.593827Z",
     "iopub.status.busy": "2022-06-22T18:18:28.593290Z",
     "iopub.status.idle": "2022-06-22T18:18:28.624258Z",
     "shell.execute_reply": "2022-06-22T18:18:28.623300Z",
     "shell.execute_reply.started": "2022-06-22T18:18:28.593771Z"
    },
    "id": "QAbwdq_dLQhz",
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATASET = 'CIFAR100' #['CIFAR10', 'CIFAR100']\n",
    "IMB_TYPE = 'exp' #['exp', 'step']\n",
    "IMB_FACTOR = 0.01 #[0.1, 0.01]\n",
    "SAVE_DIR = 'logs/100-exp-0.01-find_tau' \n",
    "\n",
    "LR = 0.1\n",
    "BATCH_SIZE = 128\n",
    "MOMENTUM = 0.9\n",
    "WEIGHT_DECAY = 0.001043\n",
    "EPOCHS = 200\n",
    "\n",
    "os.makedirs(osp.join(SAVE_DIR), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "opuih9lGPeeg",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-22T18:18:29.749021Z",
     "iopub.status.busy": "2022-06-22T18:18:29.748568Z",
     "iopub.status.idle": "2022-06-22T18:34:41.839970Z",
     "shell.execute_reply": "2022-06-22T18:34:41.839224Z",
     "shell.execute_reply.started": "2022-06-22T18:18:29.748969Z"
    },
    "id": "opuih9lGPeeg",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "cls num list(train_dataset):\n",
      "[400, 381, 364, 347, 332, 316, 302, 288, 275, 263, 251, 239, 228, 218, 208, 199, 190, 181, 173, 165, 157, 150, 143, 137, 130, 125, 119, 113, 108, 103, 99, 94, 90, 86, 82, 78, 74, 71, 68, 65, 62, 59, 56, 54, 51, 49, 47, 44, 42, 40, 39, 37, 35, 33, 32, 30, 29, 28, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 16, 15, 14, 14, 13, 12, 12, 11, 11, 10, 10, 9, 9, 8, 8, 8, 7, 7, 6, 6, 6, 6, 5, 5, 5, 5, 4, 4, 4, 4, 4]\n",
      "cls num list(val_dataset):\n",
      "[100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100]\n",
      "Epoch: [001] \t Loss 4.5303 \t Acc 12.99 \t AccHead 14.23 \t AccTail 0.00\n",
      "Epoch: [002] \t Loss 3.6992 \t Acc 15.90 \t AccHead 17.41 \t AccTail 0.00\n",
      "Epoch: [003] \t Loss 3.4327 \t Acc 18.66 \t AccHead 20.32 \t AccTail 1.21\n",
      "Epoch: [004] \t Loss 3.2876 \t Acc 21.96 \t AccHead 23.91 \t AccTail 1.22\n",
      "Epoch: [005] \t Loss 3.1640 \t Acc 24.67 \t AccHead 26.80 \t AccTail 2.28\n",
      "Epoch: [006] \t Loss 3.0515 \t Acc 26.98 \t AccHead 29.18 \t AccTail 3.90\n",
      "Epoch: [007] \t Loss 2.9705 \t Acc 26.31 \t AccHead 28.16 \t AccTail 6.73\n",
      "Epoch: [008] \t Loss 2.8605 \t Acc 28.74 \t AccHead 30.91 \t AccTail 6.02\n",
      "Epoch: [009] \t Loss 2.7682 \t Acc 28.71 \t AccHead 30.94 \t AccTail 5.23\n",
      "Epoch: [010] \t Loss 2.7009 \t Acc 31.91 \t AccHead 34.68 \t AccTail 2.70\n",
      "Epoch: [011] \t Loss 2.6104 \t Acc 34.06 \t AccHead 36.93 \t AccTail 3.90\n",
      "Epoch: [012] \t Loss 2.5594 \t Acc 35.59 \t AccHead 38.21 \t AccTail 7.83\n",
      "Epoch: [013] \t Loss 2.4744 \t Acc 36.15 \t AccHead 39.10 \t AccTail 4.98\n",
      "Epoch: [014] \t Loss 2.4301 \t Acc 36.21 \t AccHead 39.07 \t AccTail 6.16\n",
      "Epoch: [015] \t Loss 2.3547 \t Acc 37.00 \t AccHead 39.54 \t AccTail 10.12\n",
      "Epoch: [016] \t Loss 2.3075 \t Acc 40.53 \t AccHead 43.20 \t AccTail 12.20\n",
      "Epoch: [017] \t Loss 2.2642 \t Acc 36.30 \t AccHead 38.77 \t AccTail 10.12\n",
      "Epoch: [018] \t Loss 2.2253 \t Acc 40.33 \t AccHead 43.27 \t AccTail 9.19\n",
      "Epoch: [019] \t Loss 2.1825 \t Acc 43.14 \t AccHead 46.18 \t AccTail 11.26\n",
      "Epoch: [020] \t Loss 2.1418 \t Acc 42.39 \t AccHead 45.24 \t AccTail 12.26\n",
      "Epoch: [021] \t Loss 2.0936 \t Acc 44.04 \t AccHead 47.18 \t AccTail 10.90\n",
      "Epoch: [022] \t Loss 2.0870 \t Acc 41.83 \t AccHead 44.70 \t AccTail 11.66\n",
      "Epoch: [023] \t Loss 2.0160 \t Acc 47.77 \t AccHead 50.84 \t AccTail 15.48\n",
      "Epoch: [024] \t Loss 1.9838 \t Acc 41.83 \t AccHead 44.59 \t AccTail 12.67\n",
      "Epoch: [025] \t Loss 1.9706 \t Acc 46.62 \t AccHead 49.46 \t AccTail 16.76\n",
      "Epoch: [026] \t Loss 1.9675 \t Acc 47.73 \t AccHead 50.59 \t AccTail 17.52\n",
      "Epoch: [027] \t Loss 1.9164 \t Acc 46.85 \t AccHead 49.78 \t AccTail 16.02\n",
      "Epoch: [028] \t Loss 1.8803 \t Acc 42.92 \t AccHead 45.53 \t AccTail 15.36\n",
      "Epoch: [029] \t Loss 1.8498 \t Acc 51.77 \t AccHead 55.10 \t AccTail 16.69\n",
      "Epoch: [030] \t Loss 1.8320 \t Acc 49.78 \t AccHead 52.48 \t AccTail 21.19\n",
      "Epoch: [031] \t Loss 1.8007 \t Acc 52.18 \t AccHead 55.44 \t AccTail 17.96\n",
      "Epoch: [032] \t Loss 1.7633 \t Acc 49.23 \t AccHead 52.33 \t AccTail 16.64\n",
      "Epoch: [033] \t Loss 1.7667 \t Acc 49.15 \t AccHead 51.59 \t AccTail 23.42\n",
      "Epoch: [034] \t Loss 1.7053 \t Acc 50.66 \t AccHead 53.49 \t AccTail 21.12\n",
      "Epoch: [035] \t Loss 1.7325 \t Acc 51.94 \t AccHead 54.66 \t AccTail 23.50\n",
      "Epoch: [036] \t Loss 1.6609 \t Acc 51.18 \t AccHead 53.65 \t AccTail 25.13\n",
      "Epoch: [037] \t Loss 1.6717 \t Acc 54.09 \t AccHead 56.55 \t AccTail 28.07\n",
      "Epoch: [038] \t Loss 1.6177 \t Acc 54.51 \t AccHead 57.80 \t AccTail 19.97\n",
      "Epoch: [039] \t Loss 1.5725 \t Acc 51.92 \t AccHead 54.60 \t AccTail 23.76\n",
      "Epoch: [040] \t Loss 1.6115 \t Acc 53.81 \t AccHead 56.68 \t AccTail 23.55\n",
      "Epoch: [041] \t Loss 1.5813 \t Acc 55.34 \t AccHead 58.31 \t AccTail 24.23\n",
      "Epoch: [042] \t Loss 1.5236 \t Acc 55.29 \t AccHead 58.44 \t AccTail 22.25\n",
      "Epoch: [043] \t Loss 1.5258 \t Acc 54.79 \t AccHead 57.59 \t AccTail 25.37\n",
      "Epoch: [044] \t Loss 1.4928 \t Acc 60.72 \t AccHead 63.76 \t AccTail 28.67\n",
      "Epoch: [045] \t Loss 1.4632 \t Acc 55.88 \t AccHead 58.44 \t AccTail 28.90\n",
      "Epoch: [046] \t Loss 1.4698 \t Acc 58.83 \t AccHead 61.60 \t AccTail 29.51\n",
      "Epoch: [047] \t Loss 1.4729 \t Acc 56.97 \t AccHead 59.57 \t AccTail 29.76\n",
      "Epoch: [048] \t Loss 1.4247 \t Acc 45.72 \t AccHead 48.10 \t AccTail 20.83\n",
      "Epoch: [049] \t Loss 1.4357 \t Acc 53.92 \t AccHead 55.97 \t AccTail 32.30\n",
      "Epoch: [050] \t Loss 1.4068 \t Acc 55.01 \t AccHead 57.93 \t AccTail 24.40\n",
      "Epoch: [051] \t Loss 1.3683 \t Acc 55.18 \t AccHead 57.29 \t AccTail 32.98\n",
      "Epoch: [052] \t Loss 1.3923 \t Acc 61.86 \t AccHead 65.04 \t AccTail 28.36\n",
      "Epoch: [053] \t Loss 1.3251 \t Acc 58.79 \t AccHead 61.11 \t AccTail 34.28\n",
      "Epoch: [054] \t Loss 1.3528 \t Acc 54.10 \t AccHead 56.96 \t AccTail 23.89\n",
      "Epoch: [055] \t Loss 1.3236 \t Acc 59.35 \t AccHead 62.26 \t AccTail 28.92\n",
      "Epoch: [056] \t Loss 1.3162 \t Acc 57.72 \t AccHead 59.93 \t AccTail 34.50\n",
      "Epoch: [057] \t Loss 1.3160 \t Acc 64.11 \t AccHead 66.79 \t AccTail 35.76\n",
      "Epoch: [058] \t Loss 1.2784 \t Acc 57.33 \t AccHead 59.74 \t AccTail 32.04\n",
      "Epoch: [059] \t Loss 1.3009 \t Acc 56.68 \t AccHead 59.42 \t AccTail 27.76\n",
      "Epoch: [060] \t Loss 1.2441 \t Acc 62.19 \t AccHead 64.94 \t AccTail 33.24\n",
      "Epoch: [061] \t Loss 1.2513 \t Acc 66.67 \t AccHead 69.30 \t AccTail 38.92\n",
      "Epoch: [062] \t Loss 1.2184 \t Acc 61.64 \t AccHead 64.13 \t AccTail 35.48\n",
      "Epoch: [063] \t Loss 1.2483 \t Acc 63.46 \t AccHead 66.42 \t AccTail 32.17\n",
      "Epoch: [064] \t Loss 1.2060 \t Acc 61.23 \t AccHead 63.08 \t AccTail 41.64\n",
      "Epoch: [065] \t Loss 1.1721 \t Acc 63.01 \t AccHead 65.09 \t AccTail 41.11\n",
      "Epoch: [066] \t Loss 1.1757 \t Acc 62.07 \t AccHead 64.29 \t AccTail 38.71\n",
      "Epoch: [067] \t Loss 1.1869 \t Acc 68.30 \t AccHead 70.43 \t AccTail 45.91\n",
      "Epoch: [068] \t Loss 1.1613 \t Acc 59.93 \t AccHead 62.04 \t AccTail 37.85\n",
      "Epoch: [069] \t Loss 1.1766 \t Acc 68.80 \t AccHead 71.02 \t AccTail 45.36\n",
      "Epoch: [070] \t Loss 1.1487 \t Acc 64.12 \t AccHead 66.41 \t AccTail 40.05\n",
      "Epoch: [071] \t Loss 1.1281 \t Acc 63.81 \t AccHead 65.71 \t AccTail 43.76\n",
      "Epoch: [072] \t Loss 1.1765 \t Acc 61.44 \t AccHead 63.96 \t AccTail 34.82\n",
      "Epoch: [073] \t Loss 1.0982 \t Acc 65.73 \t AccHead 67.68 \t AccTail 45.23\n",
      "Epoch: [074] \t Loss 1.0654 \t Acc 65.86 \t AccHead 67.63 \t AccTail 47.26\n",
      "Epoch: [075] \t Loss 1.1108 \t Acc 68.87 \t AccHead 70.97 \t AccTail 46.63\n",
      "Epoch: [076] \t Loss 1.0710 \t Acc 66.83 \t AccHead 69.04 \t AccTail 43.45\n",
      "Epoch: [077] \t Loss 1.0682 \t Acc 66.03 \t AccHead 67.36 \t AccTail 51.96\n",
      "Epoch: [078] \t Loss 1.0960 \t Acc 61.10 \t AccHead 62.98 \t AccTail 41.42\n",
      "Epoch: [079] \t Loss 1.0376 \t Acc 68.03 \t AccHead 70.05 \t AccTail 46.70\n",
      "Epoch: [080] \t Loss 1.0538 \t Acc 66.87 \t AccHead 68.71 \t AccTail 47.66\n",
      "Epoch: [081] \t Loss 1.0273 \t Acc 65.60 \t AccHead 67.00 \t AccTail 50.87\n",
      "Epoch: [082] \t Loss 1.0750 \t Acc 63.58 \t AccHead 65.64 \t AccTail 42.01\n",
      "Epoch: [083] \t Loss 1.0158 \t Acc 68.92 \t AccHead 71.49 \t AccTail 41.84\n",
      "Epoch: [084] \t Loss 0.9831 \t Acc 58.24 \t AccHead 60.24 \t AccTail 37.35\n",
      "Epoch: [085] \t Loss 1.0589 \t Acc 65.16 \t AccHead 66.68 \t AccTail 49.19\n",
      "Epoch: [086] \t Loss 1.0253 \t Acc 71.63 \t AccHead 73.49 \t AccTail 52.02\n",
      "Epoch: [087] \t Loss 0.9832 \t Acc 70.00 \t AccHead 71.01 \t AccTail 59.35\n",
      "Epoch: [088] \t Loss 1.0572 \t Acc 69.32 \t AccHead 71.40 \t AccTail 47.52\n",
      "Epoch: [089] \t Loss 0.9813 \t Acc 67.08 \t AccHead 68.03 \t AccTail 57.12\n",
      "Epoch: [090] \t Loss 0.9629 \t Acc 68.84 \t AccHead 70.22 \t AccTail 54.31\n",
      "Epoch: [091] \t Loss 0.9812 \t Acc 74.74 \t AccHead 76.34 \t AccTail 57.97\n",
      "Epoch: [092] \t Loss 0.9952 \t Acc 67.91 \t AccHead 69.56 \t AccTail 50.54\n",
      "Epoch: [093] \t Loss 0.9651 \t Acc 66.45 \t AccHead 68.22 \t AccTail 47.70\n",
      "Epoch: [094] \t Loss 0.9759 \t Acc 68.99 \t AccHead 70.56 \t AccTail 52.49\n",
      "Epoch: [095] \t Loss 0.9475 \t Acc 69.76 \t AccHead 71.76 \t AccTail 48.72\n",
      "Epoch: [096] \t Loss 0.9948 \t Acc 71.32 \t AccHead 72.54 \t AccTail 58.36\n",
      "Epoch: [097] \t Loss 0.9545 \t Acc 67.47 \t AccHead 68.99 \t AccTail 51.35\n",
      "Epoch: [098] \t Loss 0.9715 \t Acc 63.62 \t AccHead 64.97 \t AccTail 49.39\n",
      "Epoch: [099] \t Loss 0.9345 \t Acc 70.35 \t AccHead 71.97 \t AccTail 53.40\n",
      "Epoch: [100] \t Loss 0.9267 \t Acc 72.13 \t AccHead 73.39 \t AccTail 58.89\n",
      "Epoch: [101] \t Loss 0.9205 \t Acc 70.34 \t AccHead 71.62 \t AccTail 56.74\n",
      "Epoch: [102] \t Loss 0.9167 \t Acc 73.92 \t AccHead 75.08 \t AccTail 61.55\n",
      "Epoch: [103] \t Loss 0.9253 \t Acc 69.25 \t AccHead 70.85 \t AccTail 52.42\n",
      "Epoch: [104] \t Loss 0.8826 \t Acc 69.33 \t AccHead 70.70 \t AccTail 54.86\n",
      "Epoch: [105] \t Loss 0.8959 \t Acc 69.17 \t AccHead 70.90 \t AccTail 51.07\n",
      "Epoch: [106] \t Loss 0.9060 \t Acc 66.74 \t AccHead 68.33 \t AccTail 49.93\n",
      "Epoch: [107] \t Loss 0.9221 \t Acc 71.16 \t AccHead 72.27 \t AccTail 59.63\n",
      "Epoch: [108] \t Loss 0.8834 \t Acc 61.99 \t AccHead 63.08 \t AccTail 50.41\n",
      "Epoch: [109] \t Loss 0.9828 \t Acc 70.57 \t AccHead 71.23 \t AccTail 63.58\n",
      "Epoch: [110] \t Loss 0.8544 \t Acc 69.83 \t AccHead 70.51 \t AccTail 62.72\n",
      "Epoch: [111] \t Loss 0.8761 \t Acc 69.85 \t AccHead 71.88 \t AccTail 48.38\n",
      "Epoch: [112] \t Loss 0.9321 \t Acc 66.49 \t AccHead 68.13 \t AccTail 49.13\n",
      "Epoch: [113] \t Loss 0.8846 \t Acc 74.70 \t AccHead 76.28 \t AccTail 57.95\n",
      "Epoch: [114] \t Loss 0.8616 \t Acc 73.62 \t AccHead 74.61 \t AccTail 63.32\n",
      "Epoch: [115] \t Loss 0.8614 \t Acc 69.34 \t AccHead 70.75 \t AccTail 54.56\n",
      "Epoch: [116] \t Loss 0.8567 \t Acc 71.93 \t AccHead 73.01 \t AccTail 60.59\n",
      "Epoch: [117] \t Loss 0.9097 \t Acc 70.45 \t AccHead 71.69 \t AccTail 57.51\n",
      "Epoch: [118] \t Loss 0.8921 \t Acc 74.50 \t AccHead 75.39 \t AccTail 65.14\n",
      "Epoch: [119] \t Loss 0.8714 \t Acc 68.67 \t AccHead 69.57 \t AccTail 59.11\n",
      "Epoch: [120] \t Loss 0.8752 \t Acc 71.70 \t AccHead 72.61 \t AccTail 62.13\n",
      "Epoch: [121] \t Loss 0.9077 \t Acc 72.78 \t AccHead 73.75 \t AccTail 62.63\n",
      "Epoch: [122] \t Loss 0.8411 \t Acc 74.86 \t AccHead 76.24 \t AccTail 60.27\n",
      "Epoch: [123] \t Loss 0.8235 \t Acc 74.50 \t AccHead 75.44 \t AccTail 64.62\n",
      "Epoch: [124] \t Loss 0.8311 \t Acc 70.83 \t AccHead 71.43 \t AccTail 64.52\n",
      "Epoch: [125] \t Loss 0.8717 \t Acc 76.71 \t AccHead 77.74 \t AccTail 66.00\n",
      "Epoch: [126] \t Loss 0.8676 \t Acc 66.18 \t AccHead 67.43 \t AccTail 53.15\n",
      "Epoch: [127] \t Loss 0.8431 \t Acc 73.43 \t AccHead 74.56 \t AccTail 61.46\n",
      "Epoch: [128] \t Loss 0.8016 \t Acc 69.69 \t AccHead 71.34 \t AccTail 52.48\n",
      "Epoch: [129] \t Loss 0.8584 \t Acc 67.54 \t AccHead 68.69 \t AccTail 55.38\n",
      "Epoch: [130] \t Loss 0.8196 \t Acc 73.89 \t AccHead 75.34 \t AccTail 58.74\n",
      "Epoch: [131] \t Loss 0.8639 \t Acc 74.92 \t AccHead 76.32 \t AccTail 60.13\n",
      "Epoch: [132] \t Loss 0.8142 \t Acc 71.86 \t AccHead 72.98 \t AccTail 60.13\n",
      "Epoch: [133] \t Loss 0.8390 \t Acc 68.55 \t AccHead 69.94 \t AccTail 53.96\n",
      "Epoch: [134] \t Loss 0.8311 \t Acc 73.16 \t AccHead 74.59 \t AccTail 58.12\n",
      "Epoch: [135] \t Loss 0.8616 \t Acc 74.51 \t AccHead 75.65 \t AccTail 62.55\n",
      "Epoch: [136] \t Loss 0.8253 \t Acc 72.85 \t AccHead 73.76 \t AccTail 63.26\n",
      "Epoch: [137] \t Loss 0.8167 \t Acc 74.16 \t AccHead 75.19 \t AccTail 63.32\n",
      "Epoch: [138] \t Loss 0.8001 \t Acc 71.06 \t AccHead 71.71 \t AccTail 64.25\n",
      "Epoch: [139] \t Loss 0.8543 \t Acc 73.18 \t AccHead 73.65 \t AccTail 68.27\n",
      "Epoch: [140] \t Loss 0.8333 \t Acc 74.66 \t AccHead 75.70 \t AccTail 63.71\n",
      "Epoch: [141] \t Loss 0.7681 \t Acc 67.25 \t AccHead 67.98 \t AccTail 59.49\n",
      "Epoch: [142] \t Loss 0.8262 \t Acc 74.31 \t AccHead 75.26 \t AccTail 64.30\n",
      "Epoch: [143] \t Loss 0.8150 \t Acc 72.00 \t AccHead 72.98 \t AccTail 61.80\n",
      "Epoch: [144] \t Loss 0.8125 \t Acc 73.03 \t AccHead 73.92 \t AccTail 63.62\n",
      "Epoch: [145] \t Loss 0.8207 \t Acc 74.41 \t AccHead 75.16 \t AccTail 66.40\n",
      "Epoch: [146] \t Loss 0.8414 \t Acc 73.15 \t AccHead 73.98 \t AccTail 64.33\n",
      "Epoch: [147] \t Loss 0.8172 \t Acc 72.69 \t AccHead 73.51 \t AccTail 64.12\n",
      "Epoch: [148] \t Loss 0.7629 \t Acc 78.32 \t AccHead 79.45 \t AccTail 66.40\n",
      "Epoch: [149] \t Loss 0.7810 \t Acc 70.31 \t AccHead 70.84 \t AccTail 64.78\n",
      "Epoch: [150] \t Loss 0.8013 \t Acc 77.47 \t AccHead 78.29 \t AccTail 68.86\n",
      "Epoch: [151] \t Loss 0.4060 \t Acc 95.55 \t AccHead 95.83 \t AccTail 92.60\n",
      "Epoch: [152] \t Loss 0.2098 \t Acc 97.34 \t AccHead 97.60 \t AccTail 94.65\n",
      "Epoch: [153] \t Loss 0.1499 \t Acc 98.50 \t AccHead 98.71 \t AccTail 96.23\n",
      "Epoch: [154] \t Loss 0.1231 \t Acc 98.61 \t AccHead 98.72 \t AccTail 97.43\n",
      "Epoch: [155] \t Loss 0.1016 \t Acc 99.01 \t AccHead 99.07 \t AccTail 98.39\n",
      "Epoch: [156] \t Loss 0.0941 \t Acc 99.24 \t AccHead 99.32 \t AccTail 98.37\n",
      "Epoch: [157] \t Loss 0.0798 \t Acc 99.34 \t AccHead 99.40 \t AccTail 98.65\n",
      "Epoch: [158] \t Loss 0.0687 \t Acc 99.49 \t AccHead 99.51 \t AccTail 99.20\n",
      "Epoch: [159] \t Loss 0.0678 \t Acc 99.59 \t AccHead 99.63 \t AccTail 99.19\n",
      "Epoch: [160] \t Loss 0.0600 \t Acc 99.64 \t AccHead 99.67 \t AccTail 99.32\n",
      "Epoch: [161] \t Loss 0.0539 \t Acc 99.59 \t AccHead 99.63 \t AccTail 99.19\n",
      "Epoch: [162] \t Loss 0.0529 \t Acc 99.73 \t AccHead 99.74 \t AccTail 99.60\n",
      "Epoch: [163] \t Loss 0.0488 \t Acc 99.72 \t AccHead 99.76 \t AccTail 99.33\n",
      "Epoch: [164] \t Loss 0.0433 \t Acc 99.83 \t AccHead 99.82 \t AccTail 99.87\n",
      "Epoch: [165] \t Loss 0.0422 \t Acc 99.86 \t AccHead 99.87 \t AccTail 99.73\n",
      "Epoch: [166] \t Loss 0.0405 \t Acc 99.76 \t AccHead 99.77 \t AccTail 99.60\n",
      "Epoch: [167] \t Loss 0.0392 \t Acc 99.87 \t AccHead 99.86 \t AccTail 100.00\n",
      "Epoch: [168] \t Loss 0.0380 \t Acc 99.78 \t AccHead 99.76 \t AccTail 100.00\n",
      "Epoch: [169] \t Loss 0.0349 \t Acc 99.92 \t AccHead 99.91 \t AccTail 100.00\n",
      "Epoch: [170] \t Loss 0.0337 \t Acc 99.86 \t AccHead 99.86 \t AccTail 99.87\n",
      "Epoch: [171] \t Loss 0.0325 \t Acc 99.92 \t AccHead 99.94 \t AccTail 99.73\n",
      "Epoch: [172] \t Loss 0.0297 \t Acc 99.92 \t AccHead 99.92 \t AccTail 99.87\n",
      "Epoch: [173] \t Loss 0.0325 \t Acc 99.91 \t AccHead 99.92 \t AccTail 99.73\n",
      "Epoch: [174] \t Loss 0.0291 \t Acc 99.91 \t AccHead 99.90 \t AccTail 100.00\n",
      "Epoch: [175] \t Loss 0.0283 \t Acc 99.98 \t AccHead 99.99 \t AccTail 99.87\n",
      "Epoch: [176] \t Loss 0.0288 \t Acc 99.98 \t AccHead 99.97 \t AccTail 100.00\n",
      "Epoch: [177] \t Loss 0.0258 \t Acc 100.00 \t AccHead 100.00 \t AccTail 100.00\n",
      "Epoch: [178] \t Loss 0.0276 \t Acc 99.97 \t AccHead 99.96 \t AccTail 100.00\n",
      "Epoch: [179] \t Loss 0.0255 \t Acc 99.94 \t AccHead 99.95 \t AccTail 99.86\n",
      "Epoch: [180] \t Loss 0.0244 \t Acc 99.93 \t AccHead 99.94 \t AccTail 99.87\n",
      "Epoch: [181] \t Loss 0.0238 \t Acc 99.94 \t AccHead 99.95 \t AccTail 99.87\n",
      "Epoch: [182] \t Loss 0.0241 \t Acc 99.90 \t AccHead 99.89 \t AccTail 100.00\n",
      "Epoch: [183] \t Loss 0.0236 \t Acc 99.95 \t AccHead 99.95 \t AccTail 100.00\n",
      "Epoch: [184] \t Loss 0.0218 \t Acc 99.99 \t AccHead 99.99 \t AccTail 100.00\n",
      "Epoch: [185] \t Loss 0.0209 \t Acc 99.99 \t AccHead 99.99 \t AccTail 100.00\n",
      "Epoch: [186] \t Loss 0.0204 \t Acc 100.00 \t AccHead 100.00 \t AccTail 100.00\n",
      "Epoch: [187] \t Loss 0.0196 \t Acc 99.99 \t AccHead 100.00 \t AccTail 99.87\n",
      "Epoch: [188] \t Loss 0.0198 \t Acc 99.98 \t AccHead 99.97 \t AccTail 100.00\n",
      "Epoch: [189] \t Loss 0.0206 \t Acc 99.98 \t AccHead 99.97 \t AccTail 100.00\n",
      "Epoch: [190] \t Loss 0.0189 \t Acc 99.94 \t AccHead 99.96 \t AccTail 99.73\n",
      "Epoch: [191] \t Loss 0.0193 \t Acc 99.99 \t AccHead 99.99 \t AccTail 100.00\n",
      "Epoch: [192] \t Loss 0.0180 \t Acc 100.00 \t AccHead 100.00 \t AccTail 100.00\n",
      "Epoch: [193] \t Loss 0.0209 \t Acc 99.98 \t AccHead 99.97 \t AccTail 100.00\n",
      "Epoch: [194] \t Loss 0.0180 \t Acc 100.00 \t AccHead 100.00 \t AccTail 100.00\n",
      "Epoch: [195] \t Loss 0.0172 \t Acc 99.99 \t AccHead 99.99 \t AccTail 100.00\n",
      "Epoch: [196] \t Loss 0.0190 \t Acc 99.97 \t AccHead 99.99 \t AccTail 99.73\n",
      "Epoch: [197] \t Loss 0.0177 \t Acc 100.00 \t AccHead 100.00 \t AccTail 100.00\n",
      "Epoch: [198] \t Loss 0.0171 \t Acc 100.00 \t AccHead 100.00 \t AccTail 100.00\n",
      "Epoch: [199] \t Loss 0.0175 \t Acc 100.00 \t AccHead 100.00 \t AccTail 100.00\n",
      "Epoch: [200] \t Loss 0.0176 \t Acc 99.99 \t AccHead 99.99 \t AccTail 100.00\n",
      "199\n",
      "Finished Training\n",
      "Acc 7.68 \t AccHead 15.33 \t AccTail 0.08\n",
      "----------------------------\n",
      "get the top-1 accuracy for each of the classes.\n",
      "Accuracy of apple : 81 %\n",
      "Accuracy of aquarium_fish : 71 %\n",
      "Accuracy of  baby : 50 %\n",
      "Accuracy of  bear : 31 %\n",
      "Accuracy of beaver : 40 %\n",
      "Accuracy of   bed : 55 %\n",
      "Accuracy of   bee : 40 %\n",
      "Accuracy of beetle : 42 %\n",
      "Accuracy of bicycle : 35 %\n",
      "Accuracy of bottle : 36 %\n",
      "Accuracy of  bowl : 24 %\n",
      "Accuracy of   boy : 25 %\n",
      "Accuracy of bridge : 25 %\n",
      "Accuracy of   bus : 35 %\n",
      "Accuracy of butterfly : 22 %\n",
      "Accuracy of camel : 23 %\n",
      "Accuracy of   can : 14 %\n",
      "Accuracy of castle : 16 %\n",
      "Accuracy of caterpillar :  9 %\n",
      "Accuracy of cattle :  8 %\n",
      "Accuracy of chair : 13 %\n",
      "Accuracy of chimpanzee : 12 %\n",
      "Accuracy of clock :  8 %\n",
      "Accuracy of cloud :  4 %\n",
      "Accuracy of cockroach :  1 %\n",
      "Accuracy of couch :  1 %\n",
      "Accuracy of  crab :  0 %\n",
      "Accuracy of crocodile :  0 %\n",
      "Accuracy of   cup :  0 %\n",
      "Accuracy of dinosaur :  2 %\n",
      "Accuracy of dolphin :  1 %\n",
      "Accuracy of elephant :  3 %\n",
      "Accuracy of flatfish :  1 %\n",
      "Accuracy of forest :  0 %\n",
      "Accuracy of   fox :  1 %\n",
      "Accuracy of  girl :  3 %\n",
      "Accuracy of hamster :  0 %\n",
      "Accuracy of house :  0 %\n",
      "Accuracy of kangaroo :  1 %\n",
      "Accuracy of keyboard :  0 %\n",
      "Accuracy of  lamp :  0 %\n",
      "Accuracy of lawn_mower :  0 %\n",
      "Accuracy of leopard :  2 %\n",
      "Accuracy of  lion :  0 %\n",
      "Accuracy of lizard :  0 %\n",
      "Accuracy of lobster :  0 %\n",
      "Accuracy of   man :  0 %\n",
      "Accuracy of maple_tree :  1 %\n",
      "Accuracy of motorcycle :  0 %\n",
      "Accuracy of mountain :  0 %\n",
      "Accuracy of mouse :  0 %\n",
      "Accuracy of mushroom :  0 %\n",
      "Accuracy of oak_tree :  0 %\n",
      "Accuracy of orange :  0 %\n",
      "Accuracy of orchid :  0 %\n",
      "Accuracy of otter :  0 %\n",
      "Accuracy of palm_tree :  1 %\n",
      "Accuracy of  pear :  1 %\n",
      "Accuracy of pickup_truck :  0 %\n",
      "Accuracy of pine_tree :  0 %\n",
      "Accuracy of plain :  0 %\n",
      "Accuracy of plate :  0 %\n",
      "Accuracy of poppy :  0 %\n",
      "Accuracy of porcupine :  0 %\n",
      "Accuracy of possum :  0 %\n",
      "Accuracy of rabbit :  0 %\n",
      "Accuracy of raccoon :  0 %\n",
      "Accuracy of   ray :  0 %\n",
      "Accuracy of  road :  1 %\n",
      "Accuracy of rocket :  1 %\n",
      "Accuracy of  rose :  0 %\n",
      "Accuracy of   sea :  0 %\n",
      "Accuracy of  seal :  0 %\n",
      "Accuracy of shark :  0 %\n",
      "Accuracy of shrew :  0 %\n",
      "Accuracy of skunk :  0 %\n",
      "Accuracy of skyscraper :  0 %\n",
      "Accuracy of snail :  0 %\n",
      "Accuracy of snake :  0 %\n",
      "Accuracy of spider :  0 %\n",
      "Accuracy of squirrel :  0 %\n",
      "Accuracy of streetcar :  0 %\n",
      "Accuracy of sunflower :  0 %\n",
      "Accuracy of sweet_pepper :  0 %\n",
      "Accuracy of table :  0 %\n",
      "Accuracy of  tank :  0 %\n",
      "Accuracy of telephone :  0 %\n",
      "Accuracy of television :  0 %\n",
      "Accuracy of tiger :  0 %\n",
      "Accuracy of tractor :  0 %\n",
      "Accuracy of train :  0 %\n",
      "Accuracy of trout :  0 %\n",
      "Accuracy of tulip :  0 %\n",
      "Accuracy of turtle :  0 %\n",
      "Accuracy of wardrobe :  0 %\n",
      "Accuracy of whale :  0 %\n",
      "Accuracy of willow_tree :  0 %\n",
      "Accuracy of  wolf :  0 %\n",
      "Accuracy of woman :  0 %\n",
      "Accuracy of  worm :  0 %\n",
      "0.07400000000000001\n"
     ]
    }
   ],
   "source": [
    "train_dataset, train_loader, test_loader, num_classes = get_loaders()\n",
    "model = ResNet18(num_classes)\n",
    "model = model.cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=150, gamma=0.1)\n",
    "\n",
    "# Let's build our model\n",
    "train(EPOCHS)\n",
    "print('Finished Training')\n",
    "test() \n",
    "print('----------------------------')\n",
    "print('get the top-1 accuracy for each of the classes.')\n",
    "avg_accu = testEeahClass()\n",
    "tau = np.mean(np.array(avg_accu)) / 100\n",
    "print(tau)\n",
    "tau_list.append(tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dOo_VWUbLQmk",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-22T18:34:41.853522Z",
     "iopub.status.busy": "2022-06-22T18:34:41.853353Z",
     "iopub.status.idle": "2022-06-22T18:34:41.858854Z",
     "shell.execute_reply": "2022-06-22T18:34:41.858254Z",
     "shell.execute_reply.started": "2022-06-22T18:34:41.853501Z"
    },
    "id": "dOo_VWUbLQmk",
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATASET = 'CIFAR100' #['CIFAR10', 'CIFAR100']\n",
    "IMB_TYPE = 'step' #['exp', 'step']\n",
    "IMB_FACTOR = 0.1 #[0.1, 0.01]\n",
    "SAVE_DIR = 'logs/100-step-0.1-find_tau' \n",
    "\n",
    "LR = 0.1\n",
    "BATCH_SIZE = 128\n",
    "MOMENTUM = 0.9\n",
    "WEIGHT_DECAY = 0.000479\n",
    "EPOCHS = 200\n",
    "\n",
    "os.makedirs(osp.join(SAVE_DIR), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "MkzbNiHOPfaX",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-22T18:34:41.860522Z",
     "iopub.status.busy": "2022-06-22T18:34:41.860147Z",
     "iopub.status.idle": "2022-06-22T19:05:17.083489Z",
     "shell.execute_reply": "2022-06-22T19:05:17.082469Z",
     "shell.execute_reply.started": "2022-06-22T18:34:41.860500Z"
    },
    "id": "MkzbNiHOPfaX",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "cls num list(train_dataset):\n",
      "[400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 400, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40]\n",
      "cls num list(val_dataset):\n",
      "[100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100]\n",
      "Epoch: [001] \t Loss 4.3447 \t Acc 8.34 \t AccHead 9.48 \t AccTail 0.00\n",
      "Epoch: [002] \t Loss 3.8399 \t Acc 11.33 \t AccHead 12.62 \t AccTail 1.90\n",
      "Epoch: [003] \t Loss 3.6578 \t Acc 13.64 \t AccHead 14.93 \t AccTail 4.33\n",
      "Epoch: [004] \t Loss 3.5259 \t Acc 16.40 \t AccHead 17.77 \t AccTail 6.43\n",
      "Epoch: [005] \t Loss 3.3930 \t Acc 19.12 \t AccHead 21.00 \t AccTail 5.48\n",
      "Epoch: [006] \t Loss 3.2690 \t Acc 21.14 \t AccHead 23.29 \t AccTail 5.48\n",
      "Epoch: [007] \t Loss 3.1504 \t Acc 22.00 \t AccHead 24.16 \t AccTail 6.21\n",
      "Epoch: [008] \t Loss 3.0492 \t Acc 23.97 \t AccHead 26.26 \t AccTail 7.30\n",
      "Epoch: [009] \t Loss 2.9500 \t Acc 26.54 \t AccHead 28.49 \t AccTail 12.34\n",
      "Epoch: [010] \t Loss 2.8530 \t Acc 28.09 \t AccHead 30.25 \t AccTail 12.37\n",
      "Epoch: [011] \t Loss 2.7634 \t Acc 29.37 \t AccHead 31.61 \t AccTail 13.05\n",
      "Epoch: [012] \t Loss 2.6944 \t Acc 32.08 \t AccHead 34.34 \t AccTail 15.62\n",
      "Epoch: [013] \t Loss 2.6208 \t Acc 33.24 \t AccHead 35.79 \t AccTail 14.66\n",
      "Epoch: [014] \t Loss 2.5647 \t Acc 32.68 \t AccHead 34.76 \t AccTail 17.61\n",
      "Epoch: [015] \t Loss 2.5264 \t Acc 34.66 \t AccHead 37.10 \t AccTail 16.91\n",
      "Epoch: [016] \t Loss 2.4584 \t Acc 34.53 \t AccHead 36.66 \t AccTail 19.05\n",
      "Epoch: [017] \t Loss 2.4320 \t Acc 35.77 \t AccHead 38.29 \t AccTail 17.44\n",
      "Epoch: [018] \t Loss 2.3899 \t Acc 36.52 \t AccHead 39.02 \t AccTail 18.25\n",
      "Epoch: [019] \t Loss 2.3507 \t Acc 38.01 \t AccHead 40.58 \t AccTail 19.31\n",
      "Epoch: [020] \t Loss 2.3154 \t Acc 37.78 \t AccHead 40.43 \t AccTail 18.50\n",
      "Epoch: [021] \t Loss 2.2777 \t Acc 37.82 \t AccHead 40.68 \t AccTail 17.00\n",
      "Epoch: [022] \t Loss 2.2605 \t Acc 40.77 \t AccHead 43.72 \t AccTail 19.28\n",
      "Epoch: [023] \t Loss 2.2121 \t Acc 38.62 \t AccHead 41.23 \t AccTail 19.64\n",
      "Epoch: [024] \t Loss 2.1699 \t Acc 39.97 \t AccHead 42.21 \t AccTail 23.69\n",
      "Epoch: [025] \t Loss 2.1823 \t Acc 40.35 \t AccHead 42.82 \t AccTail 22.36\n",
      "Epoch: [026] \t Loss 2.1216 \t Acc 41.65 \t AccHead 44.14 \t AccTail 23.50\n",
      "Epoch: [027] \t Loss 2.1179 \t Acc 39.24 \t AccHead 41.72 \t AccTail 21.17\n",
      "Epoch: [028] \t Loss 2.0764 \t Acc 44.38 \t AccHead 47.00 \t AccTail 25.31\n",
      "Epoch: [029] \t Loss 2.0554 \t Acc 43.12 \t AccHead 46.12 \t AccTail 21.33\n",
      "Epoch: [030] \t Loss 2.0378 \t Acc 43.70 \t AccHead 46.64 \t AccTail 22.25\n",
      "Epoch: [031] \t Loss 1.9987 \t Acc 43.27 \t AccHead 45.14 \t AccTail 29.70\n",
      "Epoch: [032] \t Loss 1.9617 \t Acc 44.52 \t AccHead 46.55 \t AccTail 29.70\n",
      "Epoch: [033] \t Loss 1.9556 \t Acc 45.32 \t AccHead 48.77 \t AccTail 20.20\n",
      "Epoch: [034] \t Loss 1.9250 \t Acc 48.24 \t AccHead 50.21 \t AccTail 33.88\n",
      "Epoch: [035] \t Loss 1.8972 \t Acc 46.32 \t AccHead 48.90 \t AccTail 27.53\n",
      "Epoch: [036] \t Loss 1.8862 \t Acc 46.80 \t AccHead 49.35 \t AccTail 28.22\n",
      "Epoch: [037] \t Loss 1.8634 \t Acc 47.94 \t AccHead 49.87 \t AccTail 33.87\n",
      "Epoch: [038] \t Loss 1.8473 \t Acc 46.77 \t AccHead 48.73 \t AccTail 32.51\n",
      "Epoch: [039] \t Loss 1.8355 \t Acc 48.24 \t AccHead 50.76 \t AccTail 29.80\n",
      "Epoch: [040] \t Loss 1.8355 \t Acc 50.80 \t AccHead 53.62 \t AccTail 30.21\n",
      "Epoch: [041] \t Loss 1.7860 \t Acc 48.92 \t AccHead 51.24 \t AccTail 32.04\n",
      "Epoch: [042] \t Loss 1.7627 \t Acc 49.73 \t AccHead 51.77 \t AccTail 34.90\n",
      "Epoch: [043] \t Loss 1.7682 \t Acc 52.10 \t AccHead 54.88 \t AccTail 31.94\n",
      "Epoch: [044] \t Loss 1.7359 \t Acc 50.71 \t AccHead 52.64 \t AccTail 36.67\n",
      "Epoch: [045] \t Loss 1.7103 \t Acc 52.21 \t AccHead 54.37 \t AccTail 36.46\n",
      "Epoch: [046] \t Loss 1.6884 \t Acc 51.49 \t AccHead 53.93 \t AccTail 33.65\n",
      "Epoch: [047] \t Loss 1.6747 \t Acc 53.01 \t AccHead 55.57 \t AccTail 34.36\n",
      "Epoch: [048] \t Loss 1.6760 \t Acc 52.50 \t AccHead 53.76 \t AccTail 43.27\n",
      "Epoch: [049] \t Loss 1.6437 \t Acc 54.47 \t AccHead 56.17 \t AccTail 42.15\n",
      "Epoch: [050] \t Loss 1.6253 \t Acc 52.20 \t AccHead 54.14 \t AccTail 38.02\n",
      "Epoch: [051] \t Loss 1.6209 \t Acc 54.04 \t AccHead 55.73 \t AccTail 41.70\n",
      "Epoch: [052] \t Loss 1.5942 \t Acc 52.69 \t AccHead 54.42 \t AccTail 40.10\n",
      "Epoch: [053] \t Loss 1.6023 \t Acc 54.96 \t AccHead 56.75 \t AccTail 41.96\n",
      "Epoch: [054] \t Loss 1.5883 \t Acc 52.49 \t AccHead 54.27 \t AccTail 39.51\n",
      "Epoch: [055] \t Loss 1.5445 \t Acc 52.20 \t AccHead 54.79 \t AccTail 33.28\n",
      "Epoch: [056] \t Loss 1.5548 \t Acc 50.24 \t AccHead 51.80 \t AccTail 38.86\n",
      "Epoch: [057] \t Loss 1.5521 \t Acc 57.85 \t AccHead 59.53 \t AccTail 45.56\n",
      "Epoch: [058] \t Loss 1.5047 \t Acc 57.39 \t AccHead 58.83 \t AccTail 46.88\n",
      "Epoch: [059] \t Loss 1.4951 \t Acc 56.70 \t AccHead 58.51 \t AccTail 43.53\n",
      "Epoch: [060] \t Loss 1.4811 \t Acc 56.98 \t AccHead 58.41 \t AccTail 46.58\n",
      "Epoch: [061] \t Loss 1.4795 \t Acc 56.08 \t AccHead 57.93 \t AccTail 42.67\n",
      "Epoch: [062] \t Loss 1.4609 \t Acc 59.28 \t AccHead 61.16 \t AccTail 45.54\n",
      "Epoch: [063] \t Loss 1.4447 \t Acc 56.92 \t AccHead 59.33 \t AccTail 39.31\n",
      "Epoch: [064] \t Loss 1.4363 \t Acc 61.47 \t AccHead 63.73 \t AccTail 45.04\n",
      "Epoch: [065] \t Loss 1.4082 \t Acc 58.83 \t AccHead 60.53 \t AccTail 46.50\n",
      "Epoch: [066] \t Loss 1.4217 \t Acc 57.45 \t AccHead 59.65 \t AccTail 41.49\n",
      "Epoch: [067] \t Loss 1.4175 \t Acc 57.77 \t AccHead 59.26 \t AccTail 46.88\n",
      "Epoch: [068] \t Loss 1.3914 \t Acc 57.84 \t AccHead 60.11 \t AccTail 41.27\n",
      "Epoch: [069] \t Loss 1.3718 \t Acc 62.83 \t AccHead 64.32 \t AccTail 51.92\n",
      "Epoch: [070] \t Loss 1.3684 \t Acc 59.22 \t AccHead 60.82 \t AccTail 47.64\n",
      "Epoch: [071] \t Loss 1.3616 \t Acc 57.06 \t AccHead 58.60 \t AccTail 45.87\n",
      "Epoch: [072] \t Loss 1.3549 \t Acc 63.11 \t AccHead 64.41 \t AccTail 53.63\n",
      "Epoch: [073] \t Loss 1.3153 \t Acc 62.63 \t AccHead 64.03 \t AccTail 52.51\n",
      "Epoch: [074] \t Loss 1.3453 \t Acc 62.90 \t AccHead 64.58 \t AccTail 50.63\n",
      "Epoch: [075] \t Loss 1.3091 \t Acc 60.56 \t AccHead 62.01 \t AccTail 49.98\n",
      "Epoch: [076] \t Loss 1.3126 \t Acc 59.95 \t AccHead 61.58 \t AccTail 48.10\n",
      "Epoch: [077] \t Loss 1.3052 \t Acc 58.50 \t AccHead 59.95 \t AccTail 47.95\n",
      "Epoch: [078] \t Loss 1.2930 \t Acc 65.53 \t AccHead 66.97 \t AccTail 55.10\n",
      "Epoch: [079] \t Loss 1.2830 \t Acc 66.18 \t AccHead 67.48 \t AccTail 56.66\n",
      "Epoch: [080] \t Loss 1.2597 \t Acc 61.89 \t AccHead 64.44 \t AccTail 43.34\n",
      "Epoch: [081] \t Loss 1.2753 \t Acc 60.65 \t AccHead 61.75 \t AccTail 52.65\n",
      "Epoch: [082] \t Loss 1.2626 \t Acc 64.97 \t AccHead 66.12 \t AccTail 56.57\n",
      "Epoch: [083] \t Loss 1.2493 \t Acc 63.19 \t AccHead 64.76 \t AccTail 51.82\n",
      "Epoch: [084] \t Loss 1.2437 \t Acc 64.56 \t AccHead 65.46 \t AccTail 57.99\n",
      "Epoch: [085] \t Loss 1.2377 \t Acc 63.39 \t AccHead 64.65 \t AccTail 54.21\n",
      "Epoch: [086] \t Loss 1.2281 \t Acc 60.73 \t AccHead 62.31 \t AccTail 49.24\n",
      "Epoch: [087] \t Loss 1.2247 \t Acc 63.69 \t AccHead 65.04 \t AccTail 53.88\n",
      "Epoch: [088] \t Loss 1.2168 \t Acc 63.65 \t AccHead 64.50 \t AccTail 57.45\n",
      "Epoch: [089] \t Loss 1.2052 \t Acc 62.68 \t AccHead 63.61 \t AccTail 55.94\n",
      "Epoch: [090] \t Loss 1.1854 \t Acc 66.42 \t AccHead 67.88 \t AccTail 55.80\n",
      "Epoch: [091] \t Loss 1.1981 \t Acc 64.37 \t AccHead 65.23 \t AccTail 58.14\n",
      "Epoch: [092] \t Loss 1.1709 \t Acc 64.70 \t AccHead 66.21 \t AccTail 53.71\n",
      "Epoch: [093] \t Loss 1.1648 \t Acc 63.37 \t AccHead 64.66 \t AccTail 54.00\n",
      "Epoch: [094] \t Loss 1.1571 \t Acc 63.99 \t AccHead 64.64 \t AccTail 59.27\n",
      "Epoch: [095] \t Loss 1.1708 \t Acc 67.31 \t AccHead 68.54 \t AccTail 58.38\n",
      "Epoch: [096] \t Loss 1.1699 \t Acc 67.77 \t AccHead 68.78 \t AccTail 60.40\n",
      "Epoch: [097] \t Loss 1.1246 \t Acc 67.17 \t AccHead 68.29 \t AccTail 59.06\n",
      "Epoch: [098] \t Loss 1.1708 \t Acc 68.08 \t AccHead 68.91 \t AccTail 62.06\n",
      "Epoch: [099] \t Loss 1.1559 \t Acc 67.35 \t AccHead 67.78 \t AccTail 64.24\n",
      "Epoch: [100] \t Loss 1.1374 \t Acc 67.62 \t AccHead 68.72 \t AccTail 59.59\n",
      "Epoch: [101] \t Loss 1.1103 \t Acc 65.87 \t AccHead 67.46 \t AccTail 54.36\n",
      "Epoch: [102] \t Loss 1.1287 \t Acc 63.47 \t AccHead 64.60 \t AccTail 55.27\n",
      "Epoch: [103] \t Loss 1.1488 \t Acc 67.95 \t AccHead 69.20 \t AccTail 58.91\n",
      "Epoch: [104] \t Loss 1.1219 \t Acc 65.38 \t AccHead 65.68 \t AccTail 63.20\n",
      "Epoch: [105] \t Loss 1.0980 \t Acc 69.71 \t AccHead 70.12 \t AccTail 66.69\n",
      "Epoch: [106] \t Loss 1.1015 \t Acc 65.57 \t AccHead 66.24 \t AccTail 60.74\n",
      "Epoch: [107] \t Loss 1.1314 \t Acc 67.24 \t AccHead 67.90 \t AccTail 62.42\n",
      "Epoch: [108] \t Loss 1.1054 \t Acc 66.71 \t AccHead 67.33 \t AccTail 62.27\n",
      "Epoch: [109] \t Loss 1.0943 \t Acc 67.07 \t AccHead 68.15 \t AccTail 59.18\n",
      "Epoch: [110] \t Loss 1.0886 \t Acc 69.21 \t AccHead 70.44 \t AccTail 60.33\n",
      "Epoch: [111] \t Loss 1.1201 \t Acc 67.80 \t AccHead 68.90 \t AccTail 59.79\n",
      "Epoch: [112] \t Loss 1.0976 \t Acc 68.37 \t AccHead 69.49 \t AccTail 60.22\n",
      "Epoch: [113] \t Loss 1.0593 \t Acc 67.98 \t AccHead 68.41 \t AccTail 64.91\n",
      "Epoch: [114] \t Loss 1.0803 \t Acc 67.56 \t AccHead 68.52 \t AccTail 60.55\n",
      "Epoch: [115] \t Loss 1.0856 \t Acc 67.48 \t AccHead 68.49 \t AccTail 60.08\n",
      "Epoch: [116] \t Loss 1.0740 \t Acc 66.32 \t AccHead 67.08 \t AccTail 60.78\n",
      "Epoch: [117] \t Loss 1.0820 \t Acc 70.00 \t AccHead 71.19 \t AccTail 61.29\n",
      "Epoch: [118] \t Loss 1.0663 \t Acc 64.71 \t AccHead 65.18 \t AccTail 61.28\n",
      "Epoch: [119] \t Loss 1.0546 \t Acc 63.54 \t AccHead 64.44 \t AccTail 57.01\n",
      "Epoch: [120] \t Loss 1.0604 \t Acc 71.89 \t AccHead 73.13 \t AccTail 62.86\n",
      "Epoch: [121] \t Loss 1.0627 \t Acc 68.58 \t AccHead 69.50 \t AccTail 61.92\n",
      "Epoch: [122] \t Loss 1.0702 \t Acc 65.22 \t AccHead 66.33 \t AccTail 57.14\n",
      "Epoch: [123] \t Loss 1.0588 \t Acc 67.84 \t AccHead 68.98 \t AccTail 59.55\n",
      "Epoch: [124] \t Loss 1.0449 \t Acc 70.28 \t AccHead 71.06 \t AccTail 64.59\n",
      "Epoch: [125] \t Loss 1.0442 \t Acc 66.62 \t AccHead 67.27 \t AccTail 61.88\n",
      "Epoch: [126] \t Loss 1.0599 \t Acc 68.48 \t AccHead 70.02 \t AccTail 57.30\n",
      "Epoch: [127] \t Loss 1.0542 \t Acc 66.07 \t AccHead 67.15 \t AccTail 58.20\n",
      "Epoch: [128] \t Loss 1.0297 \t Acc 68.36 \t AccHead 68.83 \t AccTail 64.96\n",
      "Epoch: [129] \t Loss 1.0358 \t Acc 69.69 \t AccHead 70.28 \t AccTail 65.37\n",
      "Epoch: [130] \t Loss 1.0152 \t Acc 71.12 \t AccHead 71.65 \t AccTail 67.30\n",
      "Epoch: [131] \t Loss 1.0150 \t Acc 69.32 \t AccHead 70.45 \t AccTail 61.09\n",
      "Epoch: [132] \t Loss 1.0272 \t Acc 68.91 \t AccHead 70.05 \t AccTail 60.62\n",
      "Epoch: [133] \t Loss 1.0232 \t Acc 68.29 \t AccHead 68.78 \t AccTail 64.75\n",
      "Epoch: [134] \t Loss 1.0139 \t Acc 67.58 \t AccHead 68.16 \t AccTail 63.37\n",
      "Epoch: [135] \t Loss 1.0331 \t Acc 70.74 \t AccHead 71.40 \t AccTail 65.91\n",
      "Epoch: [136] \t Loss 1.0137 \t Acc 70.30 \t AccHead 70.41 \t AccTail 69.51\n",
      "Epoch: [137] \t Loss 0.9995 \t Acc 72.33 \t AccHead 73.26 \t AccTail 65.53\n",
      "Epoch: [138] \t Loss 1.0246 \t Acc 70.04 \t AccHead 70.66 \t AccTail 65.53\n",
      "Epoch: [139] \t Loss 0.9994 \t Acc 71.62 \t AccHead 72.46 \t AccTail 65.49\n",
      "Epoch: [140] \t Loss 0.9781 \t Acc 71.88 \t AccHead 72.76 \t AccTail 65.41\n",
      "Epoch: [141] \t Loss 1.0291 \t Acc 69.58 \t AccHead 70.85 \t AccTail 60.32\n",
      "Epoch: [142] \t Loss 0.9962 \t Acc 71.38 \t AccHead 72.11 \t AccTail 66.11\n",
      "Epoch: [143] \t Loss 0.9879 \t Acc 67.97 \t AccHead 69.12 \t AccTail 59.64\n",
      "Epoch: [144] \t Loss 1.0096 \t Acc 71.35 \t AccHead 71.96 \t AccTail 66.88\n",
      "Epoch: [145] \t Loss 0.9618 \t Acc 74.53 \t AccHead 75.07 \t AccTail 70.59\n",
      "Epoch: [146] \t Loss 1.0260 \t Acc 70.00 \t AccHead 71.54 \t AccTail 58.80\n",
      "Epoch: [147] \t Loss 0.9880 \t Acc 70.11 \t AccHead 70.89 \t AccTail 64.47\n",
      "Epoch: [148] \t Loss 1.0092 \t Acc 69.66 \t AccHead 70.07 \t AccTail 66.63\n",
      "Epoch: [149] \t Loss 0.9944 \t Acc 71.06 \t AccHead 71.59 \t AccTail 67.24\n",
      "Epoch: [150] \t Loss 0.9918 \t Acc 72.64 \t AccHead 73.45 \t AccTail 66.69\n",
      "Epoch: [151] \t Loss 0.5219 \t Acc 93.41 \t AccHead 93.49 \t AccTail 92.78\n",
      "Epoch: [152] \t Loss 0.2895 \t Acc 95.84 \t AccHead 95.82 \t AccTail 95.92\n",
      "Epoch: [153] \t Loss 0.2206 \t Acc 96.92 \t AccHead 96.95 \t AccTail 96.69\n",
      "Epoch: [154] \t Loss 0.1796 \t Acc 97.64 \t AccHead 97.71 \t AccTail 97.15\n",
      "Epoch: [155] \t Loss 0.1524 \t Acc 98.03 \t AccHead 98.07 \t AccTail 97.79\n",
      "Epoch: [156] \t Loss 0.1354 \t Acc 98.41 \t AccHead 98.44 \t AccTail 98.25\n",
      "Epoch: [157] \t Loss 0.1164 \t Acc 98.62 \t AccHead 98.61 \t AccTail 98.67\n",
      "Epoch: [158] \t Loss 0.1040 \t Acc 98.82 \t AccHead 98.86 \t AccTail 98.52\n",
      "Epoch: [159] \t Loss 0.0950 \t Acc 98.85 \t AccHead 98.79 \t AccTail 99.28\n",
      "Epoch: [160] \t Loss 0.0863 \t Acc 99.07 \t AccHead 99.01 \t AccTail 99.51\n",
      "Epoch: [161] \t Loss 0.0809 \t Acc 99.26 \t AccHead 99.20 \t AccTail 99.66\n",
      "Epoch: [162] \t Loss 0.0757 \t Acc 99.29 \t AccHead 99.29 \t AccTail 99.31\n",
      "Epoch: [163] \t Loss 0.0698 \t Acc 99.40 \t AccHead 99.38 \t AccTail 99.50\n",
      "Epoch: [164] \t Loss 0.0658 \t Acc 99.33 \t AccHead 99.29 \t AccTail 99.58\n",
      "Epoch: [165] \t Loss 0.0609 \t Acc 99.47 \t AccHead 99.43 \t AccTail 99.77\n",
      "Epoch: [166] \t Loss 0.0580 \t Acc 99.43 \t AccHead 99.39 \t AccTail 99.73\n",
      "Epoch: [167] \t Loss 0.0530 \t Acc 99.62 \t AccHead 99.59 \t AccTail 99.85\n",
      "Epoch: [168] \t Loss 0.0508 \t Acc 99.60 \t AccHead 99.57 \t AccTail 99.81\n",
      "Epoch: [169] \t Loss 0.0475 \t Acc 99.60 \t AccHead 99.60 \t AccTail 99.62\n",
      "Epoch: [170] \t Loss 0.0477 \t Acc 99.60 \t AccHead 99.57 \t AccTail 99.85\n",
      "Epoch: [171] \t Loss 0.0441 \t Acc 99.62 \t AccHead 99.60 \t AccTail 99.73\n",
      "Epoch: [172] \t Loss 0.0423 \t Acc 99.67 \t AccHead 99.66 \t AccTail 99.77\n",
      "Epoch: [173] \t Loss 0.0386 \t Acc 99.66 \t AccHead 99.64 \t AccTail 99.77\n",
      "Epoch: [174] \t Loss 0.0374 \t Acc 99.72 \t AccHead 99.71 \t AccTail 99.77\n",
      "Epoch: [175] \t Loss 0.0366 \t Acc 99.72 \t AccHead 99.69 \t AccTail 99.92\n",
      "Epoch: [176] \t Loss 0.0380 \t Acc 99.70 \t AccHead 99.66 \t AccTail 99.96\n",
      "Epoch: [177] \t Loss 0.0352 \t Acc 99.72 \t AccHead 99.70 \t AccTail 99.85\n",
      "Epoch: [178] \t Loss 0.0334 \t Acc 99.77 \t AccHead 99.77 \t AccTail 99.77\n",
      "Epoch: [179] \t Loss 0.0342 \t Acc 99.73 \t AccHead 99.71 \t AccTail 99.89\n",
      "Epoch: [180] \t Loss 0.0316 \t Acc 99.75 \t AccHead 99.73 \t AccTail 99.89\n",
      "Epoch: [181] \t Loss 0.0333 \t Acc 99.77 \t AccHead 99.74 \t AccTail 100.00\n",
      "Epoch: [182] \t Loss 0.0306 \t Acc 99.73 \t AccHead 99.72 \t AccTail 99.85\n",
      "Epoch: [183] \t Loss 0.0297 \t Acc 99.83 \t AccHead 99.81 \t AccTail 99.96\n",
      "Epoch: [184] \t Loss 0.0295 \t Acc 99.75 \t AccHead 99.73 \t AccTail 99.92\n",
      "Epoch: [185] \t Loss 0.0261 \t Acc 99.78 \t AccHead 99.77 \t AccTail 99.89\n",
      "Epoch: [186] \t Loss 0.0287 \t Acc 99.82 \t AccHead 99.81 \t AccTail 99.92\n",
      "Epoch: [187] \t Loss 0.0288 \t Acc 99.78 \t AccHead 99.76 \t AccTail 99.92\n",
      "Epoch: [188] \t Loss 0.0258 \t Acc 99.81 \t AccHead 99.79 \t AccTail 99.96\n",
      "Epoch: [189] \t Loss 0.0280 \t Acc 99.76 \t AccHead 99.73 \t AccTail 99.92\n",
      "Epoch: [190] \t Loss 0.0271 \t Acc 99.76 \t AccHead 99.76 \t AccTail 99.73\n",
      "Epoch: [191] \t Loss 0.0280 \t Acc 99.80 \t AccHead 99.79 \t AccTail 99.89\n",
      "Epoch: [192] \t Loss 0.0259 \t Acc 99.83 \t AccHead 99.82 \t AccTail 99.96\n",
      "Epoch: [193] \t Loss 0.0254 \t Acc 99.77 \t AccHead 99.74 \t AccTail 99.92\n",
      "Epoch: [194] \t Loss 0.0233 \t Acc 99.78 \t AccHead 99.75 \t AccTail 100.00\n",
      "Epoch: [195] \t Loss 0.0261 \t Acc 99.77 \t AccHead 99.76 \t AccTail 99.81\n",
      "Epoch: [196] \t Loss 0.0246 \t Acc 99.73 \t AccHead 99.70 \t AccTail 100.00\n",
      "Epoch: [197] \t Loss 0.0254 \t Acc 99.77 \t AccHead 99.74 \t AccTail 99.96\n",
      "Epoch: [198] \t Loss 0.0248 \t Acc 99.78 \t AccHead 99.78 \t AccTail 99.85\n",
      "Epoch: [199] \t Loss 0.0242 \t Acc 99.81 \t AccHead 99.79 \t AccTail 99.96\n",
      "Epoch: [200] \t Loss 0.0211 \t Acc 99.83 \t AccHead 99.82 \t AccTail 99.89\n",
      "199\n",
      "Finished Training\n",
      "Acc 11.45 \t AccHead 22.25 \t AccTail 0.78\n",
      "----------------------------\n",
      "get the top-1 accuracy for each of the classes.\n",
      "Accuracy of apple : 71 %\n",
      "Accuracy of aquarium_fish : 62 %\n",
      "Accuracy of  baby : 38 %\n",
      "Accuracy of  bear : 29 %\n",
      "Accuracy of beaver : 33 %\n",
      "Accuracy of   bed : 47 %\n",
      "Accuracy of   bee : 34 %\n",
      "Accuracy of beetle : 38 %\n",
      "Accuracy of bicycle : 27 %\n",
      "Accuracy of bottle : 34 %\n",
      "Accuracy of  bowl : 27 %\n",
      "Accuracy of   boy : 15 %\n",
      "Accuracy of bridge : 22 %\n",
      "Accuracy of   bus : 36 %\n",
      "Accuracy of butterfly : 28 %\n",
      "Accuracy of camel : 24 %\n",
      "Accuracy of   can : 35 %\n",
      "Accuracy of castle : 35 %\n",
      "Accuracy of caterpillar : 27 %\n",
      "Accuracy of cattle : 16 %\n",
      "Accuracy of chair : 39 %\n",
      "Accuracy of chimpanzee : 39 %\n",
      "Accuracy of clock : 27 %\n",
      "Accuracy of cloud : 31 %\n",
      "Accuracy of cockroach : 29 %\n",
      "Accuracy of couch : 10 %\n",
      "Accuracy of  crab : 11 %\n",
      "Accuracy of crocodile : 14 %\n",
      "Accuracy of   cup : 17 %\n",
      "Accuracy of dinosaur : 14 %\n",
      "Accuracy of dolphin :  8 %\n",
      "Accuracy of elephant : 10 %\n",
      "Accuracy of flatfish : 12 %\n",
      "Accuracy of forest :  9 %\n",
      "Accuracy of   fox :  7 %\n",
      "Accuracy of  girl : 12 %\n",
      "Accuracy of hamster :  8 %\n",
      "Accuracy of house :  6 %\n",
      "Accuracy of kangaroo : 11 %\n",
      "Accuracy of keyboard :  9 %\n",
      "Accuracy of  lamp : 10 %\n",
      "Accuracy of lawn_mower : 15 %\n",
      "Accuracy of leopard : 13 %\n",
      "Accuracy of  lion :  8 %\n",
      "Accuracy of lizard :  6 %\n",
      "Accuracy of lobster :  4 %\n",
      "Accuracy of   man :  4 %\n",
      "Accuracy of maple_tree :  7 %\n",
      "Accuracy of motorcycle :  3 %\n",
      "Accuracy of mountain :  8 %\n",
      "Accuracy of mouse :  0 %\n",
      "Accuracy of mushroom :  0 %\n",
      "Accuracy of oak_tree :  0 %\n",
      "Accuracy of orange :  0 %\n",
      "Accuracy of orchid :  0 %\n",
      "Accuracy of otter :  0 %\n",
      "Accuracy of palm_tree :  0 %\n",
      "Accuracy of  pear :  0 %\n",
      "Accuracy of pickup_truck :  0 %\n",
      "Accuracy of pine_tree :  0 %\n",
      "Accuracy of plain :  0 %\n",
      "Accuracy of plate :  0 %\n",
      "Accuracy of poppy :  1 %\n",
      "Accuracy of porcupine :  0 %\n",
      "Accuracy of possum :  0 %\n",
      "Accuracy of rabbit :  0 %\n",
      "Accuracy of raccoon :  0 %\n",
      "Accuracy of   ray :  0 %\n",
      "Accuracy of  road :  0 %\n",
      "Accuracy of rocket :  0 %\n",
      "Accuracy of  rose :  0 %\n",
      "Accuracy of   sea :  0 %\n",
      "Accuracy of  seal :  0 %\n",
      "Accuracy of shark :  0 %\n",
      "Accuracy of shrew :  0 %\n",
      "Accuracy of skunk :  1 %\n",
      "Accuracy of skyscraper :  0 %\n",
      "Accuracy of snail :  0 %\n",
      "Accuracy of snake :  0 %\n",
      "Accuracy of spider :  1 %\n",
      "Accuracy of squirrel :  0 %\n",
      "Accuracy of streetcar :  0 %\n",
      "Accuracy of sunflower :  0 %\n",
      "Accuracy of sweet_pepper :  3 %\n",
      "Accuracy of table :  0 %\n",
      "Accuracy of  tank :  0 %\n",
      "Accuracy of telephone :  0 %\n",
      "Accuracy of television :  0 %\n",
      "Accuracy of tiger :  0 %\n",
      "Accuracy of tractor :  0 %\n",
      "Accuracy of train :  4 %\n",
      "Accuracy of trout :  0 %\n",
      "Accuracy of tulip :  5 %\n",
      "Accuracy of turtle :  4 %\n",
      "Accuracy of wardrobe :  2 %\n",
      "Accuracy of whale :  6 %\n",
      "Accuracy of willow_tree :  0 %\n",
      "Accuracy of  wolf :  0 %\n",
      "Accuracy of woman :  0 %\n",
      "Accuracy of  worm :  0 %\n",
      "0.1106\n"
     ]
    }
   ],
   "source": [
    "train_dataset, train_loader, test_loader, num_classes = get_loaders()\n",
    "model = ResNet18(num_classes)\n",
    "model = model.cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=150, gamma=0.1)\n",
    "\n",
    "# Let's build our model\n",
    "train(EPOCHS)\n",
    "print('Finished Training')\n",
    "test() \n",
    "print('----------------------------')\n",
    "print('get the top-1 accuracy for each of the classes.')\n",
    "avg_accu = testEeahClass()\n",
    "tau = np.mean(np.array(avg_accu)) / 100\n",
    "print(tau)\n",
    "tau_list.append(tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "gsX5duw8LQtJ",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-22T19:05:17.085778Z",
     "iopub.status.busy": "2022-06-22T19:05:17.085290Z",
     "iopub.status.idle": "2022-06-22T19:05:17.092672Z",
     "shell.execute_reply": "2022-06-22T19:05:17.091827Z",
     "shell.execute_reply.started": "2022-06-22T19:05:17.085724Z"
    },
    "id": "gsX5duw8LQtJ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATASET = 'CIFAR10' #['CIFAR10', 'CIFAR100']\n",
    "IMB_TYPE = 'step' #['exp', 'step']\n",
    "IMB_FACTOR = 0.01 #[0.1, 0.01]\n",
    "SAVE_DIR = 'logs/100-step-0.01-find_tau' \n",
    "\n",
    "LR = 0.1\n",
    "BATCH_SIZE = 128\n",
    "MOMENTUM = 0.9\n",
    "WEIGHT_DECAY = 0.000670\n",
    "EPOCHS = 80\n",
    "\n",
    "os.makedirs(osp.join(SAVE_DIR), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "StM6A3JgPf-8",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2022-06-22T19:05:17.094106Z",
     "iopub.status.busy": "2022-06-22T19:05:17.093754Z",
     "iopub.status.idle": "2022-06-22T19:22:39.548056Z",
     "shell.execute_reply": "2022-06-22T19:22:39.546254Z",
     "shell.execute_reply.started": "2022-06-22T19:05:17.094077Z"
    },
    "id": "StM6A3JgPf-8",
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "cls num list(train_dataset):\n",
      "[4000, 4000, 4000, 4000, 4000, 40, 40, 40, 40, 40]\n",
      "cls num list(val_dataset):\n",
      "[1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000]\n",
      "Epoch: [001] \t Loss 2.2103 \t Acc 34.07 \t AccHead 28.44 \t AccTail 55.90\n",
      "Epoch: [002] \t Loss 1.3190 \t Acc 50.48 \t AccHead 47.97 \t AccTail 60.20\n",
      "Epoch: [003] \t Loss 1.1541 \t Acc 57.66 \t AccHead 56.60 \t AccTail 61.79\n",
      "Epoch: [004] \t Loss 1.0652 \t Acc 58.89 \t AccHead 55.73 \t AccTail 71.17\n",
      "Epoch: [005] \t Loss 1.0092 \t Acc 64.00 \t AccHead 67.05 \t AccTail 52.17\n",
      "Epoch: [006] \t Loss 0.9409 \t Acc 62.83 \t AccHead 61.27 \t AccTail 68.88\n",
      "Epoch: [007] \t Loss 0.8846 \t Acc 68.49 \t AccHead 71.82 \t AccTail 55.56\n",
      "Epoch: [008] \t Loss 0.8329 \t Acc 68.68 \t AccHead 72.01 \t AccTail 55.76\n",
      "Epoch: [009] \t Loss 0.7897 \t Acc 72.26 \t AccHead 73.49 \t AccTail 67.49\n",
      "Epoch: [010] \t Loss 0.7600 \t Acc 71.80 \t AccHead 76.95 \t AccTail 51.82\n",
      "Epoch: [011] \t Loss 0.7318 \t Acc 72.30 \t AccHead 71.73 \t AccTail 74.51\n",
      "Epoch: [012] \t Loss 0.7128 \t Acc 76.50 \t AccHead 78.00 \t AccTail 70.66\n",
      "Epoch: [013] \t Loss 0.6901 \t Acc 74.93 \t AccHead 75.71 \t AccTail 71.90\n",
      "Epoch: [014] \t Loss 0.6700 \t Acc 73.15 \t AccHead 74.86 \t AccTail 66.54\n",
      "Epoch: [015] \t Loss 0.6692 \t Acc 69.63 \t AccHead 73.70 \t AccTail 53.86\n",
      "Epoch: [016] \t Loss 0.6482 \t Acc 77.74 \t AccHead 80.09 \t AccTail 68.61\n",
      "Epoch: [017] \t Loss 0.6353 \t Acc 75.35 \t AccHead 80.05 \t AccTail 57.13\n",
      "Epoch: [018] \t Loss 0.6286 \t Acc 78.02 \t AccHead 78.38 \t AccTail 76.60\n",
      "Epoch: [019] \t Loss 0.6100 \t Acc 79.22 \t AccHead 80.87 \t AccTail 72.86\n",
      "Epoch: [020] \t Loss 0.6060 \t Acc 78.60 \t AccHead 76.97 \t AccTail 84.92\n",
      "Epoch: [021] \t Loss 0.5985 \t Acc 81.18 \t AccHead 82.59 \t AccTail 75.71\n",
      "Epoch: [022] \t Loss 0.5859 \t Acc 77.91 \t AccHead 80.91 \t AccTail 66.30\n",
      "Epoch: [023] \t Loss 0.5991 \t Acc 78.77 \t AccHead 78.86 \t AccTail 78.42\n",
      "Epoch: [024] \t Loss 0.5749 \t Acc 78.23 \t AccHead 76.81 \t AccTail 83.77\n",
      "Epoch: [025] \t Loss 0.5698 \t Acc 77.11 \t AccHead 81.26 \t AccTail 61.00\n",
      "Epoch: [026] \t Loss 0.5704 \t Acc 79.57 \t AccHead 79.68 \t AccTail 79.16\n",
      "Epoch: [027] \t Loss 0.5674 \t Acc 78.96 \t AccHead 84.11 \t AccTail 58.98\n",
      "Epoch: [028] \t Loss 0.5560 \t Acc 77.61 \t AccHead 75.33 \t AccTail 86.48\n",
      "Epoch: [029] \t Loss 0.5577 \t Acc 75.13 \t AccHead 71.34 \t AccTail 89.84\n",
      "Epoch: [030] \t Loss 0.5468 \t Acc 80.93 \t AccHead 81.92 \t AccTail 77.10\n",
      "Epoch: [031] \t Loss 0.5551 \t Acc 75.46 \t AccHead 73.36 \t AccTail 83.57\n",
      "Epoch: [032] \t Loss 0.5487 \t Acc 79.87 \t AccHead 79.55 \t AccTail 81.12\n",
      "Epoch: [033] \t Loss 0.5392 \t Acc 81.41 \t AccHead 85.19 \t AccTail 66.75\n",
      "Epoch: [034] \t Loss 0.5340 \t Acc 82.17 \t AccHead 82.35 \t AccTail 81.46\n",
      "Epoch: [035] \t Loss 0.5442 \t Acc 78.11 \t AccHead 75.49 \t AccTail 88.28\n",
      "Epoch: [036] \t Loss 0.5335 \t Acc 82.05 \t AccHead 81.69 \t AccTail 83.41\n",
      "Epoch: [037] \t Loss 0.5357 \t Acc 81.70 \t AccHead 84.79 \t AccTail 69.73\n",
      "Epoch: [038] \t Loss 0.5238 \t Acc 82.88 \t AccHead 85.04 \t AccTail 74.48\n",
      "Epoch: [039] \t Loss 0.5248 \t Acc 79.49 \t AccHead 80.02 \t AccTail 77.41\n",
      "Epoch: [040] \t Loss 0.5402 \t Acc 82.36 \t AccHead 83.30 \t AccTail 78.70\n",
      "Epoch: [041] \t Loss 0.5184 \t Acc 82.21 \t AccHead 85.49 \t AccTail 69.50\n",
      "Epoch: [042] \t Loss 0.5222 \t Acc 80.29 \t AccHead 79.43 \t AccTail 83.63\n",
      "Epoch: [043] \t Loss 0.5240 \t Acc 82.36 \t AccHead 85.90 \t AccTail 68.66\n",
      "Epoch: [044] \t Loss 0.5170 \t Acc 80.76 \t AccHead 86.60 \t AccTail 58.15\n",
      "Epoch: [045] \t Loss 0.5045 \t Acc 80.53 \t AccHead 79.31 \t AccTail 85.28\n",
      "Epoch: [046] \t Loss 0.5052 \t Acc 83.99 \t AccHead 84.52 \t AccTail 81.91\n",
      "Epoch: [047] \t Loss 0.5146 \t Acc 81.13 \t AccHead 81.86 \t AccTail 78.29\n",
      "Epoch: [048] \t Loss 0.5175 \t Acc 81.91 \t AccHead 81.75 \t AccTail 82.50\n",
      "Epoch: [049] \t Loss 0.5073 \t Acc 80.92 \t AccHead 84.02 \t AccTail 68.92\n",
      "Epoch: [050] \t Loss 0.5147 \t Acc 81.65 \t AccHead 82.30 \t AccTail 79.14\n",
      "Epoch: [051] \t Loss 0.5055 \t Acc 80.36 \t AccHead 84.94 \t AccTail 62.62\n",
      "Epoch: [052] \t Loss 0.5077 \t Acc 80.07 \t AccHead 81.37 \t AccTail 75.02\n",
      "Epoch: [053] \t Loss 0.5033 \t Acc 82.52 \t AccHead 83.09 \t AccTail 80.34\n",
      "Epoch: [054] \t Loss 0.5033 \t Acc 80.41 \t AccHead 86.87 \t AccTail 55.33\n",
      "Epoch: [055] \t Loss 0.5054 \t Acc 83.16 \t AccHead 82.16 \t AccTail 87.02\n",
      "Epoch: [056] \t Loss 0.5067 \t Acc 79.68 \t AccHead 79.01 \t AccTail 82.28\n",
      "Epoch: [057] \t Loss 0.4989 \t Acc 81.11 \t AccHead 83.49 \t AccTail 71.88\n",
      "Epoch: [058] \t Loss 0.4959 \t Acc 81.55 \t AccHead 80.53 \t AccTail 85.52\n",
      "Epoch: [059] \t Loss 0.4947 \t Acc 83.58 \t AccHead 83.85 \t AccTail 82.57\n",
      "Epoch: [060] \t Loss 0.4903 \t Acc 81.88 \t AccHead 81.93 \t AccTail 81.69\n",
      "Epoch: [061] \t Loss 0.4874 \t Acc 83.78 \t AccHead 84.39 \t AccTail 81.40\n",
      "Epoch: [062] \t Loss 0.4884 \t Acc 79.00 \t AccHead 77.81 \t AccTail 83.60\n",
      "Epoch: [063] \t Loss 0.4985 \t Acc 79.80 \t AccHead 82.40 \t AccTail 69.75\n",
      "Epoch: [064] \t Loss 0.4915 \t Acc 83.41 \t AccHead 84.48 \t AccTail 79.27\n",
      "Epoch: [065] \t Loss 0.4953 \t Acc 82.94 \t AccHead 85.54 \t AccTail 72.86\n",
      "Epoch: [066] \t Loss 0.4990 \t Acc 83.91 \t AccHead 83.59 \t AccTail 85.15\n",
      "Epoch: [067] \t Loss 0.4829 \t Acc 83.18 \t AccHead 84.61 \t AccTail 77.64\n",
      "Epoch: [068] \t Loss 0.4928 \t Acc 80.19 \t AccHead 77.50 \t AccTail 90.62\n",
      "Epoch: [069] \t Loss 0.4856 \t Acc 82.87 \t AccHead 87.26 \t AccTail 65.83\n",
      "Epoch: [070] \t Loss 0.4872 \t Acc 83.48 \t AccHead 83.40 \t AccTail 83.77\n",
      "Epoch: [071] \t Loss 0.4809 \t Acc 80.19 \t AccHead 78.61 \t AccTail 86.30\n",
      "Epoch: [072] \t Loss 0.4834 \t Acc 82.00 \t AccHead 83.24 \t AccTail 77.19\n",
      "Epoch: [073] \t Loss 0.4899 \t Acc 81.82 \t AccHead 83.36 \t AccTail 75.86\n",
      "Epoch: [074] \t Loss 0.4961 \t Acc 83.22 \t AccHead 83.38 \t AccTail 82.57\n",
      "Epoch: [075] \t Loss 0.4846 \t Acc 83.13 \t AccHead 82.98 \t AccTail 83.73\n",
      "Epoch: [076] \t Loss 0.4788 \t Acc 80.08 \t AccHead 82.05 \t AccTail 72.42\n",
      "Epoch: [077] \t Loss 0.4913 \t Acc 79.59 \t AccHead 79.90 \t AccTail 78.40\n",
      "Epoch: [078] \t Loss 0.4826 \t Acc 82.59 \t AccHead 81.66 \t AccTail 86.19\n",
      "Epoch: [079] \t Loss 0.4756 \t Acc 83.79 \t AccHead 84.51 \t AccTail 81.00\n",
      "Epoch: [080] \t Loss 0.4840 \t Acc 83.47 \t AccHead 82.98 \t AccTail 85.39\n",
      "Epoch: [081] \t Loss 0.4762 \t Acc 83.07 \t AccHead 84.68 \t AccTail 76.83\n",
      "Epoch: [082] \t Loss 0.4864 \t Acc 81.94 \t AccHead 82.86 \t AccTail 78.38\n",
      "Epoch: [083] \t Loss 0.4859 \t Acc 80.05 \t AccHead 84.12 \t AccTail 64.28\n",
      "Epoch: [084] \t Loss 0.4869 \t Acc 82.63 \t AccHead 85.86 \t AccTail 70.13\n",
      "Epoch: [085] \t Loss 0.4747 \t Acc 81.25 \t AccHead 85.38 \t AccTail 65.22\n",
      "Epoch: [086] \t Loss 0.4744 \t Acc 79.22 \t AccHead 76.75 \t AccTail 88.83\n",
      "Epoch: [087] \t Loss 0.4801 \t Acc 82.13 \t AccHead 80.64 \t AccTail 87.89\n",
      "Epoch: [088] \t Loss 0.4732 \t Acc 83.62 \t AccHead 85.76 \t AccTail 75.35\n",
      "Epoch: [089] \t Loss 0.4852 \t Acc 79.02 \t AccHead 85.22 \t AccTail 54.95\n",
      "Epoch: [090] \t Loss 0.4747 \t Acc 80.88 \t AccHead 82.81 \t AccTail 73.42\n",
      "Epoch: [091] \t Loss 0.4809 \t Acc 83.40 \t AccHead 86.51 \t AccTail 71.33\n",
      "Epoch: [092] \t Loss 0.4833 \t Acc 84.83 \t AccHead 84.97 \t AccTail 84.30\n",
      "Epoch: [093] \t Loss 0.4721 \t Acc 83.79 \t AccHead 84.64 \t AccTail 80.49\n",
      "Epoch: [094] \t Loss 0.4704 \t Acc 80.38 \t AccHead 81.95 \t AccTail 74.31\n",
      "Epoch: [095] \t Loss 0.4743 \t Acc 83.73 \t AccHead 84.84 \t AccTail 79.41\n",
      "Epoch: [096] \t Loss 0.4712 \t Acc 83.27 \t AccHead 84.18 \t AccTail 79.72\n",
      "Epoch: [097] \t Loss 0.4880 \t Acc 83.24 \t AccHead 85.60 \t AccTail 74.08\n",
      "Epoch: [098] \t Loss 0.4785 \t Acc 80.95 \t AccHead 80.04 \t AccTail 84.46\n",
      "Epoch: [099] \t Loss 0.4729 \t Acc 82.18 \t AccHead 80.88 \t AccTail 87.21\n",
      "Epoch: [100] \t Loss 0.4743 \t Acc 85.71 \t AccHead 87.04 \t AccTail 80.56\n",
      "Epoch: [101] \t Loss 0.4723 \t Acc 82.41 \t AccHead 85.32 \t AccTail 71.14\n",
      "Epoch: [102] \t Loss 0.4751 \t Acc 81.94 \t AccHead 85.61 \t AccTail 67.73\n",
      "Epoch: [103] \t Loss 0.4652 \t Acc 83.91 \t AccHead 83.84 \t AccTail 84.15\n",
      "Epoch: [104] \t Loss 0.4639 \t Acc 82.82 \t AccHead 86.31 \t AccTail 69.28\n",
      "Epoch: [105] \t Loss 0.4712 \t Acc 84.59 \t AccHead 85.36 \t AccTail 81.64\n",
      "Epoch: [106] \t Loss 0.4733 \t Acc 82.47 \t AccHead 87.05 \t AccTail 64.71\n",
      "Epoch: [107] \t Loss 0.4658 \t Acc 84.51 \t AccHead 85.63 \t AccTail 80.18\n",
      "Epoch: [108] \t Loss 0.4645 \t Acc 83.73 \t AccHead 86.22 \t AccTail 74.05\n",
      "Epoch: [109] \t Loss 0.4707 \t Acc 80.10 \t AccHead 80.93 \t AccTail 76.87\n",
      "Epoch: [110] \t Loss 0.4659 \t Acc 80.99 \t AccHead 83.33 \t AccTail 71.93\n",
      "Epoch: [111] \t Loss 0.4714 \t Acc 85.35 \t AccHead 85.72 \t AccTail 83.91\n",
      "Epoch: [112] \t Loss 0.4726 \t Acc 80.65 \t AccHead 85.00 \t AccTail 63.80\n",
      "Epoch: [113] \t Loss 0.4693 \t Acc 82.24 \t AccHead 84.76 \t AccTail 72.48\n",
      "Epoch: [114] \t Loss 0.4614 \t Acc 78.37 \t AccHead 83.81 \t AccTail 57.30\n",
      "Epoch: [115] \t Loss 0.4682 \t Acc 83.47 \t AccHead 86.63 \t AccTail 71.24\n",
      "Epoch: [116] \t Loss 0.4639 \t Acc 84.50 \t AccHead 87.00 \t AccTail 74.82\n",
      "Epoch: [117] \t Loss 0.4603 \t Acc 82.95 \t AccHead 82.86 \t AccTail 83.31\n",
      "Epoch: [118] \t Loss 0.4782 \t Acc 82.95 \t AccHead 84.32 \t AccTail 77.63\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [45]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mStepLR(optimizer, step_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m150\u001b[39m, gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Let's build our model\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFinished Training\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     11\u001b[0m test() \n",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(EPOCHS)\u001b[0m\n\u001b[1;32m     14\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     16\u001b[0m     loss_history\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m---> 18\u001b[0m topk_acc, head_acc, tail_acc \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_accuracy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m loss_mean \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(loss_history)\n\u001b[1;32m     20\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/student18b/misc/project/utils.py:369\u001b[0m, in \u001b[0;36mcompute_accuracy\u001b[0;34m(data_loader, model, topk)\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_accuracy\u001b[39m(data_loader, model, topk\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m,)):\n\u001b[0;32m--> 369\u001b[0m     pred_list, label_list \u001b[38;5;241m=\u001b[39m \u001b[43m_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    370\u001b[0m     pred_list \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(pred_list, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    371\u001b[0m     label_list \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(label_list, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/student18b/misc/project/utils.py:334\u001b[0m, in \u001b[0;36m_predict\u001b[0;34m(data_loader, model)\u001b[0m\n\u001b[1;32m    331\u001b[0m images, labels \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m    332\u001b[0m images, labels \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mcuda(), labels\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[0;32m--> 334\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    336\u001b[0m pred_list\u001b[38;5;241m.\u001b[39mappend(outputs)\n\u001b[1;32m    337\u001b[0m label_list\u001b[38;5;241m.\u001b[39mappend(labels)\n",
      "File \u001b[0;32m~/.venv/torch1.9.0-py3.8-cuda11.1/lib/python3.8/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36mResNet18.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m      8\u001b[0m     batch_size \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m----> 9\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackbone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39madaptive_max_pool2d(x, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     11\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(batch_size, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/.venv/torch1.9.0-py3.8-cuda11.1/lib/python3.8/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/student18b/misc/project/utils.py:302\u001b[0m, in \u001b[0;36mResNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    300\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer2(x)\n\u001b[1;32m    301\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer3(x)\n\u001b[0;32m--> 302\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer4\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/.venv/torch1.9.0-py3.8-cuda11.1/lib/python3.8/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.venv/torch1.9.0-py3.8-cuda11.1/lib/python3.8/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/.venv/torch1.9.0-py3.8-cuda11.1/lib/python3.8/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/student18b/misc/project/utils.py:197\u001b[0m, in \u001b[0;36mBasicBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    195\u001b[0m     residual \u001b[38;5;241m=\u001b[39m x\n\u001b[0;32m--> 197\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    198\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(out)\n\u001b[1;32m    199\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(out)\n",
      "File \u001b[0;32m~/.venv/torch1.9.0-py3.8-cuda11.1/lib/python3.8/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.venv/torch1.9.0-py3.8-cuda11.1/lib/python3.8/site-packages/torch/nn/modules/conv.py:443\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 443\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.venv/torch1.9.0-py3.8-cuda11.1/lib/python3.8/site-packages/torch/nn/modules/conv.py:439\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    437\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    438\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 439\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    440\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_dataset, train_loader, test_loader, num_classes = get_loaders()\n",
    "model = ResNet18(num_classes)\n",
    "model = model.cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=150, gamma=0.1)\n",
    "\n",
    "# Let's build our model\n",
    "train(EPOCHS)\n",
    "print('Finished Training')\n",
    "test() \n",
    "print('----------------------------')\n",
    "print('get the top-1 accuracy for each of the classes.')\n",
    "avg_accu = testEeahClass()\n",
    "tau = np.mean(np.array(avg_accu)) / 100\n",
    "print(tau)\n",
    "tau_list.append(tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c62eb706-d24d-4599-982e-369195186593",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-22T19:22:58.224402Z",
     "iopub.status.busy": "2022-06-22T19:22:58.223935Z",
     "iopub.status.idle": "2022-06-22T19:23:02.267732Z",
     "shell.execute_reply": "2022-06-22T19:23:02.266200Z",
     "shell.execute_reply.started": "2022-06-22T19:22:58.224343Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc 41.38 \t AccHead 81.92 \t AccTail 0.04\n",
      "----------------------------\n",
      "get the top-1 accuracy for each of the classes.\n",
      "Accuracy of airplane : 89 %\n",
      "Accuracy of automobile : 91 %\n",
      "Accuracy of  bird : 81 %\n",
      "Accuracy of   cat : 70 %\n",
      "Accuracy of  deer : 75 %\n",
      "Accuracy of   dog :  0 %\n",
      "Accuracy of  frog :  0 %\n",
      "Accuracy of horse :  0 %\n",
      "Accuracy of  ship :  0 %\n",
      "Accuracy of truck :  0 %\n",
      "0.40980000000000005\n"
     ]
    }
   ],
   "source": [
    "test() \n",
    "print('----------------------------')\n",
    "print('get the top-1 accuracy for each of the classes.')\n",
    "avg_accu = testEeahClass()\n",
    "tau = np.mean(np.array(avg_accu)) / 100\n",
    "print(tau)\n",
    "tau_list.append(tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8fpiwiYcK8Ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-22T19:23:25.590869Z",
     "iopub.status.busy": "2022-06-22T19:23:25.590388Z",
     "iopub.status.idle": "2022-06-22T19:23:25.599452Z",
     "shell.execute_reply": "2022-06-22T19:23:25.598447Z",
     "shell.execute_reply.started": "2022-06-22T19:23:25.590818Z"
    },
    "id": "8fpiwiYcK8Ae",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.67859012345679,\n",
       " 0.49658367346938775,\n",
       " 0.6087934655775963,\n",
       " 0.44349999999999995,\n",
       " 0.0175,\n",
       " 0.07400000000000001,\n",
       " 0.1106,\n",
       " 0.40980000000000005]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tau_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "661ef574-99e9-4807-828a-2c67c97d251a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-23T03:45:34.692457Z",
     "iopub.status.busy": "2022-06-23T03:45:34.691974Z",
     "iopub.status.idle": "2022-06-23T03:45:34.700214Z",
     "shell.execute_reply": "2022-06-23T03:45:34.699081Z",
     "shell.execute_reply.started": "2022-06-23T03:45:34.692403Z"
    }
   },
   "outputs": [],
   "source": [
    "tau_dict = {'10_exp_1': 0.67859012345679, '10_exp_01':0.49658367346938775, '10_step_1': 0.6087934655775963, '10_step_01': 0.44349999999999995, '100_exp_1': 0.0175, '100_exp_01': 0.07400000000000001, '100_step_1': 0.1106, '100_step_01': 0.40980000000000005}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "9fe5f797-1ba7-4be1-9c09-ccc3ef92dfe0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-23T03:46:25.693935Z",
     "iopub.status.busy": "2022-06-23T03:46:25.693408Z",
     "iopub.status.idle": "2022-06-23T03:46:25.704942Z",
     "shell.execute_reply": "2022-06-23T03:46:25.703925Z",
     "shell.execute_reply.started": "2022-06-23T03:46:25.693879Z"
    }
   },
   "outputs": [],
   "source": [
    "file1 = open(\"tau.txt\", \"w\") \n",
    "str1 = repr(tau_dict)\n",
    "file1.write(\"dict1 = \" + str1 + \"\\n\")\n",
    "file1.close()\n",
    " \n",
    "f = open('tau.txt', 'r')\n",
    "if f.mode=='r':\n",
    "    contents= f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef2d949-fbe7-4bbe-9dcc-2e6bb2bf8db5",
   "metadata": {
    "id": "4ef2d949-fbe7-4bbe-9dcc-2e6bb2bf8db5"
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4127a91e-94c9-47dc-bca2-51e9c26a34f1",
   "metadata": {
    "id": "4127a91e-94c9-47dc-bca2-51e9c26a34f1"
   },
   "source": [
    "## Base model (no weight decay) and Base model + $\\tau$-normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "56dc2a74-d184-42dd-ba88-2229ea76e470",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-22T19:24:11.152154Z",
     "iopub.status.busy": "2022-06-22T19:24:11.151709Z",
     "iopub.status.idle": "2022-06-22T19:24:11.162311Z",
     "shell.execute_reply": "2022-06-22T19:24:11.161501Z",
     "shell.execute_reply.started": "2022-06-22T19:24:11.152103Z"
    },
    "id": "56dc2a74-d184-42dd-ba88-2229ea76e470",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_loaders():\n",
    "    if DATASET == 'CIFAR10':\n",
    "        train_dataset = IMBALANCECIFAR10(root='../dataset/project', imb_type=IMB_TYPE, imb_factor=IMB_FACTOR, train=True, download=True, transform=transform_train)\n",
    "        test_dataset = torchvision.datasets.CIFAR10(root='../dataset/project', train=False, download=True, transform=transform_test)\n",
    "    elif DATASET == 'CIFAR100':\n",
    "        train_dataset = IMBALANCECIFAR100(root='../dataset/project', imb_type=IMB_TYPE, imb_factor=IMB_FACTOR, train=True, download=True, transform=transform_train)\n",
    "        test_dataset = torchvision.datasets.CIFAR100(root='../dataset/project', train=False, download=True, transform=transform_test)\n",
    "\n",
    "    cls_num_list = train_dataset.get_cls_num_list()\n",
    "    print('cls num list:')\n",
    "    print(cls_num_list)\n",
    "    num_classes = len(cls_num_list)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
    "        num_workers=4, drop_last=True)\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset, batch_size=100, shuffle=False,\n",
    "        num_workers=4, )\n",
    "\n",
    "    return train_dataset, train_loader, test_loader, num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "21a3e816-6e5f-4ae7-800e-c79a190132d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-22T19:24:12.163529Z",
     "iopub.status.busy": "2022-06-22T19:24:12.163056Z",
     "iopub.status.idle": "2022-06-22T19:24:12.173401Z",
     "shell.execute_reply": "2022-06-22T19:24:12.172333Z",
     "shell.execute_reply.started": "2022-06-22T19:24:12.163476Z"
    },
    "id": "21a3e816-6e5f-4ae7-800e-c79a190132d0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ResNet18(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ResNet18, self).__init__()\n",
    "        self.backbone = resnet18()\n",
    "        self.classifier = nn.Linear(512, num_classes)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        x = self.backbone(x)\n",
    "        x = F.adaptive_max_pool2d(x, 1)\n",
    "        x = x.view(batch_size, -1)\n",
    "        pred = self.classifier(x)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c175ff04-88a2-4c78-b13c-87645b6b56ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-22T19:24:13.035475Z",
     "iopub.status.busy": "2022-06-22T19:24:13.035001Z",
     "iopub.status.idle": "2022-06-22T19:24:13.049138Z",
     "shell.execute_reply": "2022-06-22T19:24:13.047953Z",
     "shell.execute_reply.started": "2022-06-22T19:24:13.035423Z"
    },
    "id": "c175ff04-88a2-4c78-b13c-87645b6b56ef",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(EPOCHS):\n",
    "    for epoch in range(EPOCHS):\n",
    "        loss_history = []\n",
    "        model.train()\n",
    "        for batch_index, data in enumerate(train_loader):\n",
    "            image, target = data\n",
    "            image, target = image.cuda(), target.cuda()\n",
    "\n",
    "            pred = model(image)\n",
    "            loss = criterion(pred, target)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_history.append(loss.item())\n",
    "\n",
    "        topk_acc, head_acc, tail_acc = compute_accuracy(train_loader, model)\n",
    "        loss_mean = np.mean(loss_history)\n",
    "        scheduler.step()\n",
    "\n",
    "        print('Epoch: [{:03d}] \\t Loss {:.4f} \\t Acc {:.2f} \\t AccHead {:.2f} \\t AccTail {:.2f}'.format(epoch+1, loss_mean, topk_acc[0], head_acc[0], tail_acc[0]))\n",
    "    \n",
    "    print(epoch)\n",
    "    torch.save({\n",
    "        'model': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'epoch': epoch},\n",
    "        osp.join(SAVE_DIR, 'ep{:03d}.pth'.format(EPOCHS))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8af40bc5-55be-4a20-84d5-1c05a24bd890",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-22T19:24:13.895328Z",
     "iopub.status.busy": "2022-06-22T19:24:13.894820Z",
     "iopub.status.idle": "2022-06-22T19:24:13.903600Z",
     "shell.execute_reply": "2022-06-22T19:24:13.902186Z",
     "shell.execute_reply.started": "2022-06-22T19:24:13.895274Z"
    },
    "id": "8af40bc5-55be-4a20-84d5-1c05a24bd890",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test():\n",
    "    model.eval()\n",
    "    topk_acc, head_acc, tail_acc = compute_accuracy(test_loader, model)\n",
    "    # head : training sample개수가 많은 집함, tail : training samplg이 적은 아이들 -> head의 성능저하를 최소화하고 tail의 성능을 올리는 것이 목표\n",
    "    print('Acc {:.2f} \\t AccHead {:.2f} \\t AccTail {:.2f}'.format(topk_acc[0], head_acc[0], tail_acc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e51cf2b5-e179-402a-915c-43445baacad9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-22T19:24:15.180076Z",
     "iopub.status.busy": "2022-06-22T19:24:15.179613Z",
     "iopub.status.idle": "2022-06-22T19:24:15.194434Z",
     "shell.execute_reply": "2022-06-22T19:24:15.193503Z",
     "shell.execute_reply.started": "2022-06-22T19:24:15.180025Z"
    },
    "id": "e51cf2b5-e179-402a-915c-43445baacad9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def testEeahClass():\n",
    "    class_correct = list(0. for i in range(num_classes))\n",
    "    class_total = list(0. for i in range(num_classes))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            images, labels = data\n",
    "            images, labels = images.cuda(), labels.cuda()\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            c = (predicted == labels).squeeze()\n",
    "            for i in range(len(labels)):  # batch size of test_loader\n",
    "                label = labels[i]\n",
    "                class_correct[label] += c[i].item()\n",
    "                class_total[label] += 1\n",
    "    \n",
    "    acc = []\n",
    "    for i in range(num_classes):\n",
    "        if class_total[i] == 0:\n",
    "            acc.append(0)\n",
    "            continue\n",
    "        acc.append(100 * class_correct[i] / class_total[i])\n",
    "    \n",
    "    for i in range(num_classes):\n",
    "        print('Accuracy of %5s : %2d %%' % (\n",
    "            train_dataset.classes[i], acc[i]))\n",
    "    \n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "013aed06-bc9a-49a6-af0d-af90e3912781",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-22T19:39:37.639056Z",
     "iopub.status.busy": "2022-06-22T19:39:37.638578Z",
     "iopub.status.idle": "2022-06-22T19:39:37.647316Z",
     "shell.execute_reply": "2022-06-22T19:39:37.646075Z",
     "shell.execute_reply.started": "2022-06-22T19:39:37.639002Z"
    },
    "id": "013aed06-bc9a-49a6-af0d-af90e3912781",
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATASET = 'CIFAR10' #['CIFAR10', 'CIFAR100']\n",
    "IMB_TYPE = 'exp' #['exp', 'step']\n",
    "IMB_FACTOR = 0.1 #[0.1, 0.01]\n",
    "SAVE_DIR = 'logs/10-exp-0.1-base' \n",
    "\n",
    "LR = 0.1\n",
    "BATCH_SIZE = 128\n",
    "MOMENTUM = 0.9\n",
    "WEIGHT_DECAY = 0\n",
    "EPOCHS = 90\n",
    "\n",
    "os.makedirs(osp.join(SAVE_DIR), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43814e65-218f-465c-8ee7-2da9abad5831",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-18T17:21:26.211488Z",
     "iopub.status.busy": "2022-06-18T17:21:26.211044Z",
     "iopub.status.idle": "2022-06-18T17:37:42.864745Z",
     "shell.execute_reply": "2022-06-18T17:37:42.863775Z",
     "shell.execute_reply.started": "2022-06-18T17:21:26.211440Z"
    },
    "id": "43814e65-218f-465c-8ee7-2da9abad5831",
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset, train_loader, test_loader, num_classes = get_loaders()\n",
    "model = ResNet18(num_classes)\n",
    "model = model.cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=150, gamma=0.1)\n",
    "\n",
    "# Let's build our model\n",
    "train(EPOCHS)\n",
    "print('Finished Training')\n",
    "test() \n",
    "print('----------------------------')\n",
    "print('get the top-1 accuracy for each of the classes.')\n",
    "avg_accu = testEeahClass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "hZe3QmFhaLZp",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-22T19:40:25.147016Z",
     "iopub.status.busy": "2022-06-22T19:40:25.146360Z",
     "iopub.status.idle": "2022-06-22T19:40:33.560301Z",
     "shell.execute_reply": "2022-06-22T19:40:33.558588Z",
     "shell.execute_reply.started": "2022-06-22T19:40:25.146953Z"
    },
    "id": "hZe3QmFhaLZp",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "cls num list:\n",
      "[5000, 3871, 2997, 2320, 1796, 1391, 1077, 834, 645, 500]\n",
      "Acc 71.72 \t AccHead 76.40 \t AccTail 67.04\n",
      "----------------------------\n",
      "get the top-1 accuracy for each of the classes.\n",
      "Acc 73.47 \t AccHead 74.76 \t AccTail 72.18\n",
      "Accuracy of airplane : 85 %\n",
      "Accuracy of automobile : 93 %\n",
      "Accuracy of  bird : 73 %\n",
      "Accuracy of   cat : 54 %\n",
      "Accuracy of  deer : 67 %\n",
      "Accuracy of   dog : 64 %\n",
      "Accuracy of  frog : 76 %\n",
      "Accuracy of horse : 74 %\n",
      "Accuracy of  ship : 76 %\n",
      "Accuracy of truck : 68 %\n"
     ]
    }
   ],
   "source": [
    "#여ㅣㅣㅣㅣㅣㅣㅣㅣㅣㅣㅣㅣㅣㅣㅣㅣㅣㅣㅣ기ㅣㅣㅣㅣㅣㅣㅣㅣ##########\n",
    "train_dataset, train_loader, test_loader, num_classes = get_loaders()\n",
    "model = ResNet18(num_classes)\n",
    "model = model.cuda()\n",
    "\n",
    "PATH = f'{SAVE_DIR}/ep{EPOCHS:03d}.pth'\n",
    "model.load_state_dict(torch.load(PATH)[\"model\"])\n",
    "\n",
    "test() \n",
    "print('----------------------------')\n",
    "print('get the top-1 accuracy for each of the classes.')\n",
    "\n",
    "tau_norm = torch.norm(model.classifier.weight, p=2.0, dim=1)\n",
    "tau_norm *= tau_norm**tau_list[0] \n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(num_classes):\n",
    "        model.classifier.weight[i] = model.classifier.weight[i].div_(tau_norm[i])\n",
    "        model.classifier.bias[i] = model.classifier.bias[i].div_(float(\"inf\")) # set 0\n",
    "\n",
    "test()\n",
    "avg_accu = testEeahClass()\n",
    "\n",
    "\n",
    "torch.save({\n",
    "    'model': model.state_dict(),\n",
    "    'optimizer': optimizer.state_dict(),\n",
    "    'epoch': EPOCHS},\n",
    "    osp.join(SAVE_DIR, 'tau-ep{:03d}.pth'.format(EPOCHS))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c05e4035-5856-4544-b28e-9fc266717cab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-22T19:55:08.991535Z",
     "iopub.status.busy": "2022-06-22T19:55:08.991068Z",
     "iopub.status.idle": "2022-06-22T19:55:09.000875Z",
     "shell.execute_reply": "2022-06-22T19:55:08.999303Z",
     "shell.execute_reply.started": "2022-06-22T19:55:08.991480Z"
    },
    "id": "c05e4035-5856-4544-b28e-9fc266717cab",
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATASET = 'CIFAR10' #['CIFAR10', 'CIFAR100']\n",
    "IMB_TYPE = 'exp' #['exp', 'step']\n",
    "IMB_FACTOR = 0.01 #[0.1, 0.01]\n",
    "SAVE_DIR = 'logs/10-exp-0.01-base' \n",
    "\n",
    "LR = 0.1\n",
    "BATCH_SIZE = 128\n",
    "MOMENTUM = 0.9\n",
    "WEIGHT_DECAY = 0\n",
    "EPOCHS = 90\n",
    "\n",
    "os.makedirs(osp.join(SAVE_DIR), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c7fc235b-8330-45e3-a007-87059dbf5bcd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-22T19:55:09.590785Z",
     "iopub.status.busy": "2022-06-22T19:55:09.590287Z",
     "iopub.status.idle": "2022-06-22T19:55:17.824361Z",
     "shell.execute_reply": "2022-06-22T19:55:17.822413Z",
     "shell.execute_reply.started": "2022-06-22T19:55:09.590732Z"
    },
    "id": "c7fc235b-8330-45e3-a007-87059dbf5bcd",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "cls num list:\n",
      "[5000, 2997, 1796, 1077, 645, 387, 232, 139, 83, 50]\n",
      "Acc 50.84 \t AccHead 72.88 \t AccTail 28.80\n",
      "----------------------------\n",
      "get the top-1 accuracy for each of the classes.\n",
      "Acc 55.03 \t AccHead 71.54 \t AccTail 38.52\n",
      "Accuracy of airplane : 89 %\n",
      "Accuracy of automobile : 92 %\n",
      "Accuracy of  bird : 62 %\n",
      "Accuracy of   cat : 55 %\n",
      "Accuracy of  deer : 57 %\n",
      "Accuracy of   dog : 49 %\n",
      "Accuracy of  frog : 51 %\n",
      "Accuracy of horse : 33 %\n",
      "Accuracy of  ship : 43 %\n",
      "Accuracy of truck : 15 %\n"
     ]
    }
   ],
   "source": [
    "#여ㅣㅣㅣㅣㅣㅣㅣㅣㅣㅣㅣㅣㅣㅣㅣㅣㅣㅣㅣ기ㅣㅣㅣㅣㅣㅣㅣㅣ##########\n",
    "train_dataset, train_loader, test_loader, num_classes = get_loaders()\n",
    "model = ResNet18(num_classes)\n",
    "model = model.cuda()\n",
    "\n",
    "PATH = f'{SAVE_DIR}/ep{EPOCHS:03d}.pth'\n",
    "model.load_state_dict(torch.load(PATH)[\"model\"])\n",
    "\n",
    "test() \n",
    "print('----------------------------')\n",
    "print('get the top-1 accuracy for each of the classes.')\n",
    "\n",
    "tau_norm = torch.norm(model.classifier.weight, p=2.0, dim=1)\n",
    "tau_norm *= tau_norm**tau_list[1] \n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(num_classes):\n",
    "        model.classifier.weight[i] = model.classifier.weight[i].div_(tau_norm[i])\n",
    "        model.classifier.bias[i] = model.classifier.bias[i].div_(float(\"inf\")) # set 0\n",
    "\n",
    "test()\n",
    "avg_accu = testEeahClass()\n",
    "\n",
    "\n",
    "torch.save({\n",
    "    'model': model.state_dict(),\n",
    "    'optimizer': optimizer.state_dict(),\n",
    "    'epoch': EPOCHS},\n",
    "    osp.join(SAVE_DIR, 'tau-ep{:03d}.pth'.format(EPOCHS))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1f5bde9e-c317-4077-9b0d-d22c5b306022",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-22T19:55:19.227407Z",
     "iopub.status.busy": "2022-06-22T19:55:19.226830Z",
     "iopub.status.idle": "2022-06-22T19:55:19.235730Z",
     "shell.execute_reply": "2022-06-22T19:55:19.234607Z",
     "shell.execute_reply.started": "2022-06-22T19:55:19.227347Z"
    },
    "id": "1f5bde9e-c317-4077-9b0d-d22c5b306022",
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATASET = 'CIFAR10' #['CIFAR10', 'CIFAR100']\n",
    "IMB_TYPE = 'step' #['exp', 'step']\n",
    "IMB_FACTOR = 0.1 #[0.1, 0.01]\n",
    "SAVE_DIR = 'logs/10-step-0.1-base' \n",
    "\n",
    "LR = 0.1\n",
    "BATCH_SIZE = 128\n",
    "MOMENTUM = 0.9\n",
    "WEIGHT_DECAY = 0\n",
    "EPOCHS = 90\n",
    "\n",
    "os.makedirs(osp.join(SAVE_DIR), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "72ffc0cf-110c-474c-ac4b-e04d58c84470",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-22T19:55:22.075410Z",
     "iopub.status.busy": "2022-06-22T19:55:22.074957Z",
     "iopub.status.idle": "2022-06-22T19:55:30.394613Z",
     "shell.execute_reply": "2022-06-22T19:55:30.393152Z",
     "shell.execute_reply.started": "2022-06-22T19:55:22.075358Z"
    },
    "id": "72ffc0cf-110c-474c-ac4b-e04d58c84470",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "cls num list:\n",
      "[5000, 5000, 5000, 5000, 5000, 500, 500, 500, 500, 500]\n",
      "Acc 68.02 \t AccHead 85.38 \t AccTail 50.66\n",
      "----------------------------\n",
      "get the top-1 accuracy for each of the classes.\n",
      "Acc 72.51 \t AccHead 82.66 \t AccTail 62.36\n",
      "Accuracy of airplane : 83 %\n",
      "Accuracy of automobile : 94 %\n",
      "Accuracy of  bird : 75 %\n",
      "Accuracy of   cat : 76 %\n",
      "Accuracy of  deer : 84 %\n",
      "Accuracy of   dog : 41 %\n",
      "Accuracy of  frog : 61 %\n",
      "Accuracy of horse : 64 %\n",
      "Accuracy of  ship : 75 %\n",
      "Accuracy of truck : 68 %\n"
     ]
    }
   ],
   "source": [
    "#여ㅣㅣㅣㅣㅣㅣㅣㅣㅣㅣㅣㅣㅣㅣㅣㅣㅣㅣㅣ기ㅣㅣㅣㅣㅣㅣㅣㅣ##########\n",
    "train_dataset, train_loader, test_loader, num_classes = get_loaders()\n",
    "model = ResNet18(num_classes)\n",
    "model = model.cuda()\n",
    "\n",
    "PATH = f'{SAVE_DIR}/ep{EPOCHS:03d}.pth'\n",
    "model.load_state_dict(torch.load(PATH)[\"model\"])\n",
    "\n",
    "test() \n",
    "print('----------------------------')\n",
    "print('get the top-1 accuracy for each of the classes.')\n",
    "\n",
    "tau_norm = torch.norm(model.classifier.weight, p=2.0, dim=1)\n",
    "tau_norm *= tau_norm**tau_list[2] \n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(num_classes):\n",
    "        model.classifier.weight[i] = model.classifier.weight[i].div_(tau_norm[i])\n",
    "        model.classifier.bias[i] = model.classifier.bias[i].div_(float(\"inf\")) # set 0\n",
    "\n",
    "test()\n",
    "avg_accu = testEeahClass()\n",
    "\n",
    "\n",
    "torch.save({\n",
    "    'model': model.state_dict(),\n",
    "    'optimizer': optimizer.state_dict(),\n",
    "    'epoch': EPOCHS},\n",
    "    osp.join(SAVE_DIR, 'tau-ep{:03d}.pth'.format(EPOCHS))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b3f6905b-2702-4bbd-9564-1d77b2d8fa3d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-22T19:55:32.593289Z",
     "iopub.status.busy": "2022-06-22T19:55:32.592827Z",
     "iopub.status.idle": "2022-06-22T19:55:32.602803Z",
     "shell.execute_reply": "2022-06-22T19:55:32.601537Z",
     "shell.execute_reply.started": "2022-06-22T19:55:32.593238Z"
    },
    "id": "b3f6905b-2702-4bbd-9564-1d77b2d8fa3d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATASET = 'CIFAR10' #['CIFAR10', 'CIFAR100']\n",
    "IMB_TYPE = 'step' #['exp', 'step']\n",
    "IMB_FACTOR = 0.01 #[0.1, 0.01]\n",
    "SAVE_DIR = 'logs/10-step-0.01-base' \n",
    "\n",
    "LR = 0.1\n",
    "BATCH_SIZE = 128\n",
    "MOMENTUM = 0.9\n",
    "WEIGHT_DECAY = 0\n",
    "EPOCHS = 200\n",
    "\n",
    "os.makedirs(osp.join(SAVE_DIR), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "NVpXD2r-cKMP",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-22T19:55:37.680762Z",
     "iopub.status.busy": "2022-06-22T19:55:37.680314Z",
     "iopub.status.idle": "2022-06-22T19:55:45.912739Z",
     "shell.execute_reply": "2022-06-22T19:55:45.911432Z",
     "shell.execute_reply.started": "2022-06-22T19:55:37.680711Z"
    },
    "id": "NVpXD2r-cKMP",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "cls num list:\n",
      "[5000, 5000, 5000, 5000, 5000, 50, 50, 50, 50, 50]\n",
      "Acc 49.11 \t AccHead 87.66 \t AccTail 10.56\n",
      "----------------------------\n",
      "get the top-1 accuracy for each of the classes.\n",
      "Acc 54.42 \t AccHead 87.02 \t AccTail 21.82\n",
      "Accuracy of airplane : 87 %\n",
      "Accuracy of automobile : 96 %\n",
      "Accuracy of  bird : 82 %\n",
      "Accuracy of   cat : 82 %\n",
      "Accuracy of  deer : 85 %\n",
      "Accuracy of   dog :  4 %\n",
      "Accuracy of  frog : 22 %\n",
      "Accuracy of horse : 29 %\n",
      "Accuracy of  ship : 37 %\n",
      "Accuracy of truck : 15 %\n"
     ]
    }
   ],
   "source": [
    "#여ㅣㅣㅣㅣㅣㅣㅣㅣㅣㅣㅣㅣㅣㅣㅣㅣㅣㅣㅣ기ㅣㅣㅣㅣㅣㅣㅣㅣ##########\n",
    "train_dataset, train_loader, test_loader, num_classes = get_loaders()\n",
    "model = ResNet18(num_classes)\n",
    "model = model.cuda()\n",
    "\n",
    "PATH = f'{SAVE_DIR}/ep{EPOCHS:03d}.pth'\n",
    "model.load_state_dict(torch.load(PATH)[\"model\"])\n",
    "\n",
    "test() \n",
    "print('----------------------------')\n",
    "print('get the top-1 accuracy for each of the classes.')\n",
    "\n",
    "tau_norm = torch.norm(model.classifier.weight, p=2.0, dim=1)\n",
    "tau_norm *= tau_norm**tau_list[3] \n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(num_classes):\n",
    "        model.classifier.weight[i] = model.classifier.weight[i].div_(tau_norm[i])\n",
    "        model.classifier.bias[i] = model.classifier.bias[i].div_(float(\"inf\")) # set 0\n",
    "\n",
    "test()\n",
    "avg_accu = testEeahClass()\n",
    "\n",
    "\n",
    "torch.save({\n",
    "    'model': model.state_dict(),\n",
    "    'optimizer': optimizer.state_dict(),\n",
    "    'epoch': EPOCHS},\n",
    "    osp.join(SAVE_DIR, 'tau-ep{:03d}.pth'.format(EPOCHS))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f8cc486b-6665-4e6b-949a-21dfa5f40fd1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-22T19:55:45.935961Z",
     "iopub.status.busy": "2022-06-22T19:55:45.935809Z",
     "iopub.status.idle": "2022-06-22T19:55:45.940801Z",
     "shell.execute_reply": "2022-06-22T19:55:45.940172Z",
     "shell.execute_reply.started": "2022-06-22T19:55:45.935941Z"
    },
    "id": "f8cc486b-6665-4e6b-949a-21dfa5f40fd1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATASET = 'CIFAR100' #['CIFAR10', 'CIFAR100']\n",
    "IMB_TYPE = 'exp' #['exp', 'step']\n",
    "IMB_FACTOR = 0.1 #[0.1, 0.01]\n",
    "SAVE_DIR = 'logs/100-exp-0.1-base' \n",
    "\n",
    "LR = 0.1\n",
    "BATCH_SIZE = 128\n",
    "MOMENTUM = 0.9\n",
    "WEIGHT_DECAY = 0\n",
    "EPOCHS = 200\n",
    "\n",
    "os.makedirs(osp.join(SAVE_DIR), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e21bc030-9e28-4f74-8aa3-0d0575b53bd0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-22T19:55:48.657940Z",
     "iopub.status.busy": "2022-06-22T19:55:48.657111Z",
     "iopub.status.idle": "2022-06-22T19:55:57.302553Z",
     "shell.execute_reply": "2022-06-22T19:55:57.301100Z",
     "shell.execute_reply.started": "2022-06-22T19:55:48.657859Z"
    },
    "id": "e21bc030-9e28-4f74-8aa3-0d0575b53bd0",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "cls num list:\n",
      "[500, 488, 477, 466, 455, 445, 434, 424, 415, 405, 396, 387, 378, 369, 361, 352, 344, 336, 328, 321, 314, 306, 299, 292, 286, 279, 273, 266, 260, 254, 248, 243, 237, 232, 226, 221, 216, 211, 206, 201, 197, 192, 188, 183, 179, 175, 171, 167, 163, 159, 156, 152, 149, 145, 142, 139, 135, 132, 129, 126, 123, 121, 118, 115, 112, 110, 107, 105, 102, 100, 98, 95, 93, 91, 89, 87, 85, 83, 81, 79, 77, 75, 74, 72, 70, 69, 67, 66, 64, 63, 61, 60, 58, 57, 56, 54, 53, 52, 51, 50]\n",
      "Acc 38.57 \t AccHead 48.10 \t AccTail 29.04\n",
      "----------------------------\n",
      "get the top-1 accuracy for each of the classes.\n",
      "Acc 40.04 \t AccHead 43.90 \t AccTail 36.18\n",
      "Accuracy of apple : 81 %\n",
      "Accuracy of aquarium_fish : 68 %\n",
      "Accuracy of  baby : 48 %\n",
      "Accuracy of  bear : 27 %\n",
      "Accuracy of beaver : 33 %\n",
      "Accuracy of   bed : 51 %\n",
      "Accuracy of   bee : 50 %\n",
      "Accuracy of beetle : 56 %\n",
      "Accuracy of bicycle : 66 %\n",
      "Accuracy of bottle : 59 %\n",
      "Accuracy of  bowl : 28 %\n",
      "Accuracy of   boy : 27 %\n",
      "Accuracy of bridge : 51 %\n",
      "Accuracy of   bus : 41 %\n",
      "Accuracy of butterfly : 23 %\n",
      "Accuracy of camel : 28 %\n",
      "Accuracy of   can : 41 %\n",
      "Accuracy of castle : 72 %\n",
      "Accuracy of caterpillar : 39 %\n",
      "Accuracy of cattle : 38 %\n",
      "Accuracy of chair : 74 %\n",
      "Accuracy of chimpanzee : 67 %\n",
      "Accuracy of clock : 34 %\n",
      "Accuracy of cloud : 68 %\n",
      "Accuracy of cockroach : 73 %\n",
      "Accuracy of couch : 29 %\n",
      "Accuracy of  crab : 30 %\n",
      "Accuracy of crocodile : 31 %\n",
      "Accuracy of   cup : 56 %\n",
      "Accuracy of dinosaur : 34 %\n",
      "Accuracy of dolphin : 52 %\n",
      "Accuracy of elephant : 45 %\n",
      "Accuracy of flatfish : 40 %\n",
      "Accuracy of forest : 36 %\n",
      "Accuracy of   fox : 30 %\n",
      "Accuracy of  girl : 23 %\n",
      "Accuracy of hamster : 44 %\n",
      "Accuracy of house : 27 %\n",
      "Accuracy of kangaroo : 29 %\n",
      "Accuracy of keyboard : 52 %\n",
      "Accuracy of  lamp : 29 %\n",
      "Accuracy of lawn_mower : 65 %\n",
      "Accuracy of leopard : 40 %\n",
      "Accuracy of  lion : 36 %\n",
      "Accuracy of lizard : 17 %\n",
      "Accuracy of lobster : 20 %\n",
      "Accuracy of   man : 16 %\n",
      "Accuracy of maple_tree : 49 %\n",
      "Accuracy of motorcycle : 70 %\n",
      "Accuracy of mountain : 52 %\n",
      "Accuracy of mouse : 18 %\n",
      "Accuracy of mushroom : 43 %\n",
      "Accuracy of oak_tree : 72 %\n",
      "Accuracy of orange : 80 %\n",
      "Accuracy of orchid : 52 %\n",
      "Accuracy of otter :  3 %\n",
      "Accuracy of palm_tree : 67 %\n",
      "Accuracy of  pear : 31 %\n",
      "Accuracy of pickup_truck : 53 %\n",
      "Accuracy of pine_tree : 25 %\n",
      "Accuracy of plain : 74 %\n",
      "Accuracy of plate : 39 %\n",
      "Accuracy of poppy : 54 %\n",
      "Accuracy of porcupine : 30 %\n",
      "Accuracy of possum :  8 %\n",
      "Accuracy of rabbit : 15 %\n",
      "Accuracy of raccoon : 20 %\n",
      "Accuracy of   ray : 29 %\n",
      "Accuracy of  road : 80 %\n",
      "Accuracy of rocket : 60 %\n",
      "Accuracy of  rose : 40 %\n",
      "Accuracy of   sea : 58 %\n",
      "Accuracy of  seal :  8 %\n",
      "Accuracy of shark : 33 %\n",
      "Accuracy of shrew : 14 %\n",
      "Accuracy of skunk : 67 %\n",
      "Accuracy of skyscraper : 65 %\n",
      "Accuracy of snail : 15 %\n",
      "Accuracy of snake : 18 %\n",
      "Accuracy of spider : 32 %\n",
      "Accuracy of squirrel : 10 %\n",
      "Accuracy of streetcar : 22 %\n",
      "Accuracy of sunflower : 69 %\n",
      "Accuracy of sweet_pepper : 26 %\n",
      "Accuracy of table : 24 %\n",
      "Accuracy of  tank : 48 %\n",
      "Accuracy of telephone : 29 %\n",
      "Accuracy of television : 43 %\n",
      "Accuracy of tiger : 24 %\n",
      "Accuracy of tractor : 37 %\n",
      "Accuracy of train : 17 %\n",
      "Accuracy of trout : 39 %\n",
      "Accuracy of tulip : 12 %\n",
      "Accuracy of turtle :  8 %\n",
      "Accuracy of wardrobe : 75 %\n",
      "Accuracy of whale : 40 %\n",
      "Accuracy of willow_tree : 26 %\n",
      "Accuracy of  wolf : 22 %\n",
      "Accuracy of woman : 12 %\n",
      "Accuracy of  worm : 23 %\n"
     ]
    }
   ],
   "source": [
    "#여ㅣㅣㅣㅣㅣㅣㅣㅣㅣㅣㅣㅣㅣㅣㅣㅣㅣㅣㅣ기ㅣㅣㅣㅣㅣㅣㅣㅣ##########\n",
    "train_dataset, train_loader, test_loader, num_classes = get_loaders()\n",
    "model = ResNet18(num_classes)\n",
    "model = model.cuda()\n",
    "\n",
    "PATH = f'{SAVE_DIR}/ep{EPOCHS:03d}.pth'\n",
    "model.load_state_dict(torch.load(PATH)[\"model\"])\n",
    "\n",
    "test() \n",
    "print('----------------------------')\n",
    "print('get the top-1 accuracy for each of the classes.')\n",
    "\n",
    "tau_norm = torch.norm(model.classifier.weight, p=2.0, dim=1)\n",
    "tau_norm *= tau_norm**tau_list[4] \n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(num_classes):\n",
    "        model.classifier.weight[i] = model.classifier.weight[i].div_(tau_norm[i])\n",
    "        model.classifier.bias[i] = model.classifier.bias[i].div_(float(\"inf\")) # set 0\n",
    "\n",
    "test()\n",
    "avg_accu = testEeahClass()\n",
    "\n",
    "\n",
    "torch.save({\n",
    "    'model': model.state_dict(),\n",
    "    'optimizer': optimizer.state_dict(),\n",
    "    'epoch': EPOCHS},\n",
    "    osp.join(SAVE_DIR, 'tau-ep{:03d}.pth'.format(EPOCHS))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "94739261-35ca-4788-af8c-e4fbbef0c4d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-22T19:56:02.777167Z",
     "iopub.status.busy": "2022-06-22T19:56:02.776463Z",
     "iopub.status.idle": "2022-06-22T19:56:02.785509Z",
     "shell.execute_reply": "2022-06-22T19:56:02.784298Z",
     "shell.execute_reply.started": "2022-06-22T19:56:02.777110Z"
    },
    "id": "94739261-35ca-4788-af8c-e4fbbef0c4d1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATASET = 'CIFAR100' #['CIFAR10', 'CIFAR100']\n",
    "IMB_TYPE = 'exp' #['exp', 'step']\n",
    "IMB_FACTOR = 0.01 #[0.1, 0.01]\n",
    "SAVE_DIR = 'logs/100-exp-0.01-base' \n",
    "\n",
    "LR = 0.1\n",
    "BATCH_SIZE = 128\n",
    "MOMENTUM = 0.9\n",
    "WEIGHT_DECAY = 0\n",
    "EPOCHS = 200\n",
    "\n",
    "os.makedirs(osp.join(SAVE_DIR), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d12e3896-e9ea-46e6-9829-21ab26a8cba1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-22T19:56:13.812844Z",
     "iopub.status.busy": "2022-06-22T19:56:13.812402Z",
     "iopub.status.idle": "2022-06-22T19:56:23.798165Z",
     "shell.execute_reply": "2022-06-22T19:56:23.796924Z",
     "shell.execute_reply.started": "2022-06-22T19:56:13.812794Z"
    },
    "id": "d12e3896-e9ea-46e6-9829-21ab26a8cba1",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "cls num list:\n",
      "[500, 477, 455, 434, 415, 396, 378, 361, 344, 328, 314, 299, 286, 273, 260, 248, 237, 226, 216, 206, 197, 188, 179, 171, 163, 156, 149, 142, 135, 129, 123, 118, 112, 107, 102, 98, 93, 89, 85, 81, 77, 74, 70, 67, 64, 61, 58, 56, 53, 51, 48, 46, 44, 42, 40, 38, 36, 35, 33, 32, 30, 29, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 15, 15, 14, 13, 13, 12, 12, 11, 11, 10, 10, 9, 9, 8, 8, 7, 7, 7, 6, 6, 6, 6, 5, 5, 5, 5]\n",
      "Acc 26.90 \t AccHead 41.80 \t AccTail 12.00\n",
      "----------------------------\n",
      "get the top-1 accuracy for each of the classes.\n",
      "Acc 27.34 \t AccHead 31.62 \t AccTail 23.06\n",
      "Accuracy of apple : 70 %\n",
      "Accuracy of aquarium_fish : 55 %\n",
      "Accuracy of  baby : 28 %\n",
      "Accuracy of  bear : 20 %\n",
      "Accuracy of beaver : 28 %\n",
      "Accuracy of   bed : 33 %\n",
      "Accuracy of   bee : 38 %\n",
      "Accuracy of beetle : 34 %\n",
      "Accuracy of bicycle : 40 %\n",
      "Accuracy of bottle : 52 %\n",
      "Accuracy of  bowl : 11 %\n",
      "Accuracy of   boy : 21 %\n",
      "Accuracy of bridge : 30 %\n",
      "Accuracy of   bus : 35 %\n",
      "Accuracy of butterfly : 21 %\n",
      "Accuracy of camel : 26 %\n",
      "Accuracy of   can : 36 %\n",
      "Accuracy of castle : 50 %\n",
      "Accuracy of caterpillar : 23 %\n",
      "Accuracy of cattle : 19 %\n",
      "Accuracy of chair : 73 %\n",
      "Accuracy of chimpanzee : 55 %\n",
      "Accuracy of clock : 22 %\n",
      "Accuracy of cloud : 55 %\n",
      "Accuracy of cockroach : 76 %\n",
      "Accuracy of couch : 18 %\n",
      "Accuracy of  crab : 18 %\n",
      "Accuracy of crocodile : 16 %\n",
      "Accuracy of   cup : 45 %\n",
      "Accuracy of dinosaur : 27 %\n",
      "Accuracy of dolphin : 52 %\n",
      "Accuracy of elephant : 30 %\n",
      "Accuracy of flatfish : 16 %\n",
      "Accuracy of forest : 30 %\n",
      "Accuracy of   fox : 16 %\n",
      "Accuracy of  girl : 20 %\n",
      "Accuracy of hamster : 26 %\n",
      "Accuracy of house : 12 %\n",
      "Accuracy of kangaroo : 13 %\n",
      "Accuracy of keyboard : 29 %\n",
      "Accuracy of  lamp : 25 %\n",
      "Accuracy of lawn_mower : 56 %\n",
      "Accuracy of leopard : 23 %\n",
      "Accuracy of  lion : 26 %\n",
      "Accuracy of lizard :  3 %\n",
      "Accuracy of lobster :  9 %\n",
      "Accuracy of   man :  8 %\n",
      "Accuracy of maple_tree : 36 %\n",
      "Accuracy of motorcycle : 52 %\n",
      "Accuracy of mountain : 24 %\n",
      "Accuracy of mouse :  3 %\n",
      "Accuracy of mushroom : 24 %\n",
      "Accuracy of oak_tree : 33 %\n",
      "Accuracy of orange : 65 %\n",
      "Accuracy of orchid : 26 %\n",
      "Accuracy of otter :  0 %\n",
      "Accuracy of palm_tree : 33 %\n",
      "Accuracy of  pear : 18 %\n",
      "Accuracy of pickup_truck : 25 %\n",
      "Accuracy of pine_tree : 19 %\n",
      "Accuracy of plain : 62 %\n",
      "Accuracy of plate : 43 %\n",
      "Accuracy of poppy : 39 %\n",
      "Accuracy of porcupine : 22 %\n",
      "Accuracy of possum :  1 %\n",
      "Accuracy of rabbit :  7 %\n",
      "Accuracy of raccoon : 13 %\n",
      "Accuracy of   ray : 12 %\n",
      "Accuracy of  road : 59 %\n",
      "Accuracy of rocket : 36 %\n",
      "Accuracy of  rose : 34 %\n",
      "Accuracy of   sea : 36 %\n",
      "Accuracy of  seal :  8 %\n",
      "Accuracy of shark : 26 %\n",
      "Accuracy of shrew : 17 %\n",
      "Accuracy of skunk : 35 %\n",
      "Accuracy of skyscraper : 57 %\n",
      "Accuracy of snail :  7 %\n",
      "Accuracy of snake :  3 %\n",
      "Accuracy of spider : 14 %\n",
      "Accuracy of squirrel :  7 %\n",
      "Accuracy of streetcar : 11 %\n",
      "Accuracy of sunflower : 48 %\n",
      "Accuracy of sweet_pepper : 22 %\n",
      "Accuracy of table :  4 %\n",
      "Accuracy of  tank : 20 %\n",
      "Accuracy of telephone :  7 %\n",
      "Accuracy of television : 33 %\n",
      "Accuracy of tiger : 26 %\n",
      "Accuracy of tractor : 14 %\n",
      "Accuracy of train : 16 %\n",
      "Accuracy of trout : 16 %\n",
      "Accuracy of tulip : 10 %\n",
      "Accuracy of turtle :  4 %\n",
      "Accuracy of wardrobe : 52 %\n",
      "Accuracy of whale : 27 %\n",
      "Accuracy of willow_tree : 33 %\n",
      "Accuracy of  wolf :  7 %\n",
      "Accuracy of woman :  8 %\n",
      "Accuracy of  worm : 11 %\n"
     ]
    }
   ],
   "source": [
    "#여ㅣㅣㅣㅣㅣㅣㅣㅣㅣㅣㅣㅣㅣㅣㅣㅣㅣㅣㅣ기ㅣㅣㅣㅣㅣㅣㅣㅣ##########\n",
    "train_dataset, train_loader, test_loader, num_classes = get_loaders()\n",
    "model = ResNet18(num_classes)\n",
    "model = model.cuda()\n",
    "\n",
    "PATH = f'{SAVE_DIR}/ep{EPOCHS:03d}.pth'\n",
    "model.load_state_dict(torch.load(PATH)[\"model\"])\n",
    "\n",
    "test() \n",
    "print('----------------------------')\n",
    "print('get the top-1 accuracy for each of the classes.')\n",
    "\n",
    "tau_norm = torch.norm(model.classifier.weight, p=2.0, dim=1)\n",
    "tau_norm *= tau_norm**tau_list[5] \n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(num_classes):\n",
    "        model.classifier.weight[i] = model.classifier.weight[i].div_(tau_norm[i])\n",
    "        model.classifier.bias[i] = model.classifier.bias[i].div_(float(\"inf\")) # set 0\n",
    "\n",
    "test()\n",
    "avg_accu = testEeahClass()\n",
    "\n",
    "\n",
    "torch.save({\n",
    "    'model': model.state_dict(),\n",
    "    'optimizer': optimizer.state_dict(),\n",
    "    'epoch': EPOCHS},\n",
    "    osp.join(SAVE_DIR, 'tau-ep{:03d}.pth'.format(EPOCHS))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "5f9eaaa2-a3a7-41fe-8a62-d9fb374bcfb1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-22T19:56:26.263147Z",
     "iopub.status.busy": "2022-06-22T19:56:26.262707Z",
     "iopub.status.idle": "2022-06-22T19:56:26.271284Z",
     "shell.execute_reply": "2022-06-22T19:56:26.270117Z",
     "shell.execute_reply.started": "2022-06-22T19:56:26.263098Z"
    },
    "id": "5f9eaaa2-a3a7-41fe-8a62-d9fb374bcfb1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATASET = 'CIFAR100' #['CIFAR10', 'CIFAR100']\n",
    "IMB_TYPE = 'step' #['exp', 'step']\n",
    "IMB_FACTOR = 0.1 #[0.1, 0.01]\n",
    "SAVE_DIR = 'logs/100-step-0.1-base' \n",
    "\n",
    "LR = 0.1\n",
    "BATCH_SIZE = 128\n",
    "MOMENTUM = 0.9\n",
    "WEIGHT_DECAY = 0\n",
    "EPOCHS = 200\n",
    "\n",
    "os.makedirs(osp.join(SAVE_DIR), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "334ce775-289d-46dc-94ee-1f9590c9fc52",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-22T19:56:30.598501Z",
     "iopub.status.busy": "2022-06-22T19:56:30.593520Z",
     "iopub.status.idle": "2022-06-22T19:56:40.755631Z",
     "shell.execute_reply": "2022-06-22T19:56:40.754137Z",
     "shell.execute_reply.started": "2022-06-22T19:56:30.598418Z"
    },
    "id": "334ce775-289d-46dc-94ee-1f9590c9fc52",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "cls num list:\n",
      "[500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50]\n",
      "Acc 37.98 \t AccHead 56.44 \t AccTail 19.52\n",
      "----------------------------\n",
      "get the top-1 accuracy for each of the classes.\n",
      "Acc 40.92 \t AccHead 48.54 \t AccTail 33.30\n",
      "Accuracy of apple : 77 %\n",
      "Accuracy of aquarium_fish : 67 %\n",
      "Accuracy of  baby : 38 %\n",
      "Accuracy of  bear : 21 %\n",
      "Accuracy of beaver : 28 %\n",
      "Accuracy of   bed : 49 %\n",
      "Accuracy of   bee : 43 %\n",
      "Accuracy of beetle : 50 %\n",
      "Accuracy of bicycle : 65 %\n",
      "Accuracy of bottle : 57 %\n",
      "Accuracy of  bowl : 18 %\n",
      "Accuracy of   boy : 21 %\n",
      "Accuracy of bridge : 45 %\n",
      "Accuracy of   bus : 48 %\n",
      "Accuracy of butterfly : 33 %\n",
      "Accuracy of camel : 43 %\n",
      "Accuracy of   can : 51 %\n",
      "Accuracy of castle : 72 %\n",
      "Accuracy of caterpillar : 38 %\n",
      "Accuracy of cattle : 38 %\n",
      "Accuracy of chair : 80 %\n",
      "Accuracy of chimpanzee : 79 %\n",
      "Accuracy of clock : 34 %\n",
      "Accuracy of cloud : 67 %\n",
      "Accuracy of cockroach : 82 %\n",
      "Accuracy of couch : 32 %\n",
      "Accuracy of  crab : 39 %\n",
      "Accuracy of crocodile : 35 %\n",
      "Accuracy of   cup : 61 %\n",
      "Accuracy of dinosaur : 45 %\n",
      "Accuracy of dolphin : 60 %\n",
      "Accuracy of elephant : 51 %\n",
      "Accuracy of flatfish : 42 %\n",
      "Accuracy of forest : 34 %\n",
      "Accuracy of   fox : 53 %\n",
      "Accuracy of  girl : 25 %\n",
      "Accuracy of hamster : 47 %\n",
      "Accuracy of house : 40 %\n",
      "Accuracy of kangaroo : 38 %\n",
      "Accuracy of keyboard : 67 %\n",
      "Accuracy of  lamp : 48 %\n",
      "Accuracy of lawn_mower : 74 %\n",
      "Accuracy of leopard : 49 %\n",
      "Accuracy of  lion : 51 %\n",
      "Accuracy of lizard : 28 %\n",
      "Accuracy of lobster : 23 %\n",
      "Accuracy of   man : 27 %\n",
      "Accuracy of maple_tree : 66 %\n",
      "Accuracy of motorcycle : 81 %\n",
      "Accuracy of mountain : 67 %\n",
      "Accuracy of mouse :  5 %\n",
      "Accuracy of mushroom : 19 %\n",
      "Accuracy of oak_tree : 59 %\n",
      "Accuracy of orange : 67 %\n",
      "Accuracy of orchid : 48 %\n",
      "Accuracy of otter :  7 %\n",
      "Accuracy of palm_tree : 63 %\n",
      "Accuracy of  pear : 35 %\n",
      "Accuracy of pickup_truck : 53 %\n",
      "Accuracy of pine_tree : 24 %\n",
      "Accuracy of plain : 76 %\n",
      "Accuracy of plate : 44 %\n",
      "Accuracy of poppy : 43 %\n",
      "Accuracy of porcupine : 29 %\n",
      "Accuracy of possum :  8 %\n",
      "Accuracy of rabbit :  8 %\n",
      "Accuracy of raccoon : 19 %\n",
      "Accuracy of   ray : 28 %\n",
      "Accuracy of  road : 70 %\n",
      "Accuracy of rocket : 51 %\n",
      "Accuracy of  rose : 45 %\n",
      "Accuracy of   sea : 57 %\n",
      "Accuracy of  seal :  9 %\n",
      "Accuracy of shark : 27 %\n",
      "Accuracy of shrew : 21 %\n",
      "Accuracy of skunk : 61 %\n",
      "Accuracy of skyscraper : 65 %\n",
      "Accuracy of snail :  9 %\n",
      "Accuracy of snake :  8 %\n",
      "Accuracy of spider : 27 %\n",
      "Accuracy of squirrel : 11 %\n",
      "Accuracy of streetcar : 17 %\n",
      "Accuracy of sunflower : 67 %\n",
      "Accuracy of sweet_pepper : 26 %\n",
      "Accuracy of table : 22 %\n",
      "Accuracy of  tank : 40 %\n",
      "Accuracy of telephone : 31 %\n",
      "Accuracy of television : 39 %\n",
      "Accuracy of tiger : 25 %\n",
      "Accuracy of tractor : 42 %\n",
      "Accuracy of train : 12 %\n",
      "Accuracy of trout : 36 %\n",
      "Accuracy of tulip : 14 %\n",
      "Accuracy of turtle : 11 %\n",
      "Accuracy of wardrobe : 79 %\n",
      "Accuracy of whale : 26 %\n",
      "Accuracy of willow_tree : 27 %\n",
      "Accuracy of  wolf : 27 %\n",
      "Accuracy of woman :  7 %\n",
      "Accuracy of  worm : 21 %\n"
     ]
    }
   ],
   "source": [
    "#여ㅣㅣㅣㅣㅣㅣㅣㅣㅣㅣㅣㅣㅣㅣㅣㅣㅣㅣㅣ기ㅣㅣㅣㅣㅣㅣㅣㅣ##########\n",
    "train_dataset, train_loader, test_loader, num_classes = get_loaders()\n",
    "model = ResNet18(num_classes)\n",
    "model = model.cuda()\n",
    "\n",
    "PATH = f'{SAVE_DIR}/ep{EPOCHS:03d}.pth'\n",
    "model.load_state_dict(torch.load(PATH)[\"model\"])\n",
    "\n",
    "test() \n",
    "print('----------------------------')\n",
    "print('get the top-1 accuracy for each of the classes.')\n",
    "\n",
    "tau_norm = torch.norm(model.classifier.weight, p=2.0, dim=1)\n",
    "tau_norm *= tau_norm**tau_list[6] \n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(num_classes):\n",
    "        model.classifier.weight[i] = model.classifier.weight[i].div_(tau_norm[i])\n",
    "        model.classifier.bias[i] = model.classifier.bias[i].div_(float(\"inf\")) # set 0\n",
    "\n",
    "test()\n",
    "avg_accu = testEeahClass()\n",
    "\n",
    "\n",
    "torch.save({\n",
    "    'model': model.state_dict(),\n",
    "    'optimizer': optimizer.state_dict(),\n",
    "    'epoch': EPOCHS},\n",
    "    osp.join(SAVE_DIR, 'tau-ep{:03d}.pth'.format(EPOCHS))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "12390a81-605a-43d4-92b3-e32dab6fcbcd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-22T19:56:40.793395Z",
     "iopub.status.busy": "2022-06-22T19:56:40.793060Z",
     "iopub.status.idle": "2022-06-22T19:56:40.798566Z",
     "shell.execute_reply": "2022-06-22T19:56:40.797899Z",
     "shell.execute_reply.started": "2022-06-22T19:56:40.793363Z"
    },
    "id": "12390a81-605a-43d4-92b3-e32dab6fcbcd",
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATASET = 'CIFAR100' #['CIFAR10', 'CIFAR100']\n",
    "IMB_TYPE = 'step' #['exp', 'step']\n",
    "IMB_FACTOR = 0.01 #[0.1, 0.01]\n",
    "SAVE_DIR = 'logs/100-step-0.01-base' \n",
    "\n",
    "LR = 0.1\n",
    "BATCH_SIZE = 128\n",
    "MOMENTUM = 0.9\n",
    "WEIGHT_DECAY = 0\n",
    "EPOCHS = 200\n",
    "\n",
    "os.makedirs(osp.join(SAVE_DIR), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e0263dfa-1276-4779-9e6f-de42e3dee977",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-22T19:57:03.802306Z",
     "iopub.status.busy": "2022-06-22T19:57:03.801719Z",
     "iopub.status.idle": "2022-06-22T19:57:12.269632Z",
     "shell.execute_reply": "2022-06-22T19:57:12.267975Z",
     "shell.execute_reply.started": "2022-06-22T19:57:03.802252Z"
    },
    "id": "e0263dfa-1276-4779-9e6f-de42e3dee977",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "cls num list:\n",
      "[500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]\n",
      "Acc 29.49 \t AccHead 57.10 \t AccTail 1.88\n",
      "----------------------------\n",
      "get the top-1 accuracy for each of the classes.\n",
      "Acc 22.18 \t AccHead 19.58 \t AccTail 24.78\n",
      "Accuracy of apple : 26 %\n",
      "Accuracy of aquarium_fish : 19 %\n",
      "Accuracy of  baby : 12 %\n",
      "Accuracy of  bear :  2 %\n",
      "Accuracy of beaver :  4 %\n",
      "Accuracy of   bed : 25 %\n",
      "Accuracy of   bee : 10 %\n",
      "Accuracy of beetle :  4 %\n",
      "Accuracy of bicycle : 38 %\n",
      "Accuracy of bottle : 26 %\n",
      "Accuracy of  bowl :  5 %\n",
      "Accuracy of   boy :  6 %\n",
      "Accuracy of bridge : 12 %\n",
      "Accuracy of   bus :  7 %\n",
      "Accuracy of butterfly :  6 %\n",
      "Accuracy of camel : 14 %\n",
      "Accuracy of   can : 22 %\n",
      "Accuracy of castle : 39 %\n",
      "Accuracy of caterpillar :  6 %\n",
      "Accuracy of cattle : 16 %\n",
      "Accuracy of chair : 63 %\n",
      "Accuracy of chimpanzee : 35 %\n",
      "Accuracy of clock :  3 %\n",
      "Accuracy of cloud : 27 %\n",
      "Accuracy of cockroach : 38 %\n",
      "Accuracy of couch :  8 %\n",
      "Accuracy of  crab : 16 %\n",
      "Accuracy of crocodile :  7 %\n",
      "Accuracy of   cup : 46 %\n",
      "Accuracy of dinosaur : 26 %\n",
      "Accuracy of dolphin : 19 %\n",
      "Accuracy of elephant : 23 %\n",
      "Accuracy of flatfish : 20 %\n",
      "Accuracy of forest : 17 %\n",
      "Accuracy of   fox : 13 %\n",
      "Accuracy of  girl : 11 %\n",
      "Accuracy of hamster : 31 %\n",
      "Accuracy of house : 10 %\n",
      "Accuracy of kangaroo :  6 %\n",
      "Accuracy of keyboard : 39 %\n",
      "Accuracy of  lamp : 21 %\n",
      "Accuracy of lawn_mower : 50 %\n",
      "Accuracy of leopard : 12 %\n",
      "Accuracy of  lion : 24 %\n",
      "Accuracy of lizard :  5 %\n",
      "Accuracy of lobster :  7 %\n",
      "Accuracy of   man :  8 %\n",
      "Accuracy of maple_tree : 24 %\n",
      "Accuracy of motorcycle : 48 %\n",
      "Accuracy of mountain : 23 %\n",
      "Accuracy of mouse : 12 %\n",
      "Accuracy of mushroom : 19 %\n",
      "Accuracy of oak_tree : 40 %\n",
      "Accuracy of orange : 39 %\n",
      "Accuracy of orchid : 16 %\n",
      "Accuracy of otter :  8 %\n",
      "Accuracy of palm_tree : 47 %\n",
      "Accuracy of  pear : 31 %\n",
      "Accuracy of pickup_truck : 37 %\n",
      "Accuracy of pine_tree : 18 %\n",
      "Accuracy of plain : 59 %\n",
      "Accuracy of plate : 38 %\n",
      "Accuracy of poppy : 28 %\n",
      "Accuracy of porcupine : 21 %\n",
      "Accuracy of possum :  4 %\n",
      "Accuracy of rabbit :  5 %\n",
      "Accuracy of raccoon : 21 %\n",
      "Accuracy of   ray : 11 %\n",
      "Accuracy of  road : 43 %\n",
      "Accuracy of rocket : 32 %\n",
      "Accuracy of  rose : 16 %\n",
      "Accuracy of   sea : 37 %\n",
      "Accuracy of  seal : 10 %\n",
      "Accuracy of shark : 41 %\n",
      "Accuracy of shrew : 11 %\n",
      "Accuracy of skunk : 43 %\n",
      "Accuracy of skyscraper : 53 %\n",
      "Accuracy of snail : 15 %\n",
      "Accuracy of snake :  9 %\n",
      "Accuracy of spider : 21 %\n",
      "Accuracy of squirrel : 15 %\n",
      "Accuracy of streetcar : 25 %\n",
      "Accuracy of sunflower : 59 %\n",
      "Accuracy of sweet_pepper : 14 %\n",
      "Accuracy of table : 23 %\n",
      "Accuracy of  tank : 21 %\n",
      "Accuracy of telephone :  9 %\n",
      "Accuracy of television : 44 %\n",
      "Accuracy of tiger : 18 %\n",
      "Accuracy of tractor : 11 %\n",
      "Accuracy of train : 15 %\n",
      "Accuracy of trout : 13 %\n",
      "Accuracy of tulip : 12 %\n",
      "Accuracy of turtle :  8 %\n",
      "Accuracy of wardrobe : 53 %\n",
      "Accuracy of whale : 28 %\n",
      "Accuracy of willow_tree : 23 %\n",
      "Accuracy of  wolf : 17 %\n",
      "Accuracy of woman : 28 %\n",
      "Accuracy of  worm : 18 %\n"
     ]
    }
   ],
   "source": [
    "#여ㅣㅣㅣㅣㅣㅣㅣㅣㅣㅣㅣㅣㅣㅣㅣㅣㅣㅣㅣ기ㅣㅣㅣㅣㅣㅣㅣㅣ##########\n",
    "train_dataset, train_loader, test_loader, num_classes = get_loaders()\n",
    "model = ResNet18(num_classes)\n",
    "model = model.cuda()\n",
    "\n",
    "PATH = f'{SAVE_DIR}/ep{EPOCHS:03d}.pth'\n",
    "model.load_state_dict(torch.load(PATH)[\"model\"])\n",
    "\n",
    "test() \n",
    "print('----------------------------')\n",
    "print('get the top-1 accuracy for each of the classes.')\n",
    "\n",
    "tau_norm = torch.norm(model.classifier.weight, p=2.0, dim=1)\n",
    "tau_norm *= tau_norm**tau_list[7] \n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(num_classes):\n",
    "        model.classifier.weight[i] = model.classifier.weight[i].div_(tau_norm[i])\n",
    "        model.classifier.bias[i] = model.classifier.bias[i].div_(float(\"inf\")) # set 0\n",
    "\n",
    "test()\n",
    "avg_accu = testEeahClass()\n",
    "\n",
    "\n",
    "torch.save({\n",
    "    'model': model.state_dict(),\n",
    "    'optimizer': optimizer.state_dict(),\n",
    "    'epoch': EPOCHS},\n",
    "    osp.join(SAVE_DIR, 'tau-ep{:03d}.pth'.format(EPOCHS))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GBQHYCxEISKh",
   "metadata": {
    "id": "GBQHYCxEISKh"
   },
   "source": [
    "## Weight decay + tau-norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e8f14295-403c-447d-badf-2cf2d8fdfddc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-22T19:24:34.590720Z",
     "iopub.status.busy": "2022-06-22T19:24:34.590152Z",
     "iopub.status.idle": "2022-06-22T19:24:34.598270Z",
     "shell.execute_reply": "2022-06-22T19:24:34.597309Z",
     "shell.execute_reply.started": "2022-06-22T19:24:34.590669Z"
    },
    "id": "e8f14295-403c-447d-badf-2cf2d8fdfddc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATASET = 'CIFAR10' #['CIFAR10', 'CIFAR100']\n",
    "IMB_TYPE = 'exp' #['exp', 'step']\n",
    "IMB_FACTOR = 0.1 #[0.1, 0.01]\n",
    "SAVE_DIR = 'logs/10-exp-0.1' \n",
    "\n",
    "LR = 0.1\n",
    "BATCH_SIZE = 128\n",
    "MOMENTUM = 0.9\n",
    "WEIGHT_DECAY = 0.000023\n",
    "EPOCHS = 90\n",
    "\n",
    "os.makedirs(osp.join(SAVE_DIR), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9e4766f4-3db2-40df-898c-acb6c242bad4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-22T19:24:39.701381Z",
     "iopub.status.busy": "2022-06-22T19:24:39.700904Z",
     "iopub.status.idle": "2022-06-22T19:37:20.622072Z",
     "shell.execute_reply": "2022-06-22T19:37:20.620608Z",
     "shell.execute_reply.started": "2022-06-22T19:24:39.701329Z"
    },
    "id": "9e4766f4-3db2-40df-898c-acb6c242bad4",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "cls num list:\n",
      "[5000, 3871, 2997, 2320, 1796, 1391, 1077, 834, 645, 500]\n",
      "Epoch: [001] \t Loss 2.2998 \t Acc 42.92 \t AccHead 54.80 \t AccTail 0.20\n",
      "Epoch: [002] \t Loss 1.5921 \t Acc 47.68 \t AccHead 57.71 \t AccTail 11.64\n",
      "Epoch: [003] \t Loss 1.4587 \t Acc 51.78 \t AccHead 61.78 \t AccTail 15.81\n",
      "Epoch: [004] \t Loss 1.3611 \t Acc 53.41 \t AccHead 61.25 \t AccTail 25.27\n",
      "Epoch: [005] \t Loss 1.2992 \t Acc 55.78 \t AccHead 66.57 \t AccTail 16.98\n",
      "Epoch: [006] \t Loss 1.2349 \t Acc 56.99 \t AccHead 66.90 \t AccTail 21.26\n",
      "Epoch: [007] \t Loss 1.1832 \t Acc 60.87 \t AccHead 70.21 \t AccTail 27.25\n",
      "Epoch: [008] \t Loss 1.1181 \t Acc 60.20 \t AccHead 66.15 \t AccTail 38.77\n",
      "Epoch: [009] \t Loss 1.0701 \t Acc 63.95 \t AccHead 70.83 \t AccTail 39.18\n",
      "Epoch: [010] \t Loss 1.0169 \t Acc 66.34 \t AccHead 72.54 \t AccTail 44.09\n",
      "Epoch: [011] \t Loss 0.9777 \t Acc 66.64 \t AccHead 73.39 \t AccTail 42.38\n",
      "Epoch: [012] \t Loss 0.9439 \t Acc 69.30 \t AccHead 75.77 \t AccTail 46.05\n",
      "Epoch: [013] \t Loss 0.8986 \t Acc 66.82 \t AccHead 71.46 \t AccTail 50.15\n",
      "Epoch: [014] \t Loss 0.8663 \t Acc 68.07 \t AccHead 75.95 \t AccTail 39.78\n",
      "Epoch: [015] \t Loss 0.8437 \t Acc 71.36 \t AccHead 77.95 \t AccTail 47.65\n",
      "Epoch: [016] \t Loss 0.8154 \t Acc 71.33 \t AccHead 77.87 \t AccTail 47.86\n",
      "Epoch: [017] \t Loss 0.7944 \t Acc 73.13 \t AccHead 80.27 \t AccTail 47.41\n",
      "Epoch: [018] \t Loss 0.7820 \t Acc 74.34 \t AccHead 79.89 \t AccTail 54.40\n",
      "Epoch: [019] \t Loss 0.7518 \t Acc 74.52 \t AccHead 78.94 \t AccTail 58.58\n",
      "Epoch: [020] \t Loss 0.7306 \t Acc 75.10 \t AccHead 80.61 \t AccTail 55.29\n",
      "Epoch: [021] \t Loss 0.7078 \t Acc 74.82 \t AccHead 78.13 \t AccTail 62.88\n",
      "Epoch: [022] \t Loss 0.6931 \t Acc 75.34 \t AccHead 82.30 \t AccTail 50.34\n",
      "Epoch: [023] \t Loss 0.6859 \t Acc 75.64 \t AccHead 81.45 \t AccTail 54.80\n",
      "Epoch: [024] \t Loss 0.6627 \t Acc 78.59 \t AccHead 81.39 \t AccTail 68.53\n",
      "Epoch: [025] \t Loss 0.6440 \t Acc 77.00 \t AccHead 79.89 \t AccTail 66.62\n",
      "Epoch: [026] \t Loss 0.6256 \t Acc 79.58 \t AccHead 84.23 \t AccTail 62.88\n",
      "Epoch: [027] \t Loss 0.6060 \t Acc 78.86 \t AccHead 84.40 \t AccTail 58.96\n",
      "Epoch: [028] \t Loss 0.6003 \t Acc 80.17 \t AccHead 84.42 \t AccTail 64.92\n",
      "Epoch: [029] \t Loss 0.5855 \t Acc 80.65 \t AccHead 84.06 \t AccTail 68.38\n",
      "Epoch: [030] \t Loss 0.5682 \t Acc 81.80 \t AccHead 85.52 \t AccTail 68.40\n",
      "Epoch: [031] \t Loss 0.5522 \t Acc 80.26 \t AccHead 82.92 \t AccTail 70.71\n",
      "Epoch: [032] \t Loss 0.5522 \t Acc 81.69 \t AccHead 85.93 \t AccTail 66.46\n",
      "Epoch: [033] \t Loss 0.5348 \t Acc 81.35 \t AccHead 84.33 \t AccTail 70.63\n",
      "Epoch: [034] \t Loss 0.5214 \t Acc 82.59 \t AccHead 85.73 \t AccTail 71.32\n",
      "Epoch: [035] \t Loss 0.5167 \t Acc 82.25 \t AccHead 86.70 \t AccTail 66.25\n",
      "Epoch: [036] \t Loss 0.5030 \t Acc 83.32 \t AccHead 86.46 \t AccTail 72.07\n",
      "Epoch: [037] \t Loss 0.4896 \t Acc 83.90 \t AccHead 87.43 \t AccTail 71.23\n",
      "Epoch: [038] \t Loss 0.4727 \t Acc 83.50 \t AccHead 87.50 \t AccTail 69.17\n",
      "Epoch: [039] \t Loss 0.4748 \t Acc 84.95 \t AccHead 90.20 \t AccTail 66.12\n",
      "Epoch: [040] \t Loss 0.4612 \t Acc 85.11 \t AccHead 87.38 \t AccTail 76.96\n",
      "Epoch: [041] \t Loss 0.4457 \t Acc 84.96 \t AccHead 87.23 \t AccTail 76.79\n",
      "Epoch: [042] \t Loss 0.4355 \t Acc 83.85 \t AccHead 84.92 \t AccTail 80.03\n",
      "Epoch: [043] \t Loss 0.4323 \t Acc 85.01 \t AccHead 89.37 \t AccTail 69.32\n",
      "Epoch: [044] \t Loss 0.4198 \t Acc 85.78 \t AccHead 88.48 \t AccTail 76.06\n",
      "Epoch: [045] \t Loss 0.4259 \t Acc 85.07 \t AccHead 87.75 \t AccTail 75.43\n",
      "Epoch: [046] \t Loss 0.4089 \t Acc 86.68 \t AccHead 89.43 \t AccTail 76.78\n",
      "Epoch: [047] \t Loss 0.3994 \t Acc 85.91 \t AccHead 89.23 \t AccTail 74.01\n",
      "Epoch: [048] \t Loss 0.3886 \t Acc 87.91 \t AccHead 90.06 \t AccTail 80.21\n",
      "Epoch: [049] \t Loss 0.3904 \t Acc 86.87 \t AccHead 87.55 \t AccTail 84.42\n",
      "Epoch: [050] \t Loss 0.3764 \t Acc 88.07 \t AccHead 89.52 \t AccTail 82.86\n",
      "Epoch: [051] \t Loss 0.3607 \t Acc 87.46 \t AccHead 90.24 \t AccTail 77.45\n",
      "Epoch: [052] \t Loss 0.3613 \t Acc 88.73 \t AccHead 91.16 \t AccTail 80.00\n",
      "Epoch: [053] \t Loss 0.3470 \t Acc 88.66 \t AccHead 89.85 \t AccTail 84.39\n",
      "Epoch: [054] \t Loss 0.3529 \t Acc 86.81 \t AccHead 90.56 \t AccTail 73.35\n",
      "Epoch: [055] \t Loss 0.3349 \t Acc 88.64 \t AccHead 89.89 \t AccTail 84.15\n",
      "Epoch: [056] \t Loss 0.3369 \t Acc 88.92 \t AccHead 90.33 \t AccTail 83.90\n",
      "Epoch: [057] \t Loss 0.3402 \t Acc 89.90 \t AccHead 91.16 \t AccTail 85.38\n",
      "Epoch: [058] \t Loss 0.3256 \t Acc 88.95 \t AccHead 90.69 \t AccTail 82.72\n",
      "Epoch: [059] \t Loss 0.3199 \t Acc 89.71 \t AccHead 91.25 \t AccTail 84.17\n",
      "Epoch: [060] \t Loss 0.3159 \t Acc 89.59 \t AccHead 93.07 \t AccTail 77.07\n",
      "Epoch: [061] \t Loss 0.3042 \t Acc 90.38 \t AccHead 91.89 \t AccTail 84.97\n",
      "Epoch: [062] \t Loss 0.2996 \t Acc 89.65 \t AccHead 91.78 \t AccTail 82.00\n",
      "Epoch: [063] \t Loss 0.2885 \t Acc 91.26 \t AccHead 93.22 \t AccTail 84.22\n",
      "Epoch: [064] \t Loss 0.2882 \t Acc 89.47 \t AccHead 90.52 \t AccTail 85.66\n",
      "Epoch: [065] \t Loss 0.2849 \t Acc 89.51 \t AccHead 90.71 \t AccTail 85.22\n",
      "Epoch: [066] \t Loss 0.2790 \t Acc 90.86 \t AccHead 91.99 \t AccTail 86.77\n",
      "Epoch: [067] \t Loss 0.2767 \t Acc 91.35 \t AccHead 92.75 \t AccTail 86.34\n",
      "Epoch: [068] \t Loss 0.2737 \t Acc 90.43 \t AccHead 91.26 \t AccTail 87.45\n",
      "Epoch: [069] \t Loss 0.2623 \t Acc 91.92 \t AccHead 94.78 \t AccTail 81.61\n",
      "Epoch: [070] \t Loss 0.2561 \t Acc 91.41 \t AccHead 92.32 \t AccTail 88.13\n",
      "Epoch: [071] \t Loss 0.2525 \t Acc 91.77 \t AccHead 92.72 \t AccTail 88.36\n",
      "Epoch: [072] \t Loss 0.2541 \t Acc 91.67 \t AccHead 93.39 \t AccTail 85.50\n",
      "Epoch: [073] \t Loss 0.2483 \t Acc 91.69 \t AccHead 92.62 \t AccTail 88.37\n",
      "Epoch: [074] \t Loss 0.2395 \t Acc 91.29 \t AccHead 92.68 \t AccTail 86.31\n",
      "Epoch: [075] \t Loss 0.2501 \t Acc 93.80 \t AccHead 94.70 \t AccTail 90.57\n",
      "Epoch: [076] \t Loss 0.2417 \t Acc 92.72 \t AccHead 93.62 \t AccTail 89.49\n",
      "Epoch: [077] \t Loss 0.2326 \t Acc 93.26 \t AccHead 95.19 \t AccTail 86.32\n",
      "Epoch: [078] \t Loss 0.2265 \t Acc 92.35 \t AccHead 93.03 \t AccTail 89.91\n",
      "Epoch: [079] \t Loss 0.2172 \t Acc 92.61 \t AccHead 93.69 \t AccTail 88.73\n",
      "Epoch: [080] \t Loss 0.2165 \t Acc 93.06 \t AccHead 93.53 \t AccTail 91.38\n",
      "Epoch: [081] \t Loss 0.2068 \t Acc 93.73 \t AccHead 94.49 \t AccTail 91.02\n",
      "Epoch: [082] \t Loss 0.2101 \t Acc 93.21 \t AccHead 94.48 \t AccTail 88.67\n",
      "Epoch: [083] \t Loss 0.2211 \t Acc 91.91 \t AccHead 93.23 \t AccTail 87.16\n",
      "Epoch: [084] \t Loss 0.2043 \t Acc 94.18 \t AccHead 95.58 \t AccTail 89.14\n",
      "Epoch: [085] \t Loss 0.2041 \t Acc 93.31 \t AccHead 94.44 \t AccTail 89.23\n",
      "Epoch: [086] \t Loss 0.1944 \t Acc 93.00 \t AccHead 93.59 \t AccTail 90.87\n",
      "Epoch: [087] \t Loss 0.1954 \t Acc 94.32 \t AccHead 95.77 \t AccTail 89.13\n",
      "Epoch: [088] \t Loss 0.1998 \t Acc 94.17 \t AccHead 95.71 \t AccTail 88.66\n",
      "Epoch: [089] \t Loss 0.1871 \t Acc 93.90 \t AccHead 95.17 \t AccTail 89.34\n",
      "Epoch: [090] \t Loss 0.1886 \t Acc 94.72 \t AccHead 95.35 \t AccTail 92.46\n",
      "89\n",
      "Finished Training\n",
      "Acc 73.42 \t AccHead 79.90 \t AccTail 66.94\n",
      "----------------------------\n",
      "get the top-1 accuracy for each of the classes.\n",
      "0.67859012345679\n",
      "Acc 74.95 \t AccHead 75.54 \t AccTail 74.36\n",
      "Accuracy of airplane : 85 %\n",
      "Accuracy of automobile : 92 %\n",
      "Accuracy of  bird : 70 %\n",
      "Accuracy of   cat : 59 %\n",
      "Accuracy of  deer : 70 %\n",
      "Accuracy of   dog : 61 %\n",
      "Accuracy of  frog : 81 %\n",
      "Accuracy of horse : 79 %\n",
      "Accuracy of  ship : 79 %\n",
      "Accuracy of truck : 69 %\n"
     ]
    }
   ],
   "source": [
    "train_dataset, train_loader, test_loader, num_classes = get_loaders()\n",
    "model = ResNet18(num_classes)\n",
    "model = model.cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=150, gamma=0.1)\n",
    "\n",
    "# Let's build our model\n",
    "train(EPOCHS)\n",
    "print('Finished Training')\n",
    "test() \n",
    "print('----------------------------')\n",
    "print('get the top-1 accuracy for each of the classes.')\n",
    "\n",
    "print(tau_list[0])\n",
    "tau_norm = torch.norm(model.classifier.weight, p=2.0, dim=1)\n",
    "tau_norm *= tau_norm**tau_list[0]\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(num_classes):\n",
    "        model.classifier.weight[i] = model.classifier.weight[i].div_(tau_norm[i])\n",
    "        model.classifier.bias[i] = model.classifier.bias[i].div_(float(\"inf\")) # set 0\n",
    "\n",
    "test()\n",
    "avg_accu = testEeahClass()\n",
    "\n",
    "\n",
    "torch.save({\n",
    "    'model': model.state_dict(),\n",
    "    'optimizer': optimizer.state_dict(),\n",
    "    'epoch': EPOCHS},\n",
    "    osp.join(SAVE_DIR, 'tau-ep{:03d}.pth'.format(EPOCHS))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ad868e0c-4bfb-4b3b-8c48-69363775db0d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-22T19:42:12.672153Z",
     "iopub.status.busy": "2022-06-22T19:42:12.671641Z",
     "iopub.status.idle": "2022-06-22T19:42:12.681419Z",
     "shell.execute_reply": "2022-06-22T19:42:12.680324Z",
     "shell.execute_reply.started": "2022-06-22T19:42:12.672097Z"
    },
    "id": "ad868e0c-4bfb-4b3b-8c48-69363775db0d",
    "tags": []
   },
   "outputs": [],
   "source": [
    " \n",
    "\n",
    "DATASET = 'CIFAR10' #['CIFAR10', 'CIFAR100']\n",
    "IMB_TYPE = 'exp' #['exp', 'step']\n",
    "IMB_FACTOR = 0.01 #[0.1, 0.01]\n",
    "SAVE_DIR = 'logs/10-exp-0.01' \n",
    "\n",
    "LR = 0.1\n",
    "BATCH_SIZE = 128\n",
    "MOMENTUM = 0.9\n",
    "WEIGHT_DECAY = 0.000042\n",
    "EPOCHS = 90\n",
    "\n",
    "os.makedirs(osp.join(SAVE_DIR), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "76980153-4307-4d46-962f-f6282dc2b2a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-22T19:42:13.039280Z",
     "iopub.status.busy": "2022-06-22T19:42:13.038796Z",
     "iopub.status.idle": "2022-06-22T19:51:02.739130Z",
     "shell.execute_reply": "2022-06-22T19:51:02.738084Z",
     "shell.execute_reply.started": "2022-06-22T19:42:13.039227Z"
    },
    "id": "76980153-4307-4d46-962f-f6282dc2b2a4",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "cls num list:\n",
      "[5000, 2997, 1796, 1077, 645, 387, 232, 139, 83, 50]\n",
      "Epoch: [001] \t Loss 2.7194 \t Acc 50.17 \t AccHead 54.06 \t AccTail 0.00\n",
      "Epoch: [002] \t Loss 1.4019 \t Acc 57.68 \t AccHead 62.15 \t AccTail 0.00\n",
      "Epoch: [003] \t Loss 1.2421 \t Acc 62.51 \t AccHead 67.35 \t AccTail 0.00\n",
      "Epoch: [004] \t Loss 1.1675 \t Acc 63.19 \t AccHead 68.08 \t AccTail 0.00\n",
      "Epoch: [005] \t Loss 1.1463 \t Acc 64.34 \t AccHead 69.36 \t AccTail 0.00\n",
      "Epoch: [006] \t Loss 1.0543 \t Acc 64.63 \t AccHead 69.65 \t AccTail 0.00\n",
      "Epoch: [007] \t Loss 1.0136 \t Acc 67.84 \t AccHead 73.08 \t AccTail 0.00\n",
      "Epoch: [008] \t Loss 0.9770 \t Acc 67.51 \t AccHead 72.69 \t AccTail 0.79\n",
      "Epoch: [009] \t Loss 0.9483 \t Acc 68.57 \t AccHead 73.75 \t AccTail 1.81\n",
      "Epoch: [010] \t Loss 0.9176 \t Acc 70.26 \t AccHead 75.66 \t AccTail 0.00\n",
      "Epoch: [011] \t Loss 0.9112 \t Acc 70.47 \t AccHead 75.89 \t AccTail 0.68\n",
      "Epoch: [012] \t Loss 0.8984 \t Acc 71.00 \t AccHead 75.39 \t AccTail 14.27\n",
      "Epoch: [013] \t Loss 0.8784 \t Acc 69.94 \t AccHead 74.50 \t AccTail 11.19\n",
      "Epoch: [014] \t Loss 0.8442 \t Acc 71.65 \t AccHead 77.16 \t AccTail 0.68\n",
      "Epoch: [015] \t Loss 0.8094 \t Acc 71.24 \t AccHead 76.32 \t AccTail 5.76\n",
      "Epoch: [016] \t Loss 0.7788 \t Acc 74.75 \t AccHead 79.29 \t AccTail 15.81\n",
      "Epoch: [017] \t Loss 0.7663 \t Acc 74.45 \t AccHead 79.92 \t AccTail 4.06\n",
      "Epoch: [018] \t Loss 0.7550 \t Acc 75.68 \t AccHead 80.61 \t AccTail 12.20\n",
      "Epoch: [019] \t Loss 0.7318 \t Acc 76.38 \t AccHead 81.42 \t AccTail 11.11\n",
      "Epoch: [020] \t Loss 0.6991 \t Acc 76.56 \t AccHead 81.19 \t AccTail 16.69\n",
      "Epoch: [021] \t Loss 0.6814 \t Acc 76.42 \t AccHead 81.11 \t AccTail 15.74\n",
      "Epoch: [022] \t Loss 0.6902 \t Acc 77.74 \t AccHead 82.46 \t AccTail 16.86\n",
      "Epoch: [023] \t Loss 0.6610 \t Acc 77.64 \t AccHead 81.55 \t AccTail 27.40\n",
      "Epoch: [024] \t Loss 0.6390 \t Acc 77.18 \t AccHead 81.46 \t AccTail 22.27\n",
      "Epoch: [025] \t Loss 0.6355 \t Acc 79.66 \t AccHead 83.89 \t AccTail 24.63\n",
      "Epoch: [026] \t Loss 0.6182 \t Acc 80.22 \t AccHead 84.50 \t AccTail 24.74\n",
      "Epoch: [027] \t Loss 0.6057 \t Acc 81.53 \t AccHead 86.05 \t AccTail 23.45\n",
      "Epoch: [028] \t Loss 0.5929 \t Acc 81.05 \t AccHead 84.46 \t AccTail 37.18\n",
      "Epoch: [029] \t Loss 0.5733 \t Acc 79.73 \t AccHead 84.21 \t AccTail 21.95\n",
      "Epoch: [030] \t Loss 0.5661 \t Acc 82.07 \t AccHead 85.57 \t AccTail 36.99\n",
      "Epoch: [031] \t Loss 0.5515 \t Acc 80.81 \t AccHead 83.76 \t AccTail 42.95\n",
      "Epoch: [032] \t Loss 0.5379 \t Acc 81.97 \t AccHead 85.34 \t AccTail 38.25\n",
      "Epoch: [033] \t Loss 0.5442 \t Acc 81.38 \t AccHead 85.49 \t AccTail 28.31\n",
      "Epoch: [034] \t Loss 0.5289 \t Acc 83.41 \t AccHead 85.81 \t AccTail 52.27\n",
      "Epoch: [035] \t Loss 0.5089 \t Acc 82.59 \t AccHead 85.83 \t AccTail 40.84\n",
      "Epoch: [036] \t Loss 0.5064 \t Acc 83.37 \t AccHead 87.12 \t AccTail 34.88\n",
      "Epoch: [037] \t Loss 0.4942 \t Acc 84.22 \t AccHead 87.92 \t AccTail 36.57\n",
      "Epoch: [038] \t Loss 0.4900 \t Acc 81.74 \t AccHead 84.90 \t AccTail 41.08\n",
      "Epoch: [039] \t Loss 0.4786 \t Acc 83.06 \t AccHead 86.39 \t AccTail 40.00\n",
      "Epoch: [040] \t Loss 0.4528 \t Acc 84.23 \t AccHead 87.58 \t AccTail 40.93\n",
      "Epoch: [041] \t Loss 0.4530 \t Acc 84.78 \t AccHead 87.86 \t AccTail 45.01\n",
      "Epoch: [042] \t Loss 0.4531 \t Acc 84.90 \t AccHead 87.67 \t AccTail 49.10\n",
      "Epoch: [043] \t Loss 0.4407 \t Acc 85.43 \t AccHead 88.58 \t AccTail 44.78\n",
      "Epoch: [044] \t Loss 0.4257 \t Acc 85.21 \t AccHead 87.48 \t AccTail 55.75\n",
      "Epoch: [045] \t Loss 0.4219 \t Acc 86.25 \t AccHead 88.74 \t AccTail 54.23\n",
      "Epoch: [046] \t Loss 0.4189 \t Acc 86.59 \t AccHead 89.69 \t AccTail 46.49\n",
      "Epoch: [047] \t Loss 0.4043 \t Acc 86.08 \t AccHead 88.77 \t AccTail 51.42\n",
      "Epoch: [048] \t Loss 0.3852 \t Acc 87.52 \t AccHead 89.27 \t AccTail 64.86\n",
      "Epoch: [049] \t Loss 0.3951 \t Acc 86.04 \t AccHead 88.31 \t AccTail 56.67\n",
      "Epoch: [050] \t Loss 0.3787 \t Acc 87.35 \t AccHead 89.74 \t AccTail 56.45\n",
      "Epoch: [051] \t Loss 0.3689 \t Acc 87.14 \t AccHead 89.38 \t AccTail 58.18\n",
      "Epoch: [052] \t Loss 0.3656 \t Acc 87.16 \t AccHead 89.68 \t AccTail 54.54\n",
      "Epoch: [053] \t Loss 0.3572 \t Acc 88.90 \t AccHead 91.24 \t AccTail 58.57\n",
      "Epoch: [054] \t Loss 0.3551 \t Acc 88.48 \t AccHead 90.73 \t AccTail 59.43\n",
      "Epoch: [055] \t Loss 0.3399 \t Acc 89.21 \t AccHead 91.13 \t AccTail 64.37\n",
      "Epoch: [056] \t Loss 0.3401 \t Acc 88.71 \t AccHead 91.47 \t AccTail 53.22\n",
      "Epoch: [057] \t Loss 0.3372 \t Acc 88.61 \t AccHead 90.74 \t AccTail 61.11\n",
      "Epoch: [058] \t Loss 0.3257 \t Acc 90.70 \t AccHead 92.34 \t AccTail 69.47\n",
      "Epoch: [059] \t Loss 0.3256 \t Acc 89.02 \t AccHead 91.54 \t AccTail 56.56\n",
      "Epoch: [060] \t Loss 0.3002 \t Acc 89.84 \t AccHead 92.15 \t AccTail 59.79\n",
      "Epoch: [061] \t Loss 0.3103 \t Acc 89.86 \t AccHead 92.23 \t AccTail 59.18\n",
      "Epoch: [062] \t Loss 0.2989 \t Acc 89.75 \t AccHead 92.34 \t AccTail 56.48\n",
      "Epoch: [063] \t Loss 0.2944 \t Acc 88.94 \t AccHead 90.19 \t AccTail 72.88\n",
      "Epoch: [064] \t Loss 0.2947 \t Acc 90.40 \t AccHead 91.98 \t AccTail 69.89\n",
      "Epoch: [065] \t Loss 0.2819 \t Acc 91.45 \t AccHead 93.03 \t AccTail 70.88\n",
      "Epoch: [066] \t Loss 0.2784 \t Acc 90.96 \t AccHead 92.19 \t AccTail 74.97\n",
      "Epoch: [067] \t Loss 0.2814 \t Acc 91.50 \t AccHead 93.10 \t AccTail 70.63\n",
      "Epoch: [068] \t Loss 0.2594 \t Acc 91.76 \t AccHead 93.66 \t AccTail 67.16\n",
      "Epoch: [069] \t Loss 0.2734 \t Acc 91.47 \t AccHead 93.04 \t AccTail 71.17\n",
      "Epoch: [070] \t Loss 0.2718 \t Acc 92.33 \t AccHead 94.14 \t AccTail 68.89\n",
      "Epoch: [071] \t Loss 0.2592 \t Acc 92.43 \t AccHead 93.74 \t AccTail 75.57\n",
      "Epoch: [072] \t Loss 0.2405 \t Acc 91.14 \t AccHead 91.73 \t AccTail 83.47\n",
      "Epoch: [073] \t Loss 0.2434 \t Acc 92.44 \t AccHead 93.53 \t AccTail 78.34\n",
      "Epoch: [074] \t Loss 0.2501 \t Acc 91.29 \t AccHead 92.82 \t AccTail 71.48\n",
      "Epoch: [075] \t Loss 0.2404 \t Acc 92.24 \t AccHead 93.04 \t AccTail 81.92\n",
      "Epoch: [076] \t Loss 0.2201 \t Acc 91.47 \t AccHead 93.13 \t AccTail 69.99\n",
      "Epoch: [077] \t Loss 0.2347 \t Acc 92.45 \t AccHead 93.76 \t AccTail 75.48\n",
      "Epoch: [078] \t Loss 0.2240 \t Acc 92.60 \t AccHead 93.82 \t AccTail 76.87\n",
      "Epoch: [079] \t Loss 0.2150 \t Acc 93.16 \t AccHead 94.24 \t AccTail 79.16\n",
      "Epoch: [080] \t Loss 0.2177 \t Acc 92.09 \t AccHead 93.25 \t AccTail 77.15\n",
      "Epoch: [081] \t Loss 0.2118 \t Acc 92.34 \t AccHead 93.12 \t AccTail 82.25\n",
      "Epoch: [082] \t Loss 0.2017 \t Acc 93.30 \t AccHead 94.45 \t AccTail 78.53\n",
      "Epoch: [083] \t Loss 0.2036 \t Acc 93.57 \t AccHead 95.11 \t AccTail 73.61\n",
      "Epoch: [084] \t Loss 0.2032 \t Acc 93.51 \t AccHead 94.26 \t AccTail 83.94\n",
      "Epoch: [085] \t Loss 0.2037 \t Acc 92.73 \t AccHead 93.36 \t AccTail 84.62\n",
      "Epoch: [086] \t Loss 0.1924 \t Acc 93.95 \t AccHead 94.36 \t AccTail 88.55\n",
      "Epoch: [087] \t Loss 0.1965 \t Acc 93.72 \t AccHead 94.36 \t AccTail 85.46\n",
      "Epoch: [088] \t Loss 0.1974 \t Acc 94.66 \t AccHead 95.41 \t AccTail 84.92\n",
      "Epoch: [089] \t Loss 0.1870 \t Acc 94.09 \t AccHead 95.17 \t AccTail 80.11\n",
      "Epoch: [090] \t Loss 0.1925 \t Acc 93.52 \t AccHead 94.63 \t AccTail 79.28\n",
      "89\n",
      "Finished Training\n",
      "Acc 52.05 \t AccHead 73.62 \t AccTail 30.48\n",
      "----------------------------\n",
      "get the top-1 accuracy for each of the classes.\n",
      "Accuracy of airplane : 93 %\n",
      "Accuracy of automobile : 93 %\n",
      "Accuracy of  bird : 71 %\n",
      "Accuracy of   cat : 54 %\n",
      "Accuracy of  deer : 55 %\n",
      "Accuracy of   dog : 38 %\n",
      "Accuracy of  frog : 44 %\n",
      "Accuracy of horse : 49 %\n",
      "Accuracy of  ship :  9 %\n",
      "Accuracy of truck : 10 %\n",
      "0.49658367346938775\n",
      "Acc 55.48 \t AccHead 71.36 \t AccTail 39.60\n",
      "Accuracy of airplane : 89 %\n",
      "Accuracy of automobile : 94 %\n",
      "Accuracy of  bird : 69 %\n",
      "Accuracy of   cat : 44 %\n",
      "Accuracy of  deer : 58 %\n",
      "Accuracy of   dog : 52 %\n",
      "Accuracy of  frog : 55 %\n",
      "Accuracy of horse : 57 %\n",
      "Accuracy of  ship : 22 %\n",
      "Accuracy of truck :  9 %\n"
     ]
    }
   ],
   "source": [
    "train_dataset, train_loader, test_loader, num_classes = get_loaders()\n",
    "model = ResNet18(num_classes)\n",
    "model = model.cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=150, gamma=0.1)\n",
    "\n",
    "# Let's build our model\n",
    "train(EPOCHS)\n",
    "print('Finished Training')\n",
    "test() \n",
    "print('----------------------------')\n",
    "print('get the top-1 accuracy for each of the classes.')\n",
    "avg_accu = testEeahClass()\n",
    "\n",
    "print(tau_list[1])\n",
    "n2_norm = torch.norm(model.classifier.weight, p=2.0, dim=1)\n",
    "n2_norm *= n2_norm**tau_list[1]\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(num_classes):\n",
    "        model.classifier.weight[i] = model.classifier.weight[i].div_(n2_norm[i])\n",
    "        model.classifier.bias[i] = model.classifier.bias[i].div_(float(\"inf\"))\n",
    "\n",
    "test()\n",
    "avg_accu = testEeahClass()\n",
    "\n",
    "\n",
    "torch.save({\n",
    "    'model': model.state_dict(),\n",
    "    'optimizer': optimizer.state_dict(),\n",
    "    'epoch': EPOCHS},\n",
    "    osp.join(SAVE_DIR, 'tau-ep{:03d}.pth'.format(EPOCHS))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b6f4212b-f7b5-48c0-8b66-c49772ad20de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-22T19:59:12.930666Z",
     "iopub.status.busy": "2022-06-22T19:59:12.930191Z",
     "iopub.status.idle": "2022-06-22T19:59:12.939523Z",
     "shell.execute_reply": "2022-06-22T19:59:12.938138Z",
     "shell.execute_reply.started": "2022-06-22T19:59:12.930612Z"
    },
    "id": "b6f4212b-f7b5-48c0-8b66-c49772ad20de",
    "tags": []
   },
   "outputs": [],
   "source": [
    " \n",
    "\n",
    "DATASET = 'CIFAR10' #['CIFAR10', 'CIFAR100']\n",
    "IMB_TYPE = 'step' #['exp', 'step']\n",
    "IMB_FACTOR = 0.1 #[0.1, 0.01]\n",
    "SAVE_DIR = 'logs/10-step-0.1' \n",
    "\n",
    "LR = 0.1\n",
    "BATCH_SIZE = 128\n",
    "MOMENTUM = 0.9\n",
    "WEIGHT_DECAY = 0.000052\n",
    "EPOCHS = 90\n",
    "\n",
    "os.makedirs(osp.join(SAVE_DIR), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ba856e2a-4541-48d3-a8bc-76f8c4df02ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-22T19:59:13.983407Z",
     "iopub.status.busy": "2022-06-22T19:59:13.982818Z",
     "iopub.status.idle": "2022-06-22T20:15:21.653297Z",
     "shell.execute_reply": "2022-06-22T20:15:21.651680Z",
     "shell.execute_reply.started": "2022-06-22T19:59:13.983353Z"
    },
    "id": "ba856e2a-4541-48d3-a8bc-76f8c4df02ce",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "cls num list:\n",
      "[5000, 5000, 5000, 5000, 5000, 500, 500, 500, 500, 500]\n",
      "Epoch: [001] \t Loss 2.2539 \t Acc 44.46 \t AccHead 48.90 \t AccTail 0.00\n",
      "Epoch: [002] \t Loss 1.5034 \t Acc 48.45 \t AccHead 53.30 \t AccTail 0.00\n",
      "Epoch: [003] \t Loss 1.3633 \t Acc 54.46 \t AccHead 59.88 \t AccTail 0.28\n",
      "Epoch: [004] \t Loss 1.2649 \t Acc 57.37 \t AccHead 62.97 \t AccTail 1.41\n",
      "Epoch: [005] \t Loss 1.1891 \t Acc 59.11 \t AccHead 64.47 \t AccTail 5.54\n",
      "Epoch: [006] \t Loss 1.1201 \t Acc 62.12 \t AccHead 67.62 \t AccTail 7.14\n",
      "Epoch: [007] \t Loss 1.0637 \t Acc 64.75 \t AccHead 70.71 \t AccTail 5.14\n",
      "Epoch: [008] \t Loss 1.0029 \t Acc 65.17 \t AccHead 70.64 \t AccTail 10.44\n",
      "Epoch: [009] \t Loss 0.9593 \t Acc 67.47 \t AccHead 72.81 \t AccTail 14.00\n",
      "Epoch: [010] \t Loss 0.9025 \t Acc 70.52 \t AccHead 75.77 \t AccTail 17.99\n",
      "Epoch: [011] \t Loss 0.8661 \t Acc 71.49 \t AccHead 77.28 \t AccTail 13.63\n",
      "Epoch: [012] \t Loss 0.8325 \t Acc 73.70 \t AccHead 78.44 \t AccTail 26.38\n",
      "Epoch: [013] \t Loss 0.7913 \t Acc 73.07 \t AccHead 77.62 \t AccTail 27.48\n",
      "Epoch: [014] \t Loss 0.7630 \t Acc 75.28 \t AccHead 80.23 \t AccTail 25.74\n",
      "Epoch: [015] \t Loss 0.7441 \t Acc 74.88 \t AccHead 79.07 \t AccTail 32.89\n",
      "Epoch: [016] \t Loss 0.7269 \t Acc 75.85 \t AccHead 79.17 \t AccTail 42.45\n",
      "Epoch: [017] \t Loss 0.6929 \t Acc 78.34 \t AccHead 82.37 \t AccTail 38.02\n",
      "Epoch: [018] \t Loss 0.6630 \t Acc 78.26 \t AccHead 81.45 \t AccTail 46.32\n",
      "Epoch: [019] \t Loss 0.6484 \t Acc 79.62 \t AccHead 83.41 \t AccTail 41.80\n",
      "Epoch: [020] \t Loss 0.6272 \t Acc 79.90 \t AccHead 83.46 \t AccTail 44.32\n",
      "Epoch: [021] \t Loss 0.6042 \t Acc 79.49 \t AccHead 83.24 \t AccTail 41.96\n",
      "Epoch: [022] \t Loss 0.5907 \t Acc 81.15 \t AccHead 84.47 \t AccTail 47.97\n",
      "Epoch: [023] \t Loss 0.5828 \t Acc 80.92 \t AccHead 84.08 \t AccTail 49.23\n",
      "Epoch: [024] \t Loss 0.5644 \t Acc 82.01 \t AccHead 85.40 \t AccTail 48.09\n",
      "Epoch: [025] \t Loss 0.5508 \t Acc 81.95 \t AccHead 84.72 \t AccTail 54.15\n",
      "Epoch: [026] \t Loss 0.5301 \t Acc 82.44 \t AccHead 85.88 \t AccTail 47.95\n",
      "Epoch: [027] \t Loss 0.5221 \t Acc 83.68 \t AccHead 87.09 \t AccTail 49.58\n",
      "Epoch: [028] \t Loss 0.5231 \t Acc 83.44 \t AccHead 86.47 \t AccTail 53.11\n",
      "Epoch: [029] \t Loss 0.5007 \t Acc 83.17 \t AccHead 86.70 \t AccTail 47.89\n",
      "Epoch: [030] \t Loss 0.4872 \t Acc 83.69 \t AccHead 86.37 \t AccTail 56.99\n",
      "Epoch: [031] \t Loss 0.4801 \t Acc 84.96 \t AccHead 88.72 \t AccTail 47.45\n",
      "Epoch: [032] \t Loss 0.4687 \t Acc 84.73 \t AccHead 87.75 \t AccTail 54.61\n",
      "Epoch: [033] \t Loss 0.4632 \t Acc 84.89 \t AccHead 87.92 \t AccTail 54.60\n",
      "Epoch: [034] \t Loss 0.4539 \t Acc 85.43 \t AccHead 88.13 \t AccTail 58.47\n",
      "Epoch: [035] \t Loss 0.4453 \t Acc 85.71 \t AccHead 88.92 \t AccTail 53.65\n",
      "Epoch: [036] \t Loss 0.4417 \t Acc 81.40 \t AccHead 84.04 \t AccTail 54.89\n",
      "Epoch: [037] \t Loss 0.4195 \t Acc 85.70 \t AccHead 88.13 \t AccTail 61.45\n",
      "Epoch: [038] \t Loss 0.4182 \t Acc 85.78 \t AccHead 88.28 \t AccTail 60.76\n",
      "Epoch: [039] \t Loss 0.4119 \t Acc 87.38 \t AccHead 89.33 \t AccTail 67.87\n",
      "Epoch: [040] \t Loss 0.3906 \t Acc 87.37 \t AccHead 89.71 \t AccTail 63.99\n",
      "Epoch: [041] \t Loss 0.4000 \t Acc 86.56 \t AccHead 89.79 \t AccTail 54.24\n",
      "Epoch: [042] \t Loss 0.3928 \t Acc 86.11 \t AccHead 88.52 \t AccTail 62.01\n",
      "Epoch: [043] \t Loss 0.3801 \t Acc 87.68 \t AccHead 90.14 \t AccTail 63.07\n",
      "Epoch: [044] \t Loss 0.3728 \t Acc 88.41 \t AccHead 90.39 \t AccTail 68.63\n",
      "Epoch: [045] \t Loss 0.3667 \t Acc 86.53 \t AccHead 88.49 \t AccTail 66.89\n",
      "Epoch: [046] \t Loss 0.3620 \t Acc 88.36 \t AccHead 90.40 \t AccTail 67.99\n",
      "Epoch: [047] \t Loss 0.3577 \t Acc 87.86 \t AccHead 89.72 \t AccTail 69.32\n",
      "Epoch: [048] \t Loss 0.3471 \t Acc 88.83 \t AccHead 90.77 \t AccTail 69.42\n",
      "Epoch: [049] \t Loss 0.3431 \t Acc 89.03 \t AccHead 90.76 \t AccTail 71.65\n",
      "Epoch: [050] \t Loss 0.3347 \t Acc 89.15 \t AccHead 91.04 \t AccTail 70.23\n",
      "Epoch: [051] \t Loss 0.3398 \t Acc 89.06 \t AccHead 91.47 \t AccTail 64.97\n",
      "Epoch: [052] \t Loss 0.3354 \t Acc 88.72 \t AccHead 91.01 \t AccTail 65.82\n",
      "Epoch: [053] \t Loss 0.3260 \t Acc 88.57 \t AccHead 90.47 \t AccTail 69.60\n",
      "Epoch: [054] \t Loss 0.3168 \t Acc 89.32 \t AccHead 90.98 \t AccTail 72.70\n",
      "Epoch: [055] \t Loss 0.3162 \t Acc 90.53 \t AccHead 92.10 \t AccTail 74.85\n",
      "Epoch: [056] \t Loss 0.3175 \t Acc 89.25 \t AccHead 91.14 \t AccTail 70.38\n",
      "Epoch: [057] \t Loss 0.3124 \t Acc 89.45 \t AccHead 90.67 \t AccTail 77.20\n",
      "Epoch: [058] \t Loss 0.3036 \t Acc 89.89 \t AccHead 90.80 \t AccTail 80.81\n",
      "Epoch: [059] \t Loss 0.2999 \t Acc 90.39 \t AccHead 91.34 \t AccTail 80.84\n",
      "Epoch: [060] \t Loss 0.3004 \t Acc 90.28 \t AccHead 91.40 \t AccTail 79.13\n",
      "Epoch: [061] \t Loss 0.2936 \t Acc 90.22 \t AccHead 91.81 \t AccTail 74.31\n",
      "Epoch: [062] \t Loss 0.2919 \t Acc 89.91 \t AccHead 90.66 \t AccTail 82.37\n",
      "Epoch: [063] \t Loss 0.2857 \t Acc 91.21 \t AccHead 92.03 \t AccTail 82.98\n",
      "Epoch: [064] \t Loss 0.2783 \t Acc 91.72 \t AccHead 93.03 \t AccTail 78.63\n",
      "Epoch: [065] \t Loss 0.2759 \t Acc 91.03 \t AccHead 92.59 \t AccTail 75.41\n",
      "Epoch: [066] \t Loss 0.2685 \t Acc 90.32 \t AccHead 92.78 \t AccTail 65.77\n",
      "Epoch: [067] \t Loss 0.2703 \t Acc 91.76 \t AccHead 92.91 \t AccTail 80.19\n",
      "Epoch: [068] \t Loss 0.2685 \t Acc 91.14 \t AccHead 93.28 \t AccTail 69.73\n",
      "Epoch: [069] \t Loss 0.2600 \t Acc 91.45 \t AccHead 92.39 \t AccTail 82.02\n",
      "Epoch: [070] \t Loss 0.2568 \t Acc 91.13 \t AccHead 92.24 \t AccTail 79.97\n",
      "Epoch: [071] \t Loss 0.2561 \t Acc 91.51 \t AccHead 92.70 \t AccTail 79.62\n",
      "Epoch: [072] \t Loss 0.2541 \t Acc 91.70 \t AccHead 93.05 \t AccTail 78.22\n",
      "Epoch: [073] \t Loss 0.2443 \t Acc 91.28 \t AccHead 92.76 \t AccTail 76.51\n",
      "Epoch: [074] \t Loss 0.2375 \t Acc 90.87 \t AccHead 91.69 \t AccTail 82.71\n",
      "Epoch: [075] \t Loss 0.2446 \t Acc 92.38 \t AccHead 93.69 \t AccTail 79.30\n",
      "Epoch: [076] \t Loss 0.2362 \t Acc 92.84 \t AccHead 94.12 \t AccTail 79.96\n",
      "Epoch: [077] \t Loss 0.2375 \t Acc 91.86 \t AccHead 92.70 \t AccTail 83.40\n",
      "Epoch: [078] \t Loss 0.2415 \t Acc 91.50 \t AccHead 92.62 \t AccTail 80.28\n",
      "Epoch: [079] \t Loss 0.2313 \t Acc 91.49 \t AccHead 92.51 \t AccTail 81.26\n",
      "Epoch: [080] \t Loss 0.2304 \t Acc 92.71 \t AccHead 93.90 \t AccTail 80.86\n",
      "Epoch: [081] \t Loss 0.2342 \t Acc 93.32 \t AccHead 94.61 \t AccTail 80.41\n",
      "Epoch: [082] \t Loss 0.2228 \t Acc 92.19 \t AccHead 92.88 \t AccTail 85.35\n",
      "Epoch: [083] \t Loss 0.2303 \t Acc 92.59 \t AccHead 94.01 \t AccTail 78.28\n",
      "Epoch: [084] \t Loss 0.2237 \t Acc 92.18 \t AccHead 92.85 \t AccTail 85.49\n",
      "Epoch: [085] \t Loss 0.2266 \t Acc 93.73 \t AccHead 94.72 \t AccTail 83.81\n",
      "Epoch: [086] \t Loss 0.2137 \t Acc 90.56 \t AccHead 91.01 \t AccTail 86.04\n",
      "Epoch: [087] \t Loss 0.2141 \t Acc 92.36 \t AccHead 93.19 \t AccTail 84.10\n",
      "Epoch: [088] \t Loss 0.2137 \t Acc 93.07 \t AccHead 94.28 \t AccTail 80.95\n",
      "Epoch: [089] \t Loss 0.2146 \t Acc 93.43 \t AccHead 94.69 \t AccTail 80.83\n",
      "Epoch: [090] \t Loss 0.2142 \t Acc 93.18 \t AccHead 94.13 \t AccTail 83.70\n",
      "89\n",
      "Finished Training\n",
      "Acc 69.88 \t AccHead 85.74 \t AccTail 54.02\n",
      "----------------------------\n",
      "get the top-1 accuracy for each of the classes.\n",
      "Accuracy of airplane : 89 %\n",
      "Accuracy of automobile : 94 %\n",
      "Accuracy of  bird : 81 %\n",
      "Accuracy of   cat : 79 %\n",
      "Accuracy of  deer : 83 %\n",
      "Accuracy of   dog : 31 %\n",
      "Accuracy of  frog : 63 %\n",
      "Accuracy of horse : 62 %\n",
      "Accuracy of  ship : 59 %\n",
      "Accuracy of truck : 53 %\n",
      "0.6087934655775963\n",
      "Acc 73.89 \t AccHead 82.48 \t AccTail 65.30\n",
      "Accuracy of airplane : 85 %\n",
      "Accuracy of automobile : 93 %\n",
      "Accuracy of  bird : 81 %\n",
      "Accuracy of   cat : 69 %\n",
      "Accuracy of  deer : 83 %\n",
      "Accuracy of   dog : 45 %\n",
      "Accuracy of  frog : 73 %\n",
      "Accuracy of horse : 71 %\n",
      "Accuracy of  ship : 71 %\n",
      "Accuracy of truck : 65 %\n"
     ]
    }
   ],
   "source": [
    "train_dataset, train_loader, test_loader, num_classes = get_loaders()\n",
    "model = ResNet18(num_classes)\n",
    "model = model.cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=150, gamma=0.1)\n",
    "\n",
    "# Let's build our model\n",
    "train(EPOCHS)\n",
    "print('Finished Training')\n",
    "test() \n",
    "print('----------------------------')\n",
    "print('get the top-1 accuracy for each of the classes.')\n",
    "avg_accu = testEeahClass()\n",
    "\n",
    "print(tau_list[2])\n",
    "n2_norm = torch.norm(model.classifier.weight, p=2.0, dim=1)\n",
    "n2_norm *= n2_norm**tau_list[2]\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(num_classes):\n",
    "        model.classifier.weight[i] = model.classifier.weight[i].div_(n2_norm[i])\n",
    "        model.classifier.bias[i] = model.classifier.bias[i].div_(float(\"inf\"))\n",
    "\n",
    "test()\n",
    "avg_accu = testEeahClass()\n",
    "\n",
    "\n",
    "torch.save({\n",
    "    'model': model.state_dict(),\n",
    "    'optimizer': optimizer.state_dict(),\n",
    "    'epoch': EPOCHS},\n",
    "    osp.join(SAVE_DIR, 'tau-ep{:03d}.pth'.format(EPOCHS))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "4ceeb50a-ee04-47e8-a37c-996632a1b6ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-22T20:15:21.689163Z",
     "iopub.status.busy": "2022-06-22T20:15:21.688999Z",
     "iopub.status.idle": "2022-06-22T20:15:21.694148Z",
     "shell.execute_reply": "2022-06-22T20:15:21.693498Z",
     "shell.execute_reply.started": "2022-06-22T20:15:21.689145Z"
    },
    "id": "4ceeb50a-ee04-47e8-a37c-996632a1b6ee",
    "tags": []
   },
   "outputs": [],
   "source": [
    " \n",
    "\n",
    "DATASET = 'CIFAR10' #['CIFAR10', 'CIFAR100']\n",
    "IMB_TYPE = 'step' #['exp', 'step']\n",
    "IMB_FACTOR = 0.01 #[0.1, 0.01]\n",
    "SAVE_DIR = 'logs/10-step-0.01' \n",
    "\n",
    "LR = 0.1\n",
    "BATCH_SIZE = 128\n",
    "MOMENTUM = 0.9\n",
    "WEIGHT_DECAY = 0.000288\n",
    "EPOCHS = 200\n",
    "\n",
    "os.makedirs(osp.join(SAVE_DIR), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ad17d656-e7d3-461b-87e6-277a550b68f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-22T20:15:21.695837Z",
     "iopub.status.busy": "2022-06-22T20:15:21.695564Z",
     "iopub.status.idle": "2022-06-22T20:48:35.602459Z",
     "shell.execute_reply": "2022-06-22T20:48:35.600998Z",
     "shell.execute_reply.started": "2022-06-22T20:15:21.695815Z"
    },
    "id": "ad17d656-e7d3-461b-87e6-277a550b68f3",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "cls num list:\n",
      "[5000, 5000, 5000, 5000, 5000, 50, 50, 50, 50, 50]\n",
      "Epoch: [001] \t Loss 2.1273 \t Acc 45.65 \t AccHead 46.11 \t AccTail 0.00\n",
      "Epoch: [002] \t Loss 1.2457 \t Acc 53.43 \t AccHead 53.96 \t AccTail 0.00\n",
      "Epoch: [003] \t Loss 1.0876 \t Acc 56.98 \t AccHead 57.55 \t AccTail 0.00\n",
      "Epoch: [004] \t Loss 1.0122 \t Acc 63.42 \t AccHead 64.05 \t AccTail 0.00\n",
      "Epoch: [005] \t Loss 0.9377 \t Acc 61.25 \t AccHead 61.86 \t AccTail 0.00\n",
      "Epoch: [006] \t Loss 0.8685 \t Acc 67.14 \t AccHead 67.81 \t AccTail 0.00\n",
      "Epoch: [007] \t Loss 0.8109 \t Acc 73.04 \t AccHead 73.77 \t AccTail 0.00\n",
      "Epoch: [008] \t Loss 0.7571 \t Acc 73.84 \t AccHead 74.58 \t AccTail 0.00\n",
      "Epoch: [009] \t Loss 0.7127 \t Acc 75.98 \t AccHead 76.74 \t AccTail 0.00\n",
      "Epoch: [010] \t Loss 0.6739 \t Acc 76.38 \t AccHead 77.14 \t AccTail 0.00\n",
      "Epoch: [011] \t Loss 0.6617 \t Acc 77.66 \t AccHead 78.44 \t AccTail 0.00\n",
      "Epoch: [012] \t Loss 0.6432 \t Acc 78.60 \t AccHead 79.38 \t AccTail 0.00\n",
      "Epoch: [013] \t Loss 0.6169 \t Acc 79.36 \t AccHead 80.16 \t AccTail 0.00\n",
      "Epoch: [014] \t Loss 0.6004 \t Acc 78.68 \t AccHead 79.47 \t AccTail 0.00\n",
      "Epoch: [015] \t Loss 0.5902 \t Acc 81.25 \t AccHead 82.06 \t AccTail 0.00\n",
      "Epoch: [016] \t Loss 0.5683 \t Acc 77.45 \t AccHead 78.23 \t AccTail 0.00\n",
      "Epoch: [017] \t Loss 0.5555 \t Acc 78.70 \t AccHead 79.48 \t AccTail 0.00\n",
      "Epoch: [018] \t Loss 0.5546 \t Acc 80.85 \t AccHead 81.66 \t AccTail 0.00\n",
      "Epoch: [019] \t Loss 0.5380 \t Acc 82.09 \t AccHead 82.91 \t AccTail 0.00\n",
      "Epoch: [020] \t Loss 0.5271 \t Acc 83.51 \t AccHead 84.34 \t AccTail 0.00\n",
      "Epoch: [021] \t Loss 0.5111 \t Acc 79.87 \t AccHead 80.67 \t AccTail 0.00\n",
      "Epoch: [022] \t Loss 0.5049 \t Acc 82.21 \t AccHead 83.03 \t AccTail 0.00\n",
      "Epoch: [023] \t Loss 0.4992 \t Acc 82.71 \t AccHead 83.54 \t AccTail 0.00\n",
      "Epoch: [024] \t Loss 0.4823 \t Acc 83.64 \t AccHead 84.48 \t AccTail 0.00\n",
      "Epoch: [025] \t Loss 0.4769 \t Acc 80.26 \t AccHead 81.06 \t AccTail 0.00\n",
      "Epoch: [026] \t Loss 0.4729 \t Acc 83.93 \t AccHead 84.77 \t AccTail 0.00\n",
      "Epoch: [027] \t Loss 0.4674 \t Acc 85.57 \t AccHead 86.42 \t AccTail 0.00\n",
      "Epoch: [028] \t Loss 0.4541 \t Acc 83.85 \t AccHead 84.68 \t AccTail 0.00\n",
      "Epoch: [029] \t Loss 0.4559 \t Acc 85.28 \t AccHead 86.13 \t AccTail 0.00\n",
      "Epoch: [030] \t Loss 0.4532 \t Acc 84.87 \t AccHead 85.72 \t AccTail 0.00\n",
      "Epoch: [031] \t Loss 0.4389 \t Acc 85.69 \t AccHead 86.55 \t AccTail 0.00\n",
      "Epoch: [032] \t Loss 0.4318 \t Acc 84.59 \t AccHead 85.43 \t AccTail 0.00\n",
      "Epoch: [033] \t Loss 0.4308 \t Acc 83.55 \t AccHead 84.36 \t AccTail 2.00\n",
      "Epoch: [034] \t Loss 0.4411 \t Acc 84.49 \t AccHead 85.33 \t AccTail 0.40\n",
      "Epoch: [035] \t Loss 0.4208 \t Acc 84.01 \t AccHead 84.83 \t AccTail 2.40\n",
      "Epoch: [036] \t Loss 0.4194 \t Acc 84.93 \t AccHead 85.78 \t AccTail 0.40\n",
      "Epoch: [037] \t Loss 0.4097 \t Acc 84.75 \t AccHead 85.58 \t AccTail 2.01\n",
      "Epoch: [038] \t Loss 0.4109 \t Acc 84.70 \t AccHead 85.52 \t AccTail 2.00\n",
      "Epoch: [039] \t Loss 0.4090 \t Acc 85.94 \t AccHead 86.77 \t AccTail 2.40\n",
      "Epoch: [040] \t Loss 0.3986 \t Acc 83.34 \t AccHead 84.09 \t AccTail 8.80\n",
      "Epoch: [041] \t Loss 0.4068 \t Acc 84.59 \t AccHead 85.42 \t AccTail 1.61\n",
      "Epoch: [042] \t Loss 0.3971 \t Acc 86.62 \t AccHead 87.40 \t AccTail 8.40\n",
      "Epoch: [043] \t Loss 0.3974 \t Acc 84.49 \t AccHead 85.27 \t AccTail 6.00\n",
      "Epoch: [044] \t Loss 0.3955 \t Acc 83.74 \t AccHead 84.51 \t AccTail 6.80\n",
      "Epoch: [045] \t Loss 0.3924 \t Acc 84.68 \t AccHead 85.44 \t AccTail 8.80\n",
      "Epoch: [046] \t Loss 0.3788 \t Acc 86.66 \t AccHead 87.47 \t AccTail 5.20\n",
      "Epoch: [047] \t Loss 0.3801 \t Acc 86.57 \t AccHead 87.38 \t AccTail 5.62\n",
      "Epoch: [048] \t Loss 0.3805 \t Acc 86.63 \t AccHead 87.37 \t AccTail 12.40\n",
      "Epoch: [049] \t Loss 0.3734 \t Acc 86.87 \t AccHead 87.60 \t AccTail 13.65\n",
      "Epoch: [050] \t Loss 0.3758 \t Acc 85.80 \t AccHead 86.55 \t AccTail 10.80\n",
      "Epoch: [051] \t Loss 0.3744 \t Acc 86.54 \t AccHead 87.24 \t AccTail 17.27\n",
      "Epoch: [052] \t Loss 0.3664 \t Acc 88.00 \t AccHead 88.81 \t AccTail 6.80\n",
      "Epoch: [053] \t Loss 0.3647 \t Acc 85.74 \t AccHead 86.44 \t AccTail 15.66\n",
      "Epoch: [054] \t Loss 0.3733 \t Acc 86.97 \t AccHead 87.69 \t AccTail 15.20\n",
      "Epoch: [055] \t Loss 0.3689 \t Acc 87.48 \t AccHead 88.17 \t AccTail 18.15\n",
      "Epoch: [056] \t Loss 0.3613 \t Acc 88.56 \t AccHead 89.29 \t AccTail 16.00\n",
      "Epoch: [057] \t Loss 0.3586 \t Acc 87.20 \t AccHead 87.98 \t AccTail 9.20\n",
      "Epoch: [058] \t Loss 0.3576 \t Acc 88.46 \t AccHead 89.17 \t AccTail 17.60\n",
      "Epoch: [059] \t Loss 0.3547 \t Acc 87.58 \t AccHead 88.21 \t AccTail 24.80\n",
      "Epoch: [060] \t Loss 0.3462 \t Acc 85.07 \t AccHead 85.66 \t AccTail 26.00\n",
      "Epoch: [061] \t Loss 0.3521 \t Acc 87.62 \t AccHead 88.33 \t AccTail 16.47\n",
      "Epoch: [062] \t Loss 0.3525 \t Acc 87.53 \t AccHead 88.30 \t AccTail 10.80\n",
      "Epoch: [063] \t Loss 0.3396 \t Acc 88.58 \t AccHead 89.33 \t AccTail 13.60\n",
      "Epoch: [064] \t Loss 0.3519 \t Acc 87.52 \t AccHead 88.24 \t AccTail 15.60\n",
      "Epoch: [065] \t Loss 0.3400 \t Acc 85.39 \t AccHead 86.01 \t AccTail 24.10\n",
      "Epoch: [066] \t Loss 0.3446 \t Acc 87.60 \t AccHead 88.23 \t AccTail 24.40\n",
      "Epoch: [067] \t Loss 0.3476 \t Acc 88.48 \t AccHead 89.25 \t AccTail 11.60\n",
      "Epoch: [068] \t Loss 0.3420 \t Acc 89.48 \t AccHead 90.12 \t AccTail 25.60\n",
      "Epoch: [069] \t Loss 0.3247 \t Acc 89.34 \t AccHead 89.97 \t AccTail 25.60\n",
      "Epoch: [070] \t Loss 0.3358 \t Acc 86.99 \t AccHead 87.64 \t AccTail 22.00\n",
      "Epoch: [071] \t Loss 0.3362 \t Acc 89.71 \t AccHead 90.42 \t AccTail 18.80\n",
      "Epoch: [072] \t Loss 0.3377 \t Acc 88.86 \t AccHead 89.42 \t AccTail 33.20\n",
      "Epoch: [073] \t Loss 0.3384 \t Acc 87.31 \t AccHead 87.93 \t AccTail 25.20\n",
      "Epoch: [074] \t Loss 0.3296 \t Acc 89.16 \t AccHead 89.79 \t AccTail 26.40\n",
      "Epoch: [075] \t Loss 0.3319 \t Acc 89.46 \t AccHead 90.14 \t AccTail 21.20\n",
      "Epoch: [076] \t Loss 0.3283 \t Acc 88.00 \t AccHead 88.69 \t AccTail 18.80\n",
      "Epoch: [077] \t Loss 0.3192 \t Acc 88.67 \t AccHead 89.29 \t AccTail 26.40\n",
      "Epoch: [078] \t Loss 0.3211 \t Acc 88.53 \t AccHead 89.18 \t AccTail 24.00\n",
      "Epoch: [079] \t Loss 0.3326 \t Acc 88.38 \t AccHead 89.03 \t AccTail 23.20\n",
      "Epoch: [080] \t Loss 0.3317 \t Acc 88.66 \t AccHead 89.27 \t AccTail 27.71\n",
      "Epoch: [081] \t Loss 0.3221 \t Acc 88.86 \t AccHead 89.51 \t AccTail 24.10\n",
      "Epoch: [082] \t Loss 0.3278 \t Acc 89.08 \t AccHead 89.72 \t AccTail 25.60\n",
      "Epoch: [083] \t Loss 0.3251 \t Acc 87.48 \t AccHead 88.01 \t AccTail 35.20\n",
      "Epoch: [084] \t Loss 0.3236 \t Acc 89.79 \t AccHead 90.41 \t AccTail 28.40\n",
      "Epoch: [085] \t Loss 0.3197 \t Acc 89.97 \t AccHead 90.49 \t AccTail 37.35\n",
      "Epoch: [086] \t Loss 0.3202 \t Acc 89.61 \t AccHead 90.27 \t AccTail 22.89\n",
      "Epoch: [087] \t Loss 0.3162 \t Acc 90.27 \t AccHead 90.91 \t AccTail 26.40\n",
      "Epoch: [088] \t Loss 0.3177 \t Acc 88.09 \t AccHead 88.66 \t AccTail 31.60\n",
      "Epoch: [089] \t Loss 0.3145 \t Acc 88.44 \t AccHead 89.01 \t AccTail 31.60\n",
      "Epoch: [090] \t Loss 0.3164 \t Acc 86.36 \t AccHead 86.95 \t AccTail 28.00\n",
      "Epoch: [091] \t Loss 0.3165 \t Acc 89.19 \t AccHead 89.79 \t AccTail 29.60\n",
      "Epoch: [092] \t Loss 0.3107 \t Acc 88.38 \t AccHead 88.99 \t AccTail 27.31\n",
      "Epoch: [093] \t Loss 0.3221 \t Acc 88.49 \t AccHead 89.10 \t AccTail 27.60\n",
      "Epoch: [094] \t Loss 0.3116 \t Acc 89.89 \t AccHead 90.59 \t AccTail 20.00\n",
      "Epoch: [095] \t Loss 0.3132 \t Acc 90.13 \t AccHead 90.80 \t AccTail 22.80\n",
      "Epoch: [096] \t Loss 0.3155 \t Acc 90.01 \t AccHead 90.55 \t AccTail 36.00\n",
      "Epoch: [097] \t Loss 0.3084 \t Acc 87.78 \t AccHead 88.28 \t AccTail 37.60\n",
      "Epoch: [098] \t Loss 0.3147 \t Acc 89.12 \t AccHead 89.71 \t AccTail 30.12\n",
      "Epoch: [099] \t Loss 0.3148 \t Acc 88.37 \t AccHead 88.88 \t AccTail 38.00\n",
      "Epoch: [100] \t Loss 0.3075 \t Acc 88.55 \t AccHead 89.01 \t AccTail 43.60\n",
      "Epoch: [101] \t Loss 0.3106 \t Acc 89.56 \t AccHead 90.15 \t AccTail 29.72\n",
      "Epoch: [102] \t Loss 0.3044 \t Acc 89.78 \t AccHead 90.32 \t AccTail 35.60\n",
      "Epoch: [103] \t Loss 0.3078 \t Acc 89.21 \t AccHead 89.71 \t AccTail 38.80\n",
      "Epoch: [104] \t Loss 0.3024 \t Acc 89.08 \t AccHead 89.52 \t AccTail 45.60\n",
      "Epoch: [105] \t Loss 0.3106 \t Acc 87.23 \t AccHead 87.78 \t AccTail 31.73\n",
      "Epoch: [106] \t Loss 0.3061 \t Acc 87.90 \t AccHead 88.43 \t AccTail 35.20\n",
      "Epoch: [107] \t Loss 0.3153 \t Acc 90.38 \t AccHead 91.03 \t AccTail 25.60\n",
      "Epoch: [108] \t Loss 0.2958 \t Acc 89.52 \t AccHead 90.08 \t AccTail 34.00\n",
      "Epoch: [109] \t Loss 0.3106 \t Acc 89.28 \t AccHead 89.77 \t AccTail 39.76\n",
      "Epoch: [110] \t Loss 0.3020 \t Acc 88.97 \t AccHead 89.43 \t AccTail 43.60\n",
      "Epoch: [111] \t Loss 0.3025 \t Acc 88.09 \t AccHead 88.61 \t AccTail 36.00\n",
      "Epoch: [112] \t Loss 0.3072 \t Acc 88.58 \t AccHead 89.19 \t AccTail 27.20\n",
      "Epoch: [113] \t Loss 0.3043 \t Acc 89.21 \t AccHead 89.70 \t AccTail 39.60\n",
      "Epoch: [114] \t Loss 0.3008 \t Acc 88.88 \t AccHead 89.45 \t AccTail 31.45\n",
      "Epoch: [115] \t Loss 0.3027 \t Acc 89.47 \t AccHead 90.09 \t AccTail 27.60\n",
      "Epoch: [116] \t Loss 0.2940 \t Acc 87.97 \t AccHead 88.52 \t AccTail 32.80\n",
      "Epoch: [117] \t Loss 0.3070 \t Acc 89.24 \t AccHead 89.86 \t AccTail 27.20\n",
      "Epoch: [118] \t Loss 0.3031 \t Acc 89.23 \t AccHead 89.68 \t AccTail 44.80\n",
      "Epoch: [119] \t Loss 0.2948 \t Acc 89.98 \t AccHead 90.49 \t AccTail 39.20\n",
      "Epoch: [120] \t Loss 0.2937 \t Acc 91.32 \t AccHead 91.76 \t AccTail 47.60\n",
      "Epoch: [121] \t Loss 0.3020 \t Acc 87.58 \t AccHead 88.05 \t AccTail 40.00\n",
      "Epoch: [122] \t Loss 0.2992 \t Acc 89.10 \t AccHead 89.63 \t AccTail 36.40\n",
      "Epoch: [123] \t Loss 0.2987 \t Acc 88.76 \t AccHead 89.21 \t AccTail 43.20\n",
      "Epoch: [124] \t Loss 0.2954 \t Acc 89.17 \t AccHead 89.69 \t AccTail 36.80\n",
      "Epoch: [125] \t Loss 0.2902 \t Acc 88.81 \t AccHead 89.38 \t AccTail 32.00\n",
      "Epoch: [126] \t Loss 0.3037 \t Acc 88.37 \t AccHead 88.89 \t AccTail 36.95\n",
      "Epoch: [127] \t Loss 0.2883 \t Acc 90.84 \t AccHead 91.50 \t AccTail 24.10\n",
      "Epoch: [128] \t Loss 0.2951 \t Acc 89.87 \t AccHead 90.40 \t AccTail 36.80\n",
      "Epoch: [129] \t Loss 0.2966 \t Acc 90.30 \t AccHead 90.76 \t AccTail 44.80\n",
      "Epoch: [130] \t Loss 0.2935 \t Acc 89.97 \t AccHead 90.39 \t AccTail 47.60\n",
      "Epoch: [131] \t Loss 0.3014 \t Acc 87.47 \t AccHead 88.02 \t AccTail 32.53\n",
      "Epoch: [132] \t Loss 0.2984 \t Acc 88.83 \t AccHead 89.20 \t AccTail 52.00\n",
      "Epoch: [133] \t Loss 0.2951 \t Acc 88.85 \t AccHead 89.29 \t AccTail 44.40\n",
      "Epoch: [134] \t Loss 0.2938 \t Acc 89.34 \t AccHead 89.77 \t AccTail 46.40\n",
      "Epoch: [135] \t Loss 0.3010 \t Acc 89.60 \t AccHead 90.18 \t AccTail 31.33\n",
      "Epoch: [136] \t Loss 0.3021 \t Acc 89.53 \t AccHead 90.09 \t AccTail 33.20\n",
      "Epoch: [137] \t Loss 0.2932 \t Acc 90.48 \t AccHead 91.07 \t AccTail 31.05\n",
      "Epoch: [138] \t Loss 0.2890 \t Acc 90.66 \t AccHead 91.06 \t AccTail 50.80\n",
      "Epoch: [139] \t Loss 0.2961 \t Acc 89.44 \t AccHead 90.04 \t AccTail 29.60\n",
      "Epoch: [140] \t Loss 0.2915 \t Acc 89.27 \t AccHead 89.82 \t AccTail 34.40\n",
      "Epoch: [141] \t Loss 0.2901 \t Acc 91.13 \t AccHead 91.53 \t AccTail 51.00\n",
      "Epoch: [142] \t Loss 0.2963 \t Acc 88.91 \t AccHead 89.37 \t AccTail 42.80\n",
      "Epoch: [143] \t Loss 0.2937 \t Acc 90.51 \t AccHead 90.92 \t AccTail 49.60\n",
      "Epoch: [144] \t Loss 0.2879 \t Acc 90.00 \t AccHead 90.44 \t AccTail 46.40\n",
      "Epoch: [145] \t Loss 0.2852 \t Acc 90.42 \t AccHead 91.05 \t AccTail 27.71\n",
      "Epoch: [146] \t Loss 0.2902 \t Acc 89.86 \t AccHead 90.38 \t AccTail 37.35\n",
      "Epoch: [147] \t Loss 0.2935 \t Acc 90.03 \t AccHead 90.50 \t AccTail 42.97\n",
      "Epoch: [148] \t Loss 0.2891 \t Acc 89.63 \t AccHead 90.06 \t AccTail 46.80\n",
      "Epoch: [149] \t Loss 0.2936 \t Acc 89.74 \t AccHead 90.39 \t AccTail 25.20\n",
      "Epoch: [150] \t Loss 0.2857 \t Acc 89.82 \t AccHead 90.34 \t AccTail 38.40\n",
      "Epoch: [151] \t Loss 0.1824 \t Acc 95.40 \t AccHead 95.68 \t AccTail 67.20\n",
      "Epoch: [152] \t Loss 0.1441 \t Acc 96.14 \t AccHead 96.36 \t AccTail 74.00\n",
      "Epoch: [153] \t Loss 0.1309 \t Acc 96.35 \t AccHead 96.57 \t AccTail 74.30\n",
      "Epoch: [154] \t Loss 0.1237 \t Acc 96.74 \t AccHead 96.95 \t AccTail 76.00\n",
      "Epoch: [155] \t Loss 0.1115 \t Acc 96.98 \t AccHead 97.12 \t AccTail 82.73\n",
      "Epoch: [156] \t Loss 0.1022 \t Acc 97.10 \t AccHead 97.23 \t AccTail 84.00\n",
      "Epoch: [157] \t Loss 0.1002 \t Acc 97.45 \t AccHead 97.57 \t AccTail 85.60\n",
      "Epoch: [158] \t Loss 0.0974 \t Acc 97.34 \t AccHead 97.48 \t AccTail 82.80\n",
      "Epoch: [159] \t Loss 0.0869 \t Acc 97.60 \t AccHead 97.74 \t AccTail 83.60\n",
      "Epoch: [160] \t Loss 0.0847 \t Acc 97.64 \t AccHead 97.77 \t AccTail 85.14\n",
      "Epoch: [161] \t Loss 0.0793 \t Acc 97.84 \t AccHead 97.99 \t AccTail 83.13\n",
      "Epoch: [162] \t Loss 0.0761 \t Acc 98.02 \t AccHead 98.11 \t AccTail 88.40\n",
      "Epoch: [163] \t Loss 0.0771 \t Acc 98.07 \t AccHead 98.19 \t AccTail 86.40\n",
      "Epoch: [164] \t Loss 0.0704 \t Acc 98.12 \t AccHead 98.22 \t AccTail 88.00\n",
      "Epoch: [165] \t Loss 0.0697 \t Acc 98.16 \t AccHead 98.24 \t AccTail 90.32\n",
      "Epoch: [166] \t Loss 0.0660 \t Acc 98.28 \t AccHead 98.38 \t AccTail 88.80\n",
      "Epoch: [167] \t Loss 0.0637 \t Acc 98.23 \t AccHead 98.29 \t AccTail 92.00\n",
      "Epoch: [168] \t Loss 0.0648 \t Acc 98.43 \t AccHead 98.50 \t AccTail 91.20\n",
      "Epoch: [169] \t Loss 0.0632 \t Acc 98.23 \t AccHead 98.33 \t AccTail 88.00\n",
      "Epoch: [170] \t Loss 0.0593 \t Acc 98.37 \t AccHead 98.46 \t AccTail 90.00\n",
      "Epoch: [171] \t Loss 0.0537 \t Acc 98.65 \t AccHead 98.74 \t AccTail 89.60\n",
      "Epoch: [172] \t Loss 0.0545 \t Acc 98.59 \t AccHead 98.67 \t AccTail 90.80\n",
      "Epoch: [173] \t Loss 0.0511 \t Acc 98.83 \t AccHead 98.87 \t AccTail 94.00\n",
      "Epoch: [174] \t Loss 0.0528 \t Acc 98.60 \t AccHead 98.66 \t AccTail 92.40\n",
      "Epoch: [175] \t Loss 0.0524 \t Acc 98.62 \t AccHead 98.69 \t AccTail 91.94\n",
      "Epoch: [176] \t Loss 0.0495 \t Acc 98.78 \t AccHead 98.83 \t AccTail 94.40\n",
      "Epoch: [177] \t Loss 0.0495 \t Acc 98.66 \t AccHead 98.72 \t AccTail 92.77\n",
      "Epoch: [178] \t Loss 0.0454 \t Acc 98.85 \t AccHead 98.93 \t AccTail 91.20\n",
      "Epoch: [179] \t Loss 0.0491 \t Acc 98.89 \t AccHead 98.91 \t AccTail 97.19\n",
      "Epoch: [180] \t Loss 0.0446 \t Acc 98.78 \t AccHead 98.82 \t AccTail 94.76\n",
      "Epoch: [181] \t Loss 0.0445 \t Acc 98.91 \t AccHead 98.93 \t AccTail 95.98\n",
      "Epoch: [182] \t Loss 0.0459 \t Acc 98.92 \t AccHead 98.96 \t AccTail 94.80\n",
      "Epoch: [183] \t Loss 0.0468 \t Acc 98.83 \t AccHead 98.86 \t AccTail 96.00\n",
      "Epoch: [184] \t Loss 0.0454 \t Acc 98.99 \t AccHead 99.05 \t AccTail 93.57\n",
      "Epoch: [185] \t Loss 0.0460 \t Acc 98.79 \t AccHead 98.82 \t AccTail 96.40\n",
      "Epoch: [186] \t Loss 0.0437 \t Acc 98.99 \t AccHead 99.05 \t AccTail 93.60\n",
      "Epoch: [187] \t Loss 0.0420 \t Acc 98.84 \t AccHead 98.92 \t AccTail 91.20\n",
      "Epoch: [188] \t Loss 0.0443 \t Acc 98.80 \t AccHead 98.83 \t AccTail 96.40\n",
      "Epoch: [189] \t Loss 0.0370 \t Acc 99.02 \t AccHead 99.05 \t AccTail 96.39\n",
      "Epoch: [190] \t Loss 0.0396 \t Acc 98.88 \t AccHead 98.91 \t AccTail 96.00\n",
      "Epoch: [191] \t Loss 0.0401 \t Acc 99.03 \t AccHead 99.09 \t AccTail 93.20\n",
      "Epoch: [192] \t Loss 0.0429 \t Acc 98.86 \t AccHead 98.91 \t AccTail 93.60\n",
      "Epoch: [193] \t Loss 0.0401 \t Acc 99.07 \t AccHead 99.11 \t AccTail 94.78\n",
      "Epoch: [194] \t Loss 0.0417 \t Acc 99.00 \t AccHead 99.03 \t AccTail 95.58\n",
      "Epoch: [195] \t Loss 0.0415 \t Acc 99.03 \t AccHead 99.05 \t AccTail 97.19\n",
      "Epoch: [196] \t Loss 0.0410 \t Acc 99.14 \t AccHead 99.18 \t AccTail 94.80\n",
      "Epoch: [197] \t Loss 0.0376 \t Acc 99.07 \t AccHead 99.10 \t AccTail 96.40\n",
      "Epoch: [198] \t Loss 0.0371 \t Acc 98.92 \t AccHead 98.95 \t AccTail 95.98\n",
      "Epoch: [199] \t Loss 0.0395 \t Acc 99.03 \t AccHead 99.08 \t AccTail 94.40\n",
      "Epoch: [200] \t Loss 0.0383 \t Acc 99.02 \t AccHead 99.07 \t AccTail 94.40\n",
      "199\n",
      "Finished Training\n",
      "Acc 54.88 \t AccHead 90.90 \t AccTail 18.86\n",
      "----------------------------\n",
      "get the top-1 accuracy for each of the classes.\n",
      "Accuracy of airplane : 93 %\n",
      "Accuracy of automobile : 97 %\n",
      "Accuracy of  bird : 85 %\n",
      "Accuracy of   cat : 88 %\n",
      "Accuracy of  deer : 89 %\n",
      "Accuracy of   dog :  3 %\n",
      "Accuracy of  frog : 18 %\n",
      "Accuracy of horse : 28 %\n",
      "Accuracy of  ship : 22 %\n",
      "Accuracy of truck : 20 %\n",
      "0.44349999999999995\n",
      "Acc 60.12 \t AccHead 90.12 \t AccTail 30.12\n",
      "Accuracy of airplane : 93 %\n",
      "Accuracy of automobile : 97 %\n",
      "Accuracy of  bird : 84 %\n",
      "Accuracy of   cat : 88 %\n",
      "Accuracy of  deer : 88 %\n",
      "Accuracy of   dog :  9 %\n",
      "Accuracy of  frog : 30 %\n",
      "Accuracy of horse : 37 %\n",
      "Accuracy of  ship : 38 %\n",
      "Accuracy of truck : 34 %\n"
     ]
    }
   ],
   "source": [
    "train_dataset, train_loader, test_loader, num_classes = get_loaders()\n",
    "model = ResNet18(num_classes)\n",
    "model = model.cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=150, gamma=0.1)\n",
    "\n",
    "# Let's build our model\n",
    "train(EPOCHS)\n",
    "print('Finished Training')\n",
    "test() \n",
    "print('----------------------------')\n",
    "print('get the top-1 accuracy for each of the classes.')\n",
    "avg_accu = testEeahClass()\n",
    "\n",
    "print(tau_list[3])\n",
    "n2_norm = torch.norm(model.classifier.weight, p=2.0, dim=1)\n",
    "n2_norm *= n2_norm**tau_list[3]\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(num_classes):\n",
    "        model.classifier.weight[i] = model.classifier.weight[i].div_(n2_norm[i])\n",
    "        model.classifier.bias[i] = model.classifier.bias[i].div_(float(\"inf\"))\n",
    "\n",
    "test()\n",
    "avg_accu = testEeahClass()\n",
    "\n",
    "\n",
    "torch.save({\n",
    "    'model': model.state_dict(),\n",
    "    'optimizer': optimizer.state_dict(),\n",
    "    'epoch': EPOCHS},\n",
    "    osp.join(SAVE_DIR, 'tau-ep{:03d}.pth'.format(EPOCHS))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f7397650-ac24-4dac-bc7f-2efe426929a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-22T20:48:35.605405Z",
     "iopub.status.busy": "2022-06-22T20:48:35.604976Z",
     "iopub.status.idle": "2022-06-22T20:48:35.613746Z",
     "shell.execute_reply": "2022-06-22T20:48:35.612858Z",
     "shell.execute_reply.started": "2022-06-22T20:48:35.605346Z"
    },
    "id": "f7397650-ac24-4dac-bc7f-2efe426929a9",
    "tags": []
   },
   "outputs": [],
   "source": [
    " \n",
    "\n",
    "DATASET = 'CIFAR100' #['CIFAR10', 'CIFAR100']\n",
    "IMB_TYPE = 'exp' #['exp', 'step']\n",
    "IMB_FACTOR = 0.1 #[0.1, 0.01]\n",
    "SAVE_DIR = 'logs/100-exp-0.1' \n",
    "\n",
    "LR = 0.1\n",
    "BATCH_SIZE = 128\n",
    "MOMENTUM = 0.9\n",
    "WEIGHT_DECAY = 0.001405\n",
    "EPOCHS = 200\n",
    "\n",
    "os.makedirs(osp.join(SAVE_DIR), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c87acac5-236f-4824-80d3-49d173ae6b32",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-22T20:48:35.615646Z",
     "iopub.status.busy": "2022-06-22T20:48:35.615333Z",
     "iopub.status.idle": "2022-06-22T21:16:03.971340Z",
     "shell.execute_reply": "2022-06-22T21:16:03.969797Z",
     "shell.execute_reply.started": "2022-06-22T20:48:35.615608Z"
    },
    "id": "c87acac5-236f-4824-80d3-49d173ae6b32",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "cls num list:\n",
      "[500, 488, 477, 466, 455, 445, 434, 424, 415, 405, 396, 387, 378, 369, 361, 352, 344, 336, 328, 321, 314, 306, 299, 292, 286, 279, 273, 266, 260, 254, 248, 243, 237, 232, 226, 221, 216, 211, 206, 201, 197, 192, 188, 183, 179, 175, 171, 167, 163, 159, 156, 152, 149, 145, 142, 139, 135, 132, 129, 126, 123, 121, 118, 115, 112, 110, 107, 105, 102, 100, 98, 95, 93, 91, 89, 87, 85, 83, 81, 79, 77, 75, 74, 72, 70, 69, 67, 66, 64, 63, 61, 60, 58, 57, 56, 54, 53, 52, 51, 50]\n",
      "Epoch: [001] \t Loss 4.4929 \t Acc 10.41 \t AccHead 13.57 \t AccTail 0.26\n",
      "Epoch: [002] \t Loss 3.7510 \t Acc 14.08 \t AccHead 17.14 \t AccTail 4.25\n",
      "Epoch: [003] \t Loss 3.5156 \t Acc 17.39 \t AccHead 20.96 \t AccTail 5.96\n",
      "Epoch: [004] \t Loss 3.3513 \t Acc 18.56 \t AccHead 22.98 \t AccTail 4.37\n",
      "Epoch: [005] \t Loss 3.1995 \t Acc 22.78 \t AccHead 27.64 \t AccTail 7.19\n",
      "Epoch: [006] \t Loss 3.0941 \t Acc 25.88 \t AccHead 31.20 \t AccTail 8.75\n",
      "Epoch: [007] \t Loss 2.9741 \t Acc 26.30 \t AccHead 30.87 \t AccTail 11.60\n",
      "Epoch: [008] \t Loss 2.8894 \t Acc 28.35 \t AccHead 32.21 \t AccTail 15.93\n",
      "Epoch: [009] \t Loss 2.7804 \t Acc 29.72 \t AccHead 34.40 \t AccTail 14.72\n",
      "Epoch: [010] \t Loss 2.7348 \t Acc 22.59 \t AccHead 26.56 \t AccTail 9.85\n",
      "Epoch: [011] \t Loss 2.6952 \t Acc 31.64 \t AccHead 36.88 \t AccTail 14.80\n",
      "Epoch: [012] \t Loss 2.6454 \t Acc 31.13 \t AccHead 36.18 \t AccTail 14.89\n",
      "Epoch: [013] \t Loss 2.5950 \t Acc 32.06 \t AccHead 36.45 \t AccTail 17.95\n",
      "Epoch: [014] \t Loss 2.5699 \t Acc 30.52 \t AccHead 35.29 \t AccTail 15.18\n",
      "Epoch: [015] \t Loss 2.5294 \t Acc 32.08 \t AccHead 36.41 \t AccTail 18.19\n",
      "Epoch: [016] \t Loss 2.5193 \t Acc 33.97 \t AccHead 39.01 \t AccTail 17.81\n",
      "Epoch: [017] \t Loss 2.5057 \t Acc 34.51 \t AccHead 38.47 \t AccTail 21.79\n",
      "Epoch: [018] \t Loss 2.4763 \t Acc 34.25 \t AccHead 38.96 \t AccTail 19.12\n",
      "Epoch: [019] \t Loss 2.4514 \t Acc 36.36 \t AccHead 41.37 \t AccTail 20.26\n",
      "Epoch: [020] \t Loss 2.4232 \t Acc 34.37 \t AccHead 39.71 \t AccTail 17.26\n",
      "Epoch: [021] \t Loss 2.4153 \t Acc 33.84 \t AccHead 38.47 \t AccTail 19.01\n",
      "Epoch: [022] \t Loss 2.4143 \t Acc 36.15 \t AccHead 40.62 \t AccTail 21.84\n",
      "Epoch: [023] \t Loss 2.3954 \t Acc 36.19 \t AccHead 40.42 \t AccTail 22.61\n",
      "Epoch: [024] \t Loss 2.3714 \t Acc 39.32 \t AccHead 45.17 \t AccTail 20.47\n",
      "Epoch: [025] \t Loss 2.3446 \t Acc 39.81 \t AccHead 45.28 \t AccTail 22.25\n",
      "Epoch: [026] \t Loss 2.3473 \t Acc 38.56 \t AccHead 43.02 \t AccTail 24.22\n",
      "Epoch: [027] \t Loss 2.3306 \t Acc 37.84 \t AccHead 42.13 \t AccTail 24.08\n",
      "Epoch: [028] \t Loss 2.3329 \t Acc 39.00 \t AccHead 44.40 \t AccTail 21.68\n",
      "Epoch: [029] \t Loss 2.3179 \t Acc 37.05 \t AccHead 42.45 \t AccTail 19.70\n",
      "Epoch: [030] \t Loss 2.3088 \t Acc 36.03 \t AccHead 39.81 \t AccTail 23.95\n",
      "Epoch: [031] \t Loss 2.2922 \t Acc 38.26 \t AccHead 43.86 \t AccTail 20.28\n",
      "Epoch: [032] \t Loss 2.2732 \t Acc 40.31 \t AccHead 44.82 \t AccTail 25.83\n",
      "Epoch: [033] \t Loss 2.2769 \t Acc 37.55 \t AccHead 42.73 \t AccTail 20.91\n",
      "Epoch: [034] \t Loss 2.2613 \t Acc 38.00 \t AccHead 42.78 \t AccTail 22.65\n",
      "Epoch: [035] \t Loss 2.2566 \t Acc 34.84 \t AccHead 39.36 \t AccTail 20.33\n",
      "Epoch: [036] \t Loss 2.2532 \t Acc 36.05 \t AccHead 41.02 \t AccTail 20.09\n",
      "Epoch: [037] \t Loss 2.2652 \t Acc 38.83 \t AccHead 43.81 \t AccTail 22.84\n",
      "Epoch: [038] \t Loss 2.2511 \t Acc 38.95 \t AccHead 43.16 \t AccTail 25.44\n",
      "Epoch: [039] \t Loss 2.2403 \t Acc 38.46 \t AccHead 42.92 \t AccTail 24.11\n",
      "Epoch: [040] \t Loss 2.2333 \t Acc 36.49 \t AccHead 41.15 \t AccTail 21.44\n",
      "Epoch: [041] \t Loss 2.2253 \t Acc 40.18 \t AccHead 44.61 \t AccTail 25.96\n",
      "Epoch: [042] \t Loss 2.2261 \t Acc 39.33 \t AccHead 44.42 \t AccTail 23.03\n",
      "Epoch: [043] \t Loss 2.2262 \t Acc 36.97 \t AccHead 41.88 \t AccTail 21.23\n",
      "Epoch: [044] \t Loss 2.2200 \t Acc 41.98 \t AccHead 46.88 \t AccTail 26.25\n",
      "Epoch: [045] \t Loss 2.2082 \t Acc 39.76 \t AccHead 44.31 \t AccTail 25.16\n",
      "Epoch: [046] \t Loss 2.2005 \t Acc 39.81 \t AccHead 44.19 \t AccTail 25.77\n",
      "Epoch: [047] \t Loss 2.2068 \t Acc 36.17 \t AccHead 41.67 \t AccTail 18.49\n",
      "Epoch: [048] \t Loss 2.2070 \t Acc 37.17 \t AccHead 41.32 \t AccTail 23.85\n",
      "Epoch: [049] \t Loss 2.1902 \t Acc 41.25 \t AccHead 46.06 \t AccTail 25.76\n",
      "Epoch: [050] \t Loss 2.1790 \t Acc 40.10 \t AccHead 45.63 \t AccTail 22.30\n",
      "Epoch: [051] \t Loss 2.1939 \t Acc 39.68 \t AccHead 43.63 \t AccTail 27.02\n",
      "Epoch: [052] \t Loss 2.1669 \t Acc 41.77 \t AccHead 46.89 \t AccTail 25.32\n",
      "Epoch: [053] \t Loss 2.1951 \t Acc 40.71 \t AccHead 44.71 \t AccTail 27.85\n",
      "Epoch: [054] \t Loss 2.1631 \t Acc 42.20 \t AccHead 47.36 \t AccTail 25.63\n",
      "Epoch: [055] \t Loss 2.1719 \t Acc 34.74 \t AccHead 38.21 \t AccTail 23.60\n",
      "Epoch: [056] \t Loss 2.1728 \t Acc 40.07 \t AccHead 45.81 \t AccTail 21.67\n",
      "Epoch: [057] \t Loss 2.1792 \t Acc 40.76 \t AccHead 46.06 \t AccTail 23.73\n",
      "Epoch: [058] \t Loss 2.1551 \t Acc 42.63 \t AccHead 48.08 \t AccTail 25.15\n",
      "Epoch: [059] \t Loss 2.1732 \t Acc 38.31 \t AccHead 42.18 \t AccTail 25.89\n",
      "Epoch: [060] \t Loss 2.1767 \t Acc 39.64 \t AccHead 46.01 \t AccTail 19.16\n",
      "Epoch: [061] \t Loss 2.1670 \t Acc 39.47 \t AccHead 43.95 \t AccTail 25.14\n",
      "Epoch: [062] \t Loss 2.1629 \t Acc 44.43 \t AccHead 48.86 \t AccTail 30.18\n",
      "Epoch: [063] \t Loss 2.1398 \t Acc 40.33 \t AccHead 44.89 \t AccTail 25.67\n",
      "Epoch: [064] \t Loss 2.1539 \t Acc 43.74 \t AccHead 49.40 \t AccTail 25.56\n",
      "Epoch: [065] \t Loss 2.1543 \t Acc 38.90 \t AccHead 44.35 \t AccTail 21.43\n",
      "Epoch: [066] \t Loss 2.1686 \t Acc 44.56 \t AccHead 49.96 \t AccTail 27.25\n",
      "Epoch: [067] \t Loss 2.1437 \t Acc 36.01 \t AccHead 40.46 \t AccTail 21.70\n",
      "Epoch: [068] \t Loss 2.1699 \t Acc 43.14 \t AccHead 47.89 \t AccTail 27.89\n",
      "Epoch: [069] \t Loss 2.1495 \t Acc 39.96 \t AccHead 44.65 \t AccTail 24.90\n",
      "Epoch: [070] \t Loss 2.1277 \t Acc 41.69 \t AccHead 47.16 \t AccTail 24.15\n",
      "Epoch: [071] \t Loss 2.1331 \t Acc 39.48 \t AccHead 44.75 \t AccTail 22.55\n",
      "Epoch: [072] \t Loss 2.1608 \t Acc 39.62 \t AccHead 44.17 \t AccTail 24.99\n",
      "Epoch: [073] \t Loss 2.1315 \t Acc 41.36 \t AccHead 46.39 \t AccTail 25.22\n",
      "Epoch: [074] \t Loss 2.1351 \t Acc 40.82 \t AccHead 44.88 \t AccTail 27.76\n",
      "Epoch: [075] \t Loss 2.1572 \t Acc 42.84 \t AccHead 47.92 \t AccTail 26.50\n",
      "Epoch: [076] \t Loss 2.1238 \t Acc 42.12 \t AccHead 47.23 \t AccTail 25.72\n",
      "Epoch: [077] \t Loss 2.1298 \t Acc 40.40 \t AccHead 43.84 \t AccTail 29.36\n",
      "Epoch: [078] \t Loss 2.1365 \t Acc 42.12 \t AccHead 47.11 \t AccTail 26.06\n",
      "Epoch: [079] \t Loss 2.1256 \t Acc 41.47 \t AccHead 45.32 \t AccTail 29.11\n",
      "Epoch: [080] \t Loss 2.1373 \t Acc 43.42 \t AccHead 48.28 \t AccTail 27.85\n",
      "Epoch: [081] \t Loss 2.1210 \t Acc 41.74 \t AccHead 46.12 \t AccTail 27.68\n",
      "Epoch: [082] \t Loss 2.1284 \t Acc 41.43 \t AccHead 45.77 \t AccTail 27.53\n",
      "Epoch: [083] \t Loss 2.1236 \t Acc 40.06 \t AccHead 43.91 \t AccTail 27.72\n",
      "Epoch: [084] \t Loss 2.1093 \t Acc 42.07 \t AccHead 45.93 \t AccTail 29.66\n",
      "Epoch: [085] \t Loss 2.1346 \t Acc 29.85 \t AccHead 34.96 \t AccTail 13.45\n",
      "Epoch: [086] \t Loss 2.1284 \t Acc 40.70 \t AccHead 45.38 \t AccTail 25.65\n",
      "Epoch: [087] \t Loss 2.1301 \t Acc 42.43 \t AccHead 47.28 \t AccTail 26.87\n",
      "Epoch: [088] \t Loss 2.1286 \t Acc 42.19 \t AccHead 47.84 \t AccTail 24.01\n",
      "Epoch: [089] \t Loss 2.1186 \t Acc 38.53 \t AccHead 42.99 \t AccTail 24.23\n",
      "Epoch: [090] \t Loss 2.1021 \t Acc 39.59 \t AccHead 44.01 \t AccTail 25.41\n",
      "Epoch: [091] \t Loss 2.1261 \t Acc 43.27 \t AccHead 47.83 \t AccTail 28.64\n",
      "Epoch: [092] \t Loss 2.1297 \t Acc 43.18 \t AccHead 48.67 \t AccTail 25.59\n",
      "Epoch: [093] \t Loss 2.1161 \t Acc 39.18 \t AccHead 44.74 \t AccTail 21.32\n",
      "Epoch: [094] \t Loss 2.1370 \t Acc 42.00 \t AccHead 47.25 \t AccTail 25.14\n",
      "Epoch: [095] \t Loss 2.1073 \t Acc 41.85 \t AccHead 46.26 \t AccTail 27.71\n",
      "Epoch: [096] \t Loss 2.1305 \t Acc 39.71 \t AccHead 44.50 \t AccTail 24.35\n",
      "Epoch: [097] \t Loss 2.1299 \t Acc 38.73 \t AccHead 42.45 \t AccTail 26.78\n",
      "Epoch: [098] \t Loss 2.1094 \t Acc 39.66 \t AccHead 44.58 \t AccTail 23.88\n",
      "Epoch: [099] \t Loss 2.1247 \t Acc 39.97 \t AccHead 44.63 \t AccTail 25.02\n",
      "Epoch: [100] \t Loss 2.1149 \t Acc 43.14 \t AccHead 48.78 \t AccTail 25.02\n",
      "Epoch: [101] \t Loss 2.1183 \t Acc 41.32 \t AccHead 46.68 \t AccTail 24.13\n",
      "Epoch: [102] \t Loss 2.0994 \t Acc 42.67 \t AccHead 47.34 \t AccTail 27.66\n",
      "Epoch: [103] \t Loss 2.1239 \t Acc 40.27 \t AccHead 43.50 \t AccTail 29.88\n",
      "Epoch: [104] \t Loss 2.1041 \t Acc 41.05 \t AccHead 44.63 \t AccTail 29.58\n",
      "Epoch: [105] \t Loss 2.1108 \t Acc 37.45 \t AccHead 41.64 \t AccTail 24.00\n",
      "Epoch: [106] \t Loss 2.1156 \t Acc 40.86 \t AccHead 45.32 \t AccTail 26.51\n",
      "Epoch: [107] \t Loss 2.1252 \t Acc 42.98 \t AccHead 49.09 \t AccTail 23.37\n",
      "Epoch: [108] \t Loss 2.1027 \t Acc 41.79 \t AccHead 47.83 \t AccTail 22.42\n",
      "Epoch: [109] \t Loss 2.1051 \t Acc 41.55 \t AccHead 46.37 \t AccTail 26.00\n",
      "Epoch: [110] \t Loss 2.1043 \t Acc 43.07 \t AccHead 47.24 \t AccTail 29.65\n",
      "Epoch: [111] \t Loss 2.1044 \t Acc 38.01 \t AccHead 41.55 \t AccTail 26.65\n",
      "Epoch: [112] \t Loss 2.1057 \t Acc 42.02 \t AccHead 47.19 \t AccTail 25.41\n",
      "Epoch: [113] \t Loss 2.1055 \t Acc 41.86 \t AccHead 45.74 \t AccTail 29.40\n",
      "Epoch: [114] \t Loss 2.0989 \t Acc 42.81 \t AccHead 48.63 \t AccTail 24.17\n",
      "Epoch: [115] \t Loss 2.1026 \t Acc 41.77 \t AccHead 46.64 \t AccTail 26.13\n",
      "Epoch: [116] \t Loss 2.1266 \t Acc 41.72 \t AccHead 46.55 \t AccTail 26.21\n",
      "Epoch: [117] \t Loss 2.1205 \t Acc 42.59 \t AccHead 47.59 \t AccTail 26.57\n",
      "Epoch: [118] \t Loss 2.1079 \t Acc 42.07 \t AccHead 46.59 \t AccTail 27.54\n",
      "Epoch: [119] \t Loss 2.0953 \t Acc 34.06 \t AccHead 36.79 \t AccTail 25.28\n",
      "Epoch: [120] \t Loss 2.0958 \t Acc 42.00 \t AccHead 46.62 \t AccTail 27.22\n",
      "Epoch: [121] \t Loss 2.1145 \t Acc 41.23 \t AccHead 45.09 \t AccTail 28.82\n",
      "Epoch: [122] \t Loss 2.1051 \t Acc 38.82 \t AccHead 41.98 \t AccTail 28.71\n",
      "Epoch: [123] \t Loss 2.1106 \t Acc 40.08 \t AccHead 44.38 \t AccTail 26.21\n",
      "Epoch: [124] \t Loss 2.1005 \t Acc 41.40 \t AccHead 46.52 \t AccTail 24.98\n",
      "Epoch: [125] \t Loss 2.1010 \t Acc 40.69 \t AccHead 45.17 \t AccTail 26.30\n",
      "Epoch: [126] \t Loss 2.0970 \t Acc 41.48 \t AccHead 45.67 \t AccTail 28.02\n",
      "Epoch: [127] \t Loss 2.0701 \t Acc 34.34 \t AccHead 37.68 \t AccTail 23.63\n",
      "Epoch: [128] \t Loss 2.1044 \t Acc 45.26 \t AccHead 50.39 \t AccTail 28.81\n",
      "Epoch: [129] \t Loss 2.1066 \t Acc 45.32 \t AccHead 50.91 \t AccTail 27.36\n",
      "Epoch: [130] \t Loss 2.1012 \t Acc 44.13 \t AccHead 48.07 \t AccTail 31.46\n",
      "Epoch: [131] \t Loss 2.0859 \t Acc 41.31 \t AccHead 45.86 \t AccTail 26.69\n",
      "Epoch: [132] \t Loss 2.0871 \t Acc 44.94 \t AccHead 49.62 \t AccTail 29.93\n",
      "Epoch: [133] \t Loss 2.1032 \t Acc 41.57 \t AccHead 45.41 \t AccTail 29.20\n",
      "Epoch: [134] \t Loss 2.0871 \t Acc 41.98 \t AccHead 46.63 \t AccTail 27.07\n",
      "Epoch: [135] \t Loss 2.1081 \t Acc 39.91 \t AccHead 43.85 \t AccTail 27.23\n",
      "Epoch: [136] \t Loss 2.1116 \t Acc 43.83 \t AccHead 48.21 \t AccTail 29.77\n",
      "Epoch: [137] \t Loss 2.1038 \t Acc 45.25 \t AccHead 50.59 \t AccTail 28.06\n",
      "Epoch: [138] \t Loss 2.0959 \t Acc 40.01 \t AccHead 44.26 \t AccTail 26.33\n",
      "Epoch: [139] \t Loss 2.1018 \t Acc 42.11 \t AccHead 46.39 \t AccTail 28.38\n",
      "Epoch: [140] \t Loss 2.0884 \t Acc 42.27 \t AccHead 46.10 \t AccTail 30.00\n",
      "Epoch: [141] \t Loss 2.0958 \t Acc 41.30 \t AccHead 45.87 \t AccTail 26.60\n",
      "Epoch: [142] \t Loss 2.1060 \t Acc 44.31 \t AccHead 48.70 \t AccTail 30.25\n",
      "Epoch: [143] \t Loss 2.1103 \t Acc 42.96 \t AccHead 47.35 \t AccTail 28.88\n",
      "Epoch: [144] \t Loss 2.0910 \t Acc 43.59 \t AccHead 47.90 \t AccTail 29.73\n",
      "Epoch: [145] \t Loss 2.1095 \t Acc 40.04 \t AccHead 44.43 \t AccTail 25.94\n",
      "Epoch: [146] \t Loss 2.1051 \t Acc 40.76 \t AccHead 44.39 \t AccTail 29.11\n",
      "Epoch: [147] \t Loss 2.0975 \t Acc 42.47 \t AccHead 46.08 \t AccTail 30.84\n",
      "Epoch: [148] \t Loss 2.0838 \t Acc 44.34 \t AccHead 49.90 \t AccTail 26.48\n",
      "Epoch: [149] \t Loss 2.1122 \t Acc 41.39 \t AccHead 45.77 \t AccTail 27.33\n",
      "Epoch: [150] \t Loss 2.0838 \t Acc 37.93 \t AccHead 41.12 \t AccTail 27.68\n",
      "Epoch: [151] \t Loss 1.6168 \t Acc 61.92 \t AccHead 67.40 \t AccTail 44.29\n",
      "Epoch: [152] \t Loss 1.3850 \t Acc 65.11 \t AccHead 70.25 \t AccTail 48.57\n",
      "Epoch: [153] \t Loss 1.2902 \t Acc 67.41 \t AccHead 72.28 \t AccTail 51.82\n",
      "Epoch: [154] \t Loss 1.2128 \t Acc 69.35 \t AccHead 74.43 \t AccTail 53.01\n",
      "Epoch: [155] \t Loss 1.1557 \t Acc 71.12 \t AccHead 75.86 \t AccTail 55.93\n",
      "Epoch: [156] \t Loss 1.0917 \t Acc 72.13 \t AccHead 76.78 \t AccTail 57.20\n",
      "Epoch: [157] \t Loss 1.0537 \t Acc 73.30 \t AccHead 77.57 \t AccTail 59.63\n",
      "Epoch: [158] \t Loss 1.0120 \t Acc 74.33 \t AccHead 78.82 \t AccTail 59.91\n",
      "Epoch: [159] \t Loss 0.9599 \t Acc 76.04 \t AccHead 79.92 \t AccTail 63.56\n",
      "Epoch: [160] \t Loss 0.9254 \t Acc 76.80 \t AccHead 80.77 \t AccTail 64.08\n",
      "Epoch: [161] \t Loss 0.8913 \t Acc 78.53 \t AccHead 82.46 \t AccTail 65.91\n",
      "Epoch: [162] \t Loss 0.8622 \t Acc 79.24 \t AccHead 83.16 \t AccTail 66.64\n",
      "Epoch: [163] \t Loss 0.8149 \t Acc 79.65 \t AccHead 83.25 \t AccTail 68.09\n",
      "Epoch: [164] \t Loss 0.7917 \t Acc 80.67 \t AccHead 84.12 \t AccTail 69.61\n",
      "Epoch: [165] \t Loss 0.7786 \t Acc 80.79 \t AccHead 84.23 \t AccTail 69.72\n",
      "Epoch: [166] \t Loss 0.7404 \t Acc 81.21 \t AccHead 84.71 \t AccTail 69.98\n",
      "Epoch: [167] \t Loss 0.7224 \t Acc 82.11 \t AccHead 85.67 \t AccTail 70.68\n",
      "Epoch: [168] \t Loss 0.6972 \t Acc 83.61 \t AccHead 86.65 \t AccTail 73.84\n",
      "Epoch: [169] \t Loss 0.6750 \t Acc 82.64 \t AccHead 84.76 \t AccTail 75.83\n",
      "Epoch: [170] \t Loss 0.6672 \t Acc 84.00 \t AccHead 86.49 \t AccTail 76.00\n",
      "Epoch: [171] \t Loss 0.6456 \t Acc 84.87 \t AccHead 87.34 \t AccTail 76.92\n",
      "Epoch: [172] \t Loss 0.6275 \t Acc 85.40 \t AccHead 87.27 \t AccTail 79.38\n",
      "Epoch: [173] \t Loss 0.6386 \t Acc 83.63 \t AccHead 86.38 \t AccTail 74.84\n",
      "Epoch: [174] \t Loss 0.5996 \t Acc 85.44 \t AccHead 87.38 \t AccTail 79.23\n",
      "Epoch: [175] \t Loss 0.5839 \t Acc 84.24 \t AccHead 86.19 \t AccTail 77.97\n",
      "Epoch: [176] \t Loss 0.5568 \t Acc 87.14 \t AccHead 89.11 \t AccTail 80.79\n",
      "Epoch: [177] \t Loss 0.5377 \t Acc 86.60 \t AccHead 88.66 \t AccTail 79.98\n",
      "Epoch: [178] \t Loss 0.5433 \t Acc 86.22 \t AccHead 87.90 \t AccTail 80.84\n",
      "Epoch: [179] \t Loss 0.5384 \t Acc 86.60 \t AccHead 88.40 \t AccTail 80.82\n",
      "Epoch: [180] \t Loss 0.5134 \t Acc 87.00 \t AccHead 88.69 \t AccTail 81.58\n",
      "Epoch: [181] \t Loss 0.5238 \t Acc 86.70 \t AccHead 88.37 \t AccTail 81.32\n",
      "Epoch: [182] \t Loss 0.5109 \t Acc 87.94 \t AccHead 89.41 \t AccTail 83.22\n",
      "Epoch: [183] \t Loss 0.5023 \t Acc 88.49 \t AccHead 90.31 \t AccTail 82.64\n",
      "Epoch: [184] \t Loss 0.4811 \t Acc 88.13 \t AccHead 89.83 \t AccTail 82.65\n",
      "Epoch: [185] \t Loss 0.4717 \t Acc 88.20 \t AccHead 89.59 \t AccTail 83.74\n",
      "Epoch: [186] \t Loss 0.4604 \t Acc 87.97 \t AccHead 89.08 \t AccTail 84.41\n",
      "Epoch: [187] \t Loss 0.4583 \t Acc 86.47 \t AccHead 88.33 \t AccTail 80.50\n",
      "Epoch: [188] \t Loss 0.4653 \t Acc 90.06 \t AccHead 91.30 \t AccTail 86.06\n",
      "Epoch: [189] \t Loss 0.4319 \t Acc 88.27 \t AccHead 89.43 \t AccTail 84.54\n",
      "Epoch: [190] \t Loss 0.4215 \t Acc 90.35 \t AccHead 91.62 \t AccTail 86.27\n",
      "Epoch: [191] \t Loss 0.4071 \t Acc 88.59 \t AccHead 89.85 \t AccTail 84.56\n",
      "Epoch: [192] \t Loss 0.4148 \t Acc 88.49 \t AccHead 89.60 \t AccTail 84.92\n",
      "Epoch: [193] \t Loss 0.4285 \t Acc 88.89 \t AccHead 89.74 \t AccTail 86.18\n",
      "Epoch: [194] \t Loss 0.4085 \t Acc 90.83 \t AccHead 91.68 \t AccTail 88.11\n",
      "Epoch: [195] \t Loss 0.3963 \t Acc 89.45 \t AccHead 90.33 \t AccTail 86.60\n",
      "Epoch: [196] \t Loss 0.3990 \t Acc 89.14 \t AccHead 90.48 \t AccTail 84.85\n",
      "Epoch: [197] \t Loss 0.3881 \t Acc 90.38 \t AccHead 92.13 \t AccTail 84.76\n",
      "Epoch: [198] \t Loss 0.3665 \t Acc 90.65 \t AccHead 91.50 \t AccTail 87.91\n",
      "Epoch: [199] \t Loss 0.3591 \t Acc 91.44 \t AccHead 92.44 \t AccTail 88.25\n",
      "Epoch: [200] \t Loss 0.3667 \t Acc 88.02 \t AccHead 89.69 \t AccTail 82.66\n",
      "199\n",
      "Finished Training\n",
      "Acc 41.86 \t AccHead 51.52 \t AccTail 32.20\n",
      "----------------------------\n",
      "get the top-1 accuracy for each of the classes.\n",
      "Accuracy of apple : 91 %\n",
      "Accuracy of aquarium_fish : 77 %\n",
      "Accuracy of  baby : 54 %\n",
      "Accuracy of  bear : 40 %\n",
      "Accuracy of beaver : 45 %\n",
      "Accuracy of   bed : 64 %\n",
      "Accuracy of   bee : 45 %\n",
      "Accuracy of beetle : 62 %\n",
      "Accuracy of bicycle : 76 %\n",
      "Accuracy of bottle : 74 %\n",
      "Accuracy of  bowl : 45 %\n",
      "Accuracy of   boy : 47 %\n",
      "Accuracy of bridge : 74 %\n",
      "Accuracy of   bus : 49 %\n",
      "Accuracy of butterfly : 43 %\n",
      "Accuracy of camel : 49 %\n",
      "Accuracy of   can : 56 %\n",
      "Accuracy of castle : 81 %\n",
      "Accuracy of caterpillar : 41 %\n",
      "Accuracy of cattle : 42 %\n",
      "Accuracy of chair : 81 %\n",
      "Accuracy of chimpanzee : 63 %\n",
      "Accuracy of clock : 47 %\n",
      "Accuracy of cloud : 78 %\n",
      "Accuracy of cockroach : 73 %\n",
      "Accuracy of couch : 43 %\n",
      "Accuracy of  crab : 36 %\n",
      "Accuracy of crocodile : 32 %\n",
      "Accuracy of   cup : 59 %\n",
      "Accuracy of dinosaur : 41 %\n",
      "Accuracy of dolphin : 56 %\n",
      "Accuracy of elephant : 41 %\n",
      "Accuracy of flatfish : 39 %\n",
      "Accuracy of forest : 53 %\n",
      "Accuracy of   fox : 43 %\n",
      "Accuracy of  girl : 24 %\n",
      "Accuracy of hamster : 38 %\n",
      "Accuracy of house : 47 %\n",
      "Accuracy of kangaroo : 33 %\n",
      "Accuracy of keyboard : 58 %\n",
      "Accuracy of  lamp : 43 %\n",
      "Accuracy of lawn_mower : 72 %\n",
      "Accuracy of leopard : 35 %\n",
      "Accuracy of  lion : 40 %\n",
      "Accuracy of lizard : 13 %\n",
      "Accuracy of lobster : 27 %\n",
      "Accuracy of   man : 30 %\n",
      "Accuracy of maple_tree : 62 %\n",
      "Accuracy of motorcycle : 67 %\n",
      "Accuracy of mountain : 47 %\n",
      "Accuracy of mouse : 21 %\n",
      "Accuracy of mushroom : 57 %\n",
      "Accuracy of oak_tree : 43 %\n",
      "Accuracy of orange : 65 %\n",
      "Accuracy of orchid : 62 %\n",
      "Accuracy of otter :  9 %\n",
      "Accuracy of palm_tree : 56 %\n",
      "Accuracy of  pear : 41 %\n",
      "Accuracy of pickup_truck : 51 %\n",
      "Accuracy of pine_tree : 41 %\n",
      "Accuracy of plain : 74 %\n",
      "Accuracy of plate : 23 %\n",
      "Accuracy of poppy : 29 %\n",
      "Accuracy of porcupine : 27 %\n",
      "Accuracy of possum : 14 %\n",
      "Accuracy of rabbit : 17 %\n",
      "Accuracy of raccoon : 19 %\n",
      "Accuracy of   ray : 35 %\n",
      "Accuracy of  road : 59 %\n",
      "Accuracy of rocket : 65 %\n",
      "Accuracy of  rose : 44 %\n",
      "Accuracy of   sea : 44 %\n",
      "Accuracy of  seal :  2 %\n",
      "Accuracy of shark : 22 %\n",
      "Accuracy of shrew :  9 %\n",
      "Accuracy of skunk : 50 %\n",
      "Accuracy of skyscraper : 58 %\n",
      "Accuracy of snail : 14 %\n",
      "Accuracy of snake :  8 %\n",
      "Accuracy of spider : 27 %\n",
      "Accuracy of squirrel : 11 %\n",
      "Accuracy of streetcar : 34 %\n",
      "Accuracy of sunflower : 51 %\n",
      "Accuracy of sweet_pepper : 29 %\n",
      "Accuracy of table : 12 %\n",
      "Accuracy of  tank : 34 %\n",
      "Accuracy of telephone : 33 %\n",
      "Accuracy of television : 35 %\n",
      "Accuracy of tiger : 15 %\n",
      "Accuracy of tractor : 14 %\n",
      "Accuracy of train : 30 %\n",
      "Accuracy of trout : 45 %\n",
      "Accuracy of tulip :  9 %\n",
      "Accuracy of turtle : 16 %\n",
      "Accuracy of wardrobe : 53 %\n",
      "Accuracy of whale : 37 %\n",
      "Accuracy of willow_tree : 19 %\n",
      "Accuracy of  wolf : 17 %\n",
      "Accuracy of woman : 11 %\n",
      "Accuracy of  worm : 19 %\n",
      "0.0175\n",
      "Acc 43.86 \t AccHead 48.78 \t AccTail 38.94\n",
      "Accuracy of apple : 83 %\n",
      "Accuracy of aquarium_fish : 69 %\n",
      "Accuracy of  baby : 50 %\n",
      "Accuracy of  bear : 32 %\n",
      "Accuracy of beaver : 37 %\n",
      "Accuracy of   bed : 61 %\n",
      "Accuracy of   bee : 37 %\n",
      "Accuracy of beetle : 58 %\n",
      "Accuracy of bicycle : 72 %\n",
      "Accuracy of bottle : 68 %\n",
      "Accuracy of  bowl : 40 %\n",
      "Accuracy of   boy : 40 %\n",
      "Accuracy of bridge : 60 %\n",
      "Accuracy of   bus : 38 %\n",
      "Accuracy of butterfly : 34 %\n",
      "Accuracy of camel : 44 %\n",
      "Accuracy of   can : 50 %\n",
      "Accuracy of castle : 81 %\n",
      "Accuracy of caterpillar : 31 %\n",
      "Accuracy of cattle : 39 %\n",
      "Accuracy of chair : 83 %\n",
      "Accuracy of chimpanzee : 67 %\n",
      "Accuracy of clock : 39 %\n",
      "Accuracy of cloud : 78 %\n",
      "Accuracy of cockroach : 75 %\n",
      "Accuracy of couch : 38 %\n",
      "Accuracy of  crab : 36 %\n",
      "Accuracy of crocodile : 31 %\n",
      "Accuracy of   cup : 57 %\n",
      "Accuracy of dinosaur : 39 %\n",
      "Accuracy of dolphin : 51 %\n",
      "Accuracy of elephant : 45 %\n",
      "Accuracy of flatfish : 32 %\n",
      "Accuracy of forest : 50 %\n",
      "Accuracy of   fox : 44 %\n",
      "Accuracy of  girl : 25 %\n",
      "Accuracy of hamster : 41 %\n",
      "Accuracy of house : 48 %\n",
      "Accuracy of kangaroo : 34 %\n",
      "Accuracy of keyboard : 59 %\n",
      "Accuracy of  lamp : 43 %\n",
      "Accuracy of lawn_mower : 74 %\n",
      "Accuracy of leopard : 37 %\n",
      "Accuracy of  lion : 43 %\n",
      "Accuracy of lizard : 12 %\n",
      "Accuracy of lobster : 26 %\n",
      "Accuracy of   man : 33 %\n",
      "Accuracy of maple_tree : 56 %\n",
      "Accuracy of motorcycle : 70 %\n",
      "Accuracy of mountain : 49 %\n",
      "Accuracy of mouse : 20 %\n",
      "Accuracy of mushroom : 57 %\n",
      "Accuracy of oak_tree : 50 %\n",
      "Accuracy of orange : 73 %\n",
      "Accuracy of orchid : 64 %\n",
      "Accuracy of otter : 12 %\n",
      "Accuracy of palm_tree : 64 %\n",
      "Accuracy of  pear : 52 %\n",
      "Accuracy of pickup_truck : 53 %\n",
      "Accuracy of pine_tree : 42 %\n",
      "Accuracy of plain : 80 %\n",
      "Accuracy of plate : 33 %\n",
      "Accuracy of poppy : 37 %\n",
      "Accuracy of porcupine : 32 %\n",
      "Accuracy of possum : 15 %\n",
      "Accuracy of rabbit : 19 %\n",
      "Accuracy of raccoon : 23 %\n",
      "Accuracy of   ray : 40 %\n",
      "Accuracy of  road : 70 %\n",
      "Accuracy of rocket : 67 %\n",
      "Accuracy of  rose : 42 %\n",
      "Accuracy of   sea : 54 %\n",
      "Accuracy of  seal :  2 %\n",
      "Accuracy of shark : 23 %\n",
      "Accuracy of shrew : 14 %\n",
      "Accuracy of skunk : 63 %\n",
      "Accuracy of skyscraper : 70 %\n",
      "Accuracy of snail : 23 %\n",
      "Accuracy of snake : 10 %\n",
      "Accuracy of spider : 34 %\n",
      "Accuracy of squirrel : 15 %\n",
      "Accuracy of streetcar : 45 %\n",
      "Accuracy of sunflower : 60 %\n",
      "Accuracy of sweet_pepper : 36 %\n",
      "Accuracy of table : 18 %\n",
      "Accuracy of  tank : 44 %\n",
      "Accuracy of telephone : 39 %\n",
      "Accuracy of television : 49 %\n",
      "Accuracy of tiger : 26 %\n",
      "Accuracy of tractor : 22 %\n",
      "Accuracy of train : 35 %\n",
      "Accuracy of trout : 56 %\n",
      "Accuracy of tulip : 23 %\n",
      "Accuracy of turtle : 17 %\n",
      "Accuracy of wardrobe : 73 %\n",
      "Accuracy of whale : 49 %\n",
      "Accuracy of willow_tree : 32 %\n",
      "Accuracy of  wolf : 22 %\n",
      "Accuracy of woman : 22 %\n",
      "Accuracy of  worm : 26 %\n"
     ]
    }
   ],
   "source": [
    "train_dataset, train_loader, test_loader, num_classes = get_loaders()\n",
    "model = ResNet18(num_classes)\n",
    "model = model.cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=150, gamma=0.1)\n",
    "\n",
    "# Let's build our model\n",
    "train(EPOCHS)\n",
    "print('Finished Training')\n",
    "test() \n",
    "print('----------------------------')\n",
    "print('get the top-1 accuracy for each of the classes.')\n",
    "avg_accu = testEeahClass()\n",
    "\n",
    "print(tau_list[4])\n",
    "n2_norm = torch.norm(model.classifier.weight, p=2.0, dim=1)\n",
    "n2_norm *= n2_norm**tau_list[4]\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(num_classes):\n",
    "        model.classifier.weight[i] = model.classifier.weight[i].div_(n2_norm[i])\n",
    "        model.classifier.bias[i] = model.classifier.bias[i].div_(float(\"inf\"))\n",
    "\n",
    "test()\n",
    "avg_accu = testEeahClass()\n",
    "\n",
    "\n",
    "torch.save({\n",
    "    'model': model.state_dict(),\n",
    "    'optimizer': optimizer.state_dict(),\n",
    "    'epoch': EPOCHS},\n",
    "    osp.join(SAVE_DIR, 'tau-ep{:03d}.pth'.format(EPOCHS))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "71405251-22a6-4a4d-bc01-4eee3b6643d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-22T21:16:03.974829Z",
     "iopub.status.busy": "2022-06-22T21:16:03.974367Z",
     "iopub.status.idle": "2022-06-22T21:16:04.021847Z",
     "shell.execute_reply": "2022-06-22T21:16:04.020625Z",
     "shell.execute_reply.started": "2022-06-22T21:16:03.974765Z"
    },
    "id": "71405251-22a6-4a4d-bc01-4eee3b6643d3",
    "tags": []
   },
   "outputs": [],
   "source": [
    " \n",
    "\n",
    "DATASET = 'CIFAR100' #['CIFAR10', 'CIFAR100']\n",
    "IMB_TYPE = 'exp' #['exp', 'step']\n",
    "IMB_FACTOR = 0.01 #[0.1, 0.01]\n",
    "SAVE_DIR = 'logs/100-exp-0.01' \n",
    "\n",
    "LR = 0.1\n",
    "BATCH_SIZE = 128\n",
    "MOMENTUM = 0.9\n",
    "WEIGHT_DECAY = 0.001043\n",
    "EPOCHS = 200\n",
    "\n",
    "os.makedirs(osp.join(SAVE_DIR), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "60b0d54d-35c4-4fc1-a727-26cafdea972b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-22T21:16:04.024032Z",
     "iopub.status.busy": "2022-06-22T21:16:04.023648Z",
     "iopub.status.idle": "2022-06-22T21:34:20.162046Z",
     "shell.execute_reply": "2022-06-22T21:34:20.160639Z",
     "shell.execute_reply.started": "2022-06-22T21:16:04.023985Z"
    },
    "id": "60b0d54d-35c4-4fc1-a727-26cafdea972b",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "cls num list:\n",
      "[500, 477, 455, 434, 415, 396, 378, 361, 344, 328, 314, 299, 286, 273, 260, 248, 237, 226, 216, 206, 197, 188, 179, 171, 163, 156, 149, 142, 135, 129, 123, 118, 112, 107, 102, 98, 93, 89, 85, 81, 77, 74, 70, 67, 64, 61, 58, 56, 53, 51, 48, 46, 44, 42, 40, 38, 36, 35, 33, 32, 30, 29, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 15, 15, 14, 13, 13, 12, 12, 11, 11, 10, 10, 9, 9, 8, 8, 7, 7, 7, 6, 6, 6, 6, 5, 5, 5, 5]\n",
      "Epoch: [001] \t Loss 4.2559 \t Acc 13.72 \t AccHead 15.03 \t AccTail 0.00\n",
      "Epoch: [002] \t Loss 3.5742 \t Acc 17.48 \t AccHead 19.13 \t AccTail 0.11\n",
      "Epoch: [003] \t Loss 3.3653 \t Acc 22.06 \t AccHead 24.03 \t AccTail 1.49\n",
      "Epoch: [004] \t Loss 3.1752 \t Acc 22.94 \t AccHead 25.07 \t AccTail 0.64\n",
      "Epoch: [005] \t Loss 3.0355 \t Acc 26.57 \t AccHead 28.94 \t AccTail 1.81\n",
      "Epoch: [006] \t Loss 2.9157 \t Acc 27.19 \t AccHead 29.49 \t AccTail 3.18\n",
      "Epoch: [007] \t Loss 2.8211 \t Acc 30.06 \t AccHead 32.50 \t AccTail 4.48\n",
      "Epoch: [008] \t Loss 2.7360 \t Acc 32.07 \t AccHead 34.58 \t AccTail 5.76\n",
      "Epoch: [009] \t Loss 2.6430 \t Acc 33.17 \t AccHead 35.93 \t AccTail 4.07\n",
      "Epoch: [010] \t Loss 2.5726 \t Acc 33.71 \t AccHead 36.55 \t AccTail 4.05\n",
      "Epoch: [011] \t Loss 2.5100 \t Acc 35.35 \t AccHead 38.07 \t AccTail 6.83\n",
      "Epoch: [012] \t Loss 2.4815 \t Acc 37.29 \t AccHead 40.46 \t AccTail 4.06\n",
      "Epoch: [013] \t Loss 2.4133 \t Acc 38.37 \t AccHead 41.28 \t AccTail 8.09\n",
      "Epoch: [014] \t Loss 2.3638 \t Acc 37.38 \t AccHead 40.16 \t AccTail 8.14\n",
      "Epoch: [015] \t Loss 2.3213 \t Acc 37.87 \t AccHead 40.51 \t AccTail 10.41\n",
      "Epoch: [016] \t Loss 2.2606 \t Acc 42.18 \t AccHead 45.36 \t AccTail 8.68\n",
      "Epoch: [017] \t Loss 2.2222 \t Acc 42.23 \t AccHead 44.81 \t AccTail 15.26\n",
      "Epoch: [018] \t Loss 2.1696 \t Acc 41.29 \t AccHead 44.27 \t AccTail 10.14\n",
      "Epoch: [019] \t Loss 2.1457 \t Acc 43.02 \t AccHead 45.96 \t AccTail 12.15\n",
      "Epoch: [020] \t Loss 2.0932 \t Acc 45.90 \t AccHead 49.22 \t AccTail 11.10\n",
      "Epoch: [021] \t Loss 2.0749 \t Acc 45.12 \t AccHead 47.70 \t AccTail 18.06\n",
      "Epoch: [022] \t Loss 2.0451 \t Acc 42.39 \t AccHead 45.52 \t AccTail 9.52\n",
      "Epoch: [023] \t Loss 2.0115 \t Acc 46.09 \t AccHead 49.04 \t AccTail 15.10\n",
      "Epoch: [024] \t Loss 1.9742 \t Acc 45.20 \t AccHead 47.88 \t AccTail 17.09\n",
      "Epoch: [025] \t Loss 1.9407 \t Acc 46.10 \t AccHead 48.89 \t AccTail 16.79\n",
      "Epoch: [026] \t Loss 1.9433 \t Acc 46.29 \t AccHead 49.13 \t AccTail 16.52\n",
      "Epoch: [027] \t Loss 1.9133 \t Acc 47.36 \t AccHead 50.12 \t AccTail 18.60\n",
      "Epoch: [028] \t Loss 1.8839 \t Acc 47.83 \t AccHead 50.25 \t AccTail 22.58\n",
      "Epoch: [029] \t Loss 1.8684 \t Acc 48.60 \t AccHead 51.72 \t AccTail 15.74\n",
      "Epoch: [030] \t Loss 1.8413 \t Acc 49.99 \t AccHead 53.02 \t AccTail 18.32\n",
      "Epoch: [031] \t Loss 1.8143 \t Acc 51.26 \t AccHead 54.19 \t AccTail 20.55\n",
      "Epoch: [032] \t Loss 1.7749 \t Acc 51.47 \t AccHead 54.81 \t AccTail 16.29\n",
      "Epoch: [033] \t Loss 1.7848 \t Acc 49.08 \t AccHead 52.23 \t AccTail 16.19\n",
      "Epoch: [034] \t Loss 1.7528 \t Acc 51.88 \t AccHead 55.03 \t AccTail 18.87\n",
      "Epoch: [035] \t Loss 1.7472 \t Acc 49.35 \t AccHead 51.98 \t AccTail 21.79\n",
      "Epoch: [036] \t Loss 1.7171 \t Acc 51.76 \t AccHead 54.65 \t AccTail 21.66\n",
      "Epoch: [037] \t Loss 1.6893 \t Acc 52.21 \t AccHead 55.10 \t AccTail 21.90\n",
      "Epoch: [038] \t Loss 1.7021 \t Acc 52.97 \t AccHead 56.47 \t AccTail 16.24\n",
      "Epoch: [039] \t Loss 1.6470 \t Acc 52.55 \t AccHead 55.29 \t AccTail 23.82\n",
      "Epoch: [040] \t Loss 1.6416 \t Acc 52.20 \t AccHead 55.18 \t AccTail 21.17\n",
      "Epoch: [041] \t Loss 1.6534 \t Acc 56.14 \t AccHead 59.09 \t AccTail 25.21\n",
      "Epoch: [042] \t Loss 1.6184 \t Acc 52.66 \t AccHead 55.47 \t AccTail 23.13\n",
      "Epoch: [043] \t Loss 1.5930 \t Acc 55.61 \t AccHead 58.66 \t AccTail 23.78\n",
      "Epoch: [044] \t Loss 1.5997 \t Acc 54.01 \t AccHead 56.87 \t AccTail 24.01\n",
      "Epoch: [045] \t Loss 1.5884 \t Acc 52.04 \t AccHead 54.60 \t AccTail 25.29\n",
      "Epoch: [046] \t Loss 1.5991 \t Acc 54.57 \t AccHead 57.61 \t AccTail 22.73\n",
      "Epoch: [047] \t Loss 1.5617 \t Acc 56.81 \t AccHead 59.13 \t AccTail 32.44\n",
      "Epoch: [048] \t Loss 1.5535 \t Acc 54.06 \t AccHead 56.52 \t AccTail 28.43\n",
      "Epoch: [049] \t Loss 1.5422 \t Acc 58.21 \t AccHead 61.29 \t AccTail 26.09\n",
      "Epoch: [050] \t Loss 1.5574 \t Acc 57.42 \t AccHead 60.37 \t AccTail 26.50\n",
      "Epoch: [051] \t Loss 1.5022 \t Acc 55.63 \t AccHead 58.38 \t AccTail 26.79\n",
      "Epoch: [052] \t Loss 1.4987 \t Acc 56.03 \t AccHead 58.82 \t AccTail 26.71\n",
      "Epoch: [053] \t Loss 1.4983 \t Acc 56.16 \t AccHead 59.70 \t AccTail 19.00\n",
      "Epoch: [054] \t Loss 1.4947 \t Acc 55.65 \t AccHead 58.99 \t AccTail 20.62\n",
      "Epoch: [055] \t Loss 1.4722 \t Acc 57.41 \t AccHead 59.97 \t AccTail 30.56\n",
      "Epoch: [056] \t Loss 1.4648 \t Acc 54.23 \t AccHead 56.79 \t AccTail 27.51\n",
      "Epoch: [057] \t Loss 1.4494 \t Acc 57.41 \t AccHead 60.13 \t AccTail 28.80\n",
      "Epoch: [058] \t Loss 1.4609 \t Acc 57.30 \t AccHead 60.58 \t AccTail 22.95\n",
      "Epoch: [059] \t Loss 1.4286 \t Acc 57.81 \t AccHead 60.18 \t AccTail 32.98\n",
      "Epoch: [060] \t Loss 1.4129 \t Acc 57.32 \t AccHead 60.30 \t AccTail 25.97\n",
      "Epoch: [061] \t Loss 1.4327 \t Acc 56.43 \t AccHead 59.03 \t AccTail 29.26\n",
      "Epoch: [062] \t Loss 1.4001 \t Acc 56.86 \t AccHead 59.92 \t AccTail 24.79\n",
      "Epoch: [063] \t Loss 1.3901 \t Acc 56.94 \t AccHead 59.53 \t AccTail 29.89\n",
      "Epoch: [064] \t Loss 1.3864 \t Acc 57.95 \t AccHead 60.68 \t AccTail 29.38\n",
      "Epoch: [065] \t Loss 1.3825 \t Acc 60.07 \t AccHead 63.44 \t AccTail 24.71\n",
      "Epoch: [066] \t Loss 1.3927 \t Acc 63.65 \t AccHead 66.30 \t AccTail 35.73\n",
      "Epoch: [067] \t Loss 1.3484 \t Acc 54.60 \t AccHead 57.36 \t AccTail 25.72\n",
      "Epoch: [068] \t Loss 1.3757 \t Acc 61.42 \t AccHead 64.41 \t AccTail 30.10\n",
      "Epoch: [069] \t Loss 1.3257 \t Acc 61.62 \t AccHead 64.40 \t AccTail 32.48\n",
      "Epoch: [070] \t Loss 1.3443 \t Acc 63.36 \t AccHead 65.91 \t AccTail 36.42\n",
      "Epoch: [071] \t Loss 1.3115 \t Acc 58.89 \t AccHead 61.50 \t AccTail 31.63\n",
      "Epoch: [072] \t Loss 1.3329 \t Acc 60.70 \t AccHead 63.53 \t AccTail 31.02\n",
      "Epoch: [073] \t Loss 1.3224 \t Acc 57.59 \t AccHead 59.93 \t AccTail 32.90\n",
      "Epoch: [074] \t Loss 1.3232 \t Acc 62.16 \t AccHead 64.42 \t AccTail 38.40\n",
      "Epoch: [075] \t Loss 1.3367 \t Acc 62.76 \t AccHead 65.59 \t AccTail 33.08\n",
      "Epoch: [076] \t Loss 1.3219 \t Acc 60.72 \t AccHead 63.55 \t AccTail 31.09\n",
      "Epoch: [077] \t Loss 1.2887 \t Acc 63.73 \t AccHead 66.79 \t AccTail 31.66\n",
      "Epoch: [078] \t Loss 1.3251 \t Acc 62.05 \t AccHead 64.61 \t AccTail 35.46\n",
      "Epoch: [079] \t Loss 1.3016 \t Acc 61.62 \t AccHead 64.05 \t AccTail 36.11\n",
      "Epoch: [080] \t Loss 1.2727 \t Acc 60.17 \t AccHead 62.64 \t AccTail 34.33\n",
      "Epoch: [081] \t Loss 1.2600 \t Acc 60.48 \t AccHead 63.05 \t AccTail 33.72\n",
      "Epoch: [082] \t Loss 1.3014 \t Acc 59.08 \t AccHead 61.91 \t AccTail 29.47\n",
      "Epoch: [083] \t Loss 1.2700 \t Acc 57.74 \t AccHead 59.88 \t AccTail 35.22\n",
      "Epoch: [084] \t Loss 1.2774 \t Acc 61.99 \t AccHead 64.66 \t AccTail 33.97\n",
      "Epoch: [085] \t Loss 1.2526 \t Acc 63.74 \t AccHead 66.57 \t AccTail 34.01\n",
      "Epoch: [086] \t Loss 1.2538 \t Acc 61.87 \t AccHead 64.32 \t AccTail 36.37\n",
      "Epoch: [087] \t Loss 1.2524 \t Acc 59.11 \t AccHead 62.05 \t AccTail 28.11\n",
      "Epoch: [088] \t Loss 1.2820 \t Acc 58.10 \t AccHead 60.50 \t AccTail 33.05\n",
      "Epoch: [089] \t Loss 1.2422 \t Acc 63.08 \t AccHead 65.35 \t AccTail 39.34\n",
      "Epoch: [090] \t Loss 1.2390 \t Acc 61.89 \t AccHead 63.90 \t AccTail 40.83\n",
      "Epoch: [091] \t Loss 1.2446 \t Acc 64.50 \t AccHead 66.73 \t AccTail 41.13\n",
      "Epoch: [092] \t Loss 1.1935 \t Acc 60.57 \t AccHead 62.98 \t AccTail 35.32\n",
      "Epoch: [093] \t Loss 1.2337 \t Acc 60.60 \t AccHead 62.55 \t AccTail 40.11\n",
      "Epoch: [094] \t Loss 1.1999 \t Acc 61.02 \t AccHead 63.33 \t AccTail 36.85\n",
      "Epoch: [095] \t Loss 1.2109 \t Acc 57.25 \t AccHead 59.63 \t AccTail 32.30\n",
      "Epoch: [096] \t Loss 1.2247 \t Acc 63.15 \t AccHead 65.35 \t AccTail 40.13\n",
      "Epoch: [097] \t Loss 1.1674 \t Acc 65.04 \t AccHead 67.37 \t AccTail 40.62\n",
      "Epoch: [098] \t Loss 1.2133 \t Acc 61.29 \t AccHead 64.00 \t AccTail 32.91\n",
      "Epoch: [099] \t Loss 1.1985 \t Acc 66.11 \t AccHead 68.12 \t AccTail 45.10\n",
      "Epoch: [100] \t Loss 1.1836 \t Acc 62.98 \t AccHead 64.57 \t AccTail 46.37\n",
      "Epoch: [101] \t Loss 1.1981 \t Acc 58.32 \t AccHead 60.56 \t AccTail 34.96\n",
      "Epoch: [102] \t Loss 1.1823 \t Acc 65.35 \t AccHead 67.67 \t AccTail 41.06\n",
      "Epoch: [103] \t Loss 1.1735 \t Acc 63.04 \t AccHead 65.86 \t AccTail 33.58\n",
      "Epoch: [104] \t Loss 1.1650 \t Acc 57.05 \t AccHead 59.40 \t AccTail 32.48\n",
      "Epoch: [105] \t Loss 1.2118 \t Acc 56.94 \t AccHead 59.42 \t AccTail 31.03\n",
      "Epoch: [106] \t Loss 1.1832 \t Acc 65.74 \t AccHead 68.11 \t AccTail 40.89\n",
      "Epoch: [107] \t Loss 1.1622 \t Acc 60.77 \t AccHead 62.99 \t AccTail 37.43\n",
      "Epoch: [108] \t Loss 1.1603 \t Acc 64.67 \t AccHead 67.23 \t AccTail 37.69\n",
      "Epoch: [109] \t Loss 1.1795 \t Acc 63.31 \t AccHead 65.84 \t AccTail 36.68\n",
      "Epoch: [110] \t Loss 1.1622 \t Acc 65.44 \t AccHead 67.95 \t AccTail 39.19\n",
      "Epoch: [111] \t Loss 1.1347 \t Acc 66.15 \t AccHead 67.94 \t AccTail 47.33\n",
      "Epoch: [112] \t Loss 1.1494 \t Acc 58.40 \t AccHead 61.47 \t AccTail 26.25\n",
      "Epoch: [113] \t Loss 1.1413 \t Acc 64.01 \t AccHead 66.52 \t AccTail 37.65\n",
      "Epoch: [114] \t Loss 1.1231 \t Acc 68.30 \t AccHead 70.51 \t AccTail 45.39\n",
      "Epoch: [115] \t Loss 1.1382 \t Acc 65.88 \t AccHead 68.05 \t AccTail 43.27\n",
      "Epoch: [116] \t Loss 1.1320 \t Acc 62.64 \t AccHead 65.26 \t AccTail 35.08\n",
      "Epoch: [117] \t Loss 1.1343 \t Acc 68.50 \t AccHead 70.67 \t AccTail 45.86\n",
      "Epoch: [118] \t Loss 1.1515 \t Acc 66.81 \t AccHead 69.09 \t AccTail 42.84\n",
      "Epoch: [119] \t Loss 1.0925 \t Acc 59.16 \t AccHead 61.80 \t AccTail 31.63\n",
      "Epoch: [120] \t Loss 1.1199 \t Acc 61.02 \t AccHead 63.26 \t AccTail 37.69\n",
      "Epoch: [121] \t Loss 1.1068 \t Acc 66.60 \t AccHead 68.97 \t AccTail 41.79\n",
      "Epoch: [122] \t Loss 1.1279 \t Acc 67.75 \t AccHead 69.67 \t AccTail 47.82\n",
      "Epoch: [123] \t Loss 1.1009 \t Acc 62.69 \t AccHead 64.84 \t AccTail 40.11\n",
      "Epoch: [124] \t Loss 1.1007 \t Acc 63.23 \t AccHead 65.59 \t AccTail 38.57\n",
      "Epoch: [125] \t Loss 1.0906 \t Acc 65.87 \t AccHead 68.78 \t AccTail 35.33\n",
      "Epoch: [126] \t Loss 1.0903 \t Acc 64.71 \t AccHead 66.38 \t AccTail 47.35\n",
      "Epoch: [127] \t Loss 1.0927 \t Acc 67.43 \t AccHead 69.06 \t AccTail 50.43\n",
      "Epoch: [128] \t Loss 1.0831 \t Acc 66.29 \t AccHead 68.11 \t AccTail 47.18\n",
      "Epoch: [129] \t Loss 1.1080 \t Acc 67.35 \t AccHead 69.83 \t AccTail 41.30\n",
      "Epoch: [130] \t Loss 1.0958 \t Acc 61.70 \t AccHead 63.41 \t AccTail 43.80\n",
      "Epoch: [131] \t Loss 1.0777 \t Acc 66.77 \t AccHead 68.78 \t AccTail 45.74\n",
      "Epoch: [132] \t Loss 1.0840 \t Acc 68.09 \t AccHead 69.92 \t AccTail 48.88\n",
      "Epoch: [133] \t Loss 1.0693 \t Acc 67.32 \t AccHead 69.04 \t AccTail 49.31\n",
      "Epoch: [134] \t Loss 1.0724 \t Acc 66.69 \t AccHead 68.80 \t AccTail 44.67\n",
      "Epoch: [135] \t Loss 1.0950 \t Acc 67.75 \t AccHead 69.96 \t AccTail 44.68\n",
      "Epoch: [136] \t Loss 1.0842 \t Acc 65.23 \t AccHead 66.85 \t AccTail 48.24\n",
      "Epoch: [137] \t Loss 1.0946 \t Acc 64.55 \t AccHead 66.65 \t AccTail 42.60\n",
      "Epoch: [138] \t Loss 1.0576 \t Acc 67.33 \t AccHead 69.16 \t AccTail 48.08\n",
      "Epoch: [139] \t Loss 1.0484 \t Acc 66.54 \t AccHead 68.23 \t AccTail 48.77\n",
      "Epoch: [140] \t Loss 1.0605 \t Acc 64.65 \t AccHead 66.77 \t AccTail 42.63\n",
      "Epoch: [141] \t Loss 1.0539 \t Acc 64.16 \t AccHead 66.48 \t AccTail 39.91\n",
      "Epoch: [142] \t Loss 1.0556 \t Acc 66.29 \t AccHead 68.20 \t AccTail 46.26\n",
      "Epoch: [143] \t Loss 1.0356 \t Acc 66.29 \t AccHead 68.10 \t AccTail 47.40\n",
      "Epoch: [144] \t Loss 1.0630 \t Acc 70.93 \t AccHead 72.63 \t AccTail 52.95\n",
      "Epoch: [145] \t Loss 1.0384 \t Acc 67.00 \t AccHead 68.29 \t AccTail 53.61\n",
      "Epoch: [146] \t Loss 1.0653 \t Acc 70.04 \t AccHead 72.19 \t AccTail 47.54\n",
      "Epoch: [147] \t Loss 1.0264 \t Acc 70.14 \t AccHead 71.99 \t AccTail 50.74\n",
      "Epoch: [148] \t Loss 1.0381 \t Acc 66.65 \t AccHead 67.87 \t AccTail 53.85\n",
      "Epoch: [149] \t Loss 1.0308 \t Acc 68.20 \t AccHead 69.97 \t AccTail 49.63\n",
      "Epoch: [150] \t Loss 1.0490 \t Acc 66.59 \t AccHead 68.60 \t AccTail 45.56\n",
      "Epoch: [151] \t Loss 0.6222 \t Acc 90.77 \t AccHead 91.99 \t AccTail 77.99\n",
      "Epoch: [152] \t Loss 0.3756 \t Acc 93.93 \t AccHead 94.77 \t AccTail 85.11\n",
      "Epoch: [153] \t Loss 0.2887 \t Acc 95.79 \t AccHead 96.49 \t AccTail 88.44\n",
      "Epoch: [154] \t Loss 0.2394 \t Acc 96.66 \t AccHead 97.12 \t AccTail 91.91\n",
      "Epoch: [155] \t Loss 0.1995 \t Acc 96.96 \t AccHead 97.27 \t AccTail 93.70\n",
      "Epoch: [156] \t Loss 0.1755 \t Acc 97.43 \t AccHead 97.72 \t AccTail 94.47\n",
      "Epoch: [157] \t Loss 0.1576 \t Acc 98.26 \t AccHead 98.47 \t AccTail 96.06\n",
      "Epoch: [158] \t Loss 0.1373 \t Acc 98.50 \t AccHead 98.72 \t AccTail 96.26\n",
      "Epoch: [159] \t Loss 0.1241 \t Acc 98.69 \t AccHead 98.86 \t AccTail 96.91\n",
      "Epoch: [160] \t Loss 0.1128 \t Acc 99.11 \t AccHead 99.23 \t AccTail 97.87\n",
      "Epoch: [161] \t Loss 0.1031 \t Acc 98.86 \t AccHead 99.05 \t AccTail 96.79\n",
      "Epoch: [162] \t Loss 0.0963 \t Acc 99.08 \t AccHead 99.23 \t AccTail 97.55\n",
      "Epoch: [163] \t Loss 0.0854 \t Acc 99.28 \t AccHead 99.37 \t AccTail 98.39\n",
      "Epoch: [164] \t Loss 0.0771 \t Acc 99.35 \t AccHead 99.42 \t AccTail 98.61\n",
      "Epoch: [165] \t Loss 0.0728 \t Acc 99.46 \t AccHead 99.52 \t AccTail 98.83\n",
      "Epoch: [166] \t Loss 0.0655 \t Acc 99.54 \t AccHead 99.63 \t AccTail 98.62\n",
      "Epoch: [167] \t Loss 0.0642 \t Acc 99.50 \t AccHead 99.59 \t AccTail 98.51\n",
      "Epoch: [168] \t Loss 0.0607 \t Acc 99.64 \t AccHead 99.70 \t AccTail 98.93\n",
      "Epoch: [169] \t Loss 0.0595 \t Acc 99.63 \t AccHead 99.66 \t AccTail 99.25\n",
      "Epoch: [170] \t Loss 0.0550 \t Acc 99.69 \t AccHead 99.69 \t AccTail 99.68\n",
      "Epoch: [171] \t Loss 0.0552 \t Acc 99.63 \t AccHead 99.64 \t AccTail 99.47\n",
      "Epoch: [172] \t Loss 0.0479 \t Acc 99.74 \t AccHead 99.80 \t AccTail 99.15\n",
      "Epoch: [173] \t Loss 0.0443 \t Acc 99.77 \t AccHead 99.79 \t AccTail 99.57\n",
      "Epoch: [174] \t Loss 0.0450 \t Acc 99.80 \t AccHead 99.85 \t AccTail 99.36\n",
      "Epoch: [175] \t Loss 0.0418 \t Acc 99.85 \t AccHead 99.92 \t AccTail 99.15\n",
      "Epoch: [176] \t Loss 0.0402 \t Acc 99.74 \t AccHead 99.78 \t AccTail 99.36\n",
      "Epoch: [177] \t Loss 0.0423 \t Acc 99.78 \t AccHead 99.79 \t AccTail 99.68\n",
      "Epoch: [178] \t Loss 0.0384 \t Acc 99.84 \t AccHead 99.86 \t AccTail 99.68\n",
      "Epoch: [179] \t Loss 0.0370 \t Acc 99.81 \t AccHead 99.83 \t AccTail 99.68\n",
      "Epoch: [180] \t Loss 0.0365 \t Acc 99.85 \t AccHead 99.86 \t AccTail 99.79\n",
      "Epoch: [181] \t Loss 0.0335 \t Acc 99.80 \t AccHead 99.81 \t AccTail 99.68\n",
      "Epoch: [182] \t Loss 0.0328 \t Acc 99.88 \t AccHead 99.88 \t AccTail 99.89\n",
      "Epoch: [183] \t Loss 0.0320 \t Acc 99.88 \t AccHead 99.89 \t AccTail 99.79\n",
      "Epoch: [184] \t Loss 0.0331 \t Acc 99.88 \t AccHead 99.90 \t AccTail 99.68\n",
      "Epoch: [185] \t Loss 0.0319 \t Acc 99.91 \t AccHead 99.93 \t AccTail 99.68\n",
      "Epoch: [186] \t Loss 0.0301 \t Acc 99.93 \t AccHead 99.92 \t AccTail 100.00\n",
      "Epoch: [187] \t Loss 0.0276 \t Acc 99.89 \t AccHead 99.91 \t AccTail 99.68\n",
      "Epoch: [188] \t Loss 0.0282 \t Acc 99.88 \t AccHead 99.88 \t AccTail 99.89\n",
      "Epoch: [189] \t Loss 0.0282 \t Acc 99.93 \t AccHead 99.95 \t AccTail 99.79\n",
      "Epoch: [190] \t Loss 0.0294 \t Acc 99.93 \t AccHead 99.94 \t AccTail 99.89\n",
      "Epoch: [191] \t Loss 0.0315 \t Acc 99.95 \t AccHead 99.95 \t AccTail 100.00\n",
      "Epoch: [192] \t Loss 0.0276 \t Acc 99.95 \t AccHead 99.96 \t AccTail 99.89\n",
      "Epoch: [193] \t Loss 0.0283 \t Acc 99.87 \t AccHead 99.89 \t AccTail 99.68\n",
      "Epoch: [194] \t Loss 0.0263 \t Acc 99.92 \t AccHead 99.92 \t AccTail 99.89\n",
      "Epoch: [195] \t Loss 0.0252 \t Acc 99.93 \t AccHead 99.93 \t AccTail 99.89\n",
      "Epoch: [196] \t Loss 0.0230 \t Acc 99.92 \t AccHead 99.92 \t AccTail 99.89\n",
      "Epoch: [197] \t Loss 0.0254 \t Acc 99.89 \t AccHead 99.91 \t AccTail 99.68\n",
      "Epoch: [198] \t Loss 0.0247 \t Acc 99.90 \t AccHead 99.90 \t AccTail 99.89\n",
      "Epoch: [199] \t Loss 0.0226 \t Acc 99.91 \t AccHead 99.91 \t AccTail 99.89\n",
      "Epoch: [200] \t Loss 0.0257 \t Acc 99.91 \t AccHead 99.91 \t AccTail 99.89\n",
      "199\n",
      "Finished Training\n",
      "Acc 32.09 \t AccHead 50.62 \t AccTail 13.56\n",
      "----------------------------\n",
      "get the top-1 accuracy for each of the classes.\n",
      "Accuracy of apple : 91 %\n",
      "Accuracy of aquarium_fish : 88 %\n",
      "Accuracy of  baby : 59 %\n",
      "Accuracy of  bear : 54 %\n",
      "Accuracy of beaver : 61 %\n",
      "Accuracy of   bed : 69 %\n",
      "Accuracy of   bee : 66 %\n",
      "Accuracy of beetle : 67 %\n",
      "Accuracy of bicycle : 77 %\n",
      "Accuracy of bottle : 73 %\n",
      "Accuracy of  bowl : 50 %\n",
      "Accuracy of   boy : 49 %\n",
      "Accuracy of bridge : 74 %\n",
      "Accuracy of   bus : 69 %\n",
      "Accuracy of butterfly : 58 %\n",
      "Accuracy of camel : 55 %\n",
      "Accuracy of   can : 55 %\n",
      "Accuracy of castle : 72 %\n",
      "Accuracy of caterpillar : 51 %\n",
      "Accuracy of cattle : 42 %\n",
      "Accuracy of chair : 77 %\n",
      "Accuracy of chimpanzee : 60 %\n",
      "Accuracy of clock : 45 %\n",
      "Accuracy of cloud : 86 %\n",
      "Accuracy of cockroach : 70 %\n",
      "Accuracy of couch : 33 %\n",
      "Accuracy of  crab : 42 %\n",
      "Accuracy of crocodile : 27 %\n",
      "Accuracy of   cup : 63 %\n",
      "Accuracy of dinosaur : 36 %\n",
      "Accuracy of dolphin : 64 %\n",
      "Accuracy of elephant : 36 %\n",
      "Accuracy of flatfish : 35 %\n",
      "Accuracy of forest : 47 %\n",
      "Accuracy of   fox : 40 %\n",
      "Accuracy of  girl : 15 %\n",
      "Accuracy of hamster : 30 %\n",
      "Accuracy of house : 25 %\n",
      "Accuracy of kangaroo : 19 %\n",
      "Accuracy of keyboard : 45 %\n",
      "Accuracy of  lamp : 33 %\n",
      "Accuracy of lawn_mower : 62 %\n",
      "Accuracy of leopard : 36 %\n",
      "Accuracy of  lion : 43 %\n",
      "Accuracy of lizard :  7 %\n",
      "Accuracy of lobster :  7 %\n",
      "Accuracy of   man : 16 %\n",
      "Accuracy of maple_tree : 58 %\n",
      "Accuracy of motorcycle : 59 %\n",
      "Accuracy of mountain : 35 %\n",
      "Accuracy of mouse :  4 %\n",
      "Accuracy of mushroom : 18 %\n",
      "Accuracy of oak_tree : 57 %\n",
      "Accuracy of orange : 57 %\n",
      "Accuracy of orchid : 36 %\n",
      "Accuracy of otter :  2 %\n",
      "Accuracy of palm_tree : 40 %\n",
      "Accuracy of  pear : 24 %\n",
      "Accuracy of pickup_truck : 19 %\n",
      "Accuracy of pine_tree : 25 %\n",
      "Accuracy of plain : 61 %\n",
      "Accuracy of plate : 20 %\n",
      "Accuracy of poppy : 27 %\n",
      "Accuracy of porcupine : 11 %\n",
      "Accuracy of possum :  2 %\n",
      "Accuracy of rabbit :  0 %\n",
      "Accuracy of raccoon : 10 %\n",
      "Accuracy of   ray : 15 %\n",
      "Accuracy of  road : 41 %\n",
      "Accuracy of rocket : 24 %\n",
      "Accuracy of  rose : 16 %\n",
      "Accuracy of   sea : 28 %\n",
      "Accuracy of  seal :  1 %\n",
      "Accuracy of shark :  7 %\n",
      "Accuracy of shrew :  4 %\n",
      "Accuracy of skunk : 16 %\n",
      "Accuracy of skyscraper : 29 %\n",
      "Accuracy of snail :  1 %\n",
      "Accuracy of snake :  0 %\n",
      "Accuracy of spider :  4 %\n",
      "Accuracy of squirrel :  1 %\n",
      "Accuracy of streetcar :  0 %\n",
      "Accuracy of sunflower : 17 %\n",
      "Accuracy of sweet_pepper :  2 %\n",
      "Accuracy of table :  0 %\n",
      "Accuracy of  tank :  6 %\n",
      "Accuracy of telephone :  0 %\n",
      "Accuracy of television :  5 %\n",
      "Accuracy of tiger :  1 %\n",
      "Accuracy of tractor :  2 %\n",
      "Accuracy of train :  0 %\n",
      "Accuracy of trout :  3 %\n",
      "Accuracy of tulip :  0 %\n",
      "Accuracy of turtle :  2 %\n",
      "Accuracy of wardrobe : 27 %\n",
      "Accuracy of whale :  8 %\n",
      "Accuracy of willow_tree :  2 %\n",
      "Accuracy of  wolf :  0 %\n",
      "Accuracy of woman :  0 %\n",
      "Accuracy of  worm :  3 %\n",
      "0.07400000000000001\n",
      "Acc 35.85 \t AccHead 48.30 \t AccTail 23.40\n",
      "Accuracy of apple : 87 %\n",
      "Accuracy of aquarium_fish : 79 %\n",
      "Accuracy of  baby : 47 %\n",
      "Accuracy of  bear : 36 %\n",
      "Accuracy of beaver : 49 %\n",
      "Accuracy of   bed : 62 %\n",
      "Accuracy of   bee : 58 %\n",
      "Accuracy of beetle : 57 %\n",
      "Accuracy of bicycle : 70 %\n",
      "Accuracy of bottle : 71 %\n",
      "Accuracy of  bowl : 38 %\n",
      "Accuracy of   boy : 44 %\n",
      "Accuracy of bridge : 58 %\n",
      "Accuracy of   bus : 58 %\n",
      "Accuracy of butterfly : 54 %\n",
      "Accuracy of camel : 51 %\n",
      "Accuracy of   can : 50 %\n",
      "Accuracy of castle : 69 %\n",
      "Accuracy of caterpillar : 39 %\n",
      "Accuracy of cattle : 39 %\n",
      "Accuracy of chair : 77 %\n",
      "Accuracy of chimpanzee : 68 %\n",
      "Accuracy of clock : 39 %\n",
      "Accuracy of cloud : 83 %\n",
      "Accuracy of cockroach : 76 %\n",
      "Accuracy of couch : 33 %\n",
      "Accuracy of  crab : 41 %\n",
      "Accuracy of crocodile : 22 %\n",
      "Accuracy of   cup : 64 %\n",
      "Accuracy of dinosaur : 35 %\n",
      "Accuracy of dolphin : 62 %\n",
      "Accuracy of elephant : 40 %\n",
      "Accuracy of flatfish : 36 %\n",
      "Accuracy of forest : 47 %\n",
      "Accuracy of   fox : 43 %\n",
      "Accuracy of  girl : 15 %\n",
      "Accuracy of hamster : 36 %\n",
      "Accuracy of house : 27 %\n",
      "Accuracy of kangaroo : 22 %\n",
      "Accuracy of keyboard : 43 %\n",
      "Accuracy of  lamp : 30 %\n",
      "Accuracy of lawn_mower : 67 %\n",
      "Accuracy of leopard : 43 %\n",
      "Accuracy of  lion : 45 %\n",
      "Accuracy of lizard :  9 %\n",
      "Accuracy of lobster : 12 %\n",
      "Accuracy of   man : 17 %\n",
      "Accuracy of maple_tree : 52 %\n",
      "Accuracy of motorcycle : 71 %\n",
      "Accuracy of mountain : 44 %\n",
      "Accuracy of mouse :  6 %\n",
      "Accuracy of mushroom : 28 %\n",
      "Accuracy of oak_tree : 63 %\n",
      "Accuracy of orange : 62 %\n",
      "Accuracy of orchid : 43 %\n",
      "Accuracy of otter :  4 %\n",
      "Accuracy of palm_tree : 53 %\n",
      "Accuracy of  pear : 37 %\n",
      "Accuracy of pickup_truck : 36 %\n",
      "Accuracy of pine_tree : 25 %\n",
      "Accuracy of plain : 64 %\n",
      "Accuracy of plate : 36 %\n",
      "Accuracy of poppy : 43 %\n",
      "Accuracy of porcupine : 15 %\n",
      "Accuracy of possum :  3 %\n",
      "Accuracy of rabbit :  0 %\n",
      "Accuracy of raccoon : 11 %\n",
      "Accuracy of   ray : 21 %\n",
      "Accuracy of  road : 60 %\n",
      "Accuracy of rocket : 40 %\n",
      "Accuracy of  rose : 19 %\n",
      "Accuracy of   sea : 45 %\n",
      "Accuracy of  seal :  4 %\n",
      "Accuracy of shark : 18 %\n",
      "Accuracy of shrew :  9 %\n",
      "Accuracy of skunk : 39 %\n",
      "Accuracy of skyscraper : 49 %\n",
      "Accuracy of snail :  5 %\n",
      "Accuracy of snake :  1 %\n",
      "Accuracy of spider : 15 %\n",
      "Accuracy of squirrel :  6 %\n",
      "Accuracy of streetcar : 11 %\n",
      "Accuracy of sunflower : 57 %\n",
      "Accuracy of sweet_pepper : 10 %\n",
      "Accuracy of table :  7 %\n",
      "Accuracy of  tank : 23 %\n",
      "Accuracy of telephone :  6 %\n",
      "Accuracy of television : 22 %\n",
      "Accuracy of tiger :  7 %\n",
      "Accuracy of tractor :  6 %\n",
      "Accuracy of train :  3 %\n",
      "Accuracy of trout : 14 %\n",
      "Accuracy of tulip :  7 %\n",
      "Accuracy of turtle :  4 %\n",
      "Accuracy of wardrobe : 53 %\n",
      "Accuracy of whale : 26 %\n",
      "Accuracy of willow_tree : 19 %\n",
      "Accuracy of  wolf : 13 %\n",
      "Accuracy of woman :  7 %\n",
      "Accuracy of  worm : 15 %\n"
     ]
    }
   ],
   "source": [
    "train_dataset, train_loader, test_loader, num_classes = get_loaders()\n",
    "model = ResNet18(num_classes)\n",
    "model = model.cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=150, gamma=0.1)\n",
    "\n",
    "# Let's build our model\n",
    "train(EPOCHS)\n",
    "print('Finished Training')\n",
    "test() \n",
    "print('----------------------------')\n",
    "print('get the top-1 accuracy for each of the classes.')\n",
    "avg_accu = testEeahClass()\n",
    "\n",
    "print(tau_list[5])\n",
    "n2_norm = torch.norm(model.classifier.weight, p=2.0, dim=1)\n",
    "n2_norm *= n2_norm**tau_list[5]\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(num_classes):\n",
    "        model.classifier.weight[i] = model.classifier.weight[i].div_(n2_norm[i])\n",
    "        model.classifier.bias[i] = model.classifier.bias[i].div_(float(\"inf\"))\n",
    "\n",
    "test()\n",
    "avg_accu = testEeahClass()\n",
    "\n",
    "\n",
    "torch.save({\n",
    "    'model': model.state_dict(),\n",
    "    'optimizer': optimizer.state_dict(),\n",
    "    'epoch': EPOCHS},\n",
    "    osp.join(SAVE_DIR, 'tau-ep{:03d}.pth'.format(EPOCHS))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "e0bd2498-4466-46cc-8fb9-0a4f90a83056",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-22T21:34:20.165188Z",
     "iopub.status.busy": "2022-06-22T21:34:20.164746Z",
     "iopub.status.idle": "2022-06-22T21:34:20.198538Z",
     "shell.execute_reply": "2022-06-22T21:34:20.197448Z",
     "shell.execute_reply.started": "2022-06-22T21:34:20.165129Z"
    },
    "id": "e0bd2498-4466-46cc-8fb9-0a4f90a83056",
    "tags": []
   },
   "outputs": [],
   "source": [
    " \n",
    "\n",
    "DATASET = 'CIFAR100' #['CIFAR10', 'CIFAR100']\n",
    "IMB_TYPE = 'step' #['exp', 'step']\n",
    "IMB_FACTOR = 0.1 #[0.1, 0.01]\n",
    "SAVE_DIR = 'logs/100-step-0.1' \n",
    "\n",
    "LR = 0.1\n",
    "BATCH_SIZE = 128\n",
    "MOMENTUM = 0.9\n",
    "WEIGHT_DECAY = 0.000479\n",
    "EPOCHS = 200\n",
    "\n",
    "os.makedirs(osp.join(SAVE_DIR), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "0e86a3ef-507f-4d52-b863-501818e51acc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-22T21:34:20.202889Z",
     "iopub.status.busy": "2022-06-22T21:34:20.202492Z",
     "iopub.status.idle": "2022-06-22T22:10:53.800774Z",
     "shell.execute_reply": "2022-06-22T22:10:53.799453Z",
     "shell.execute_reply.started": "2022-06-22T21:34:20.202841Z"
    },
    "id": "0e86a3ef-507f-4d52-b863-501818e51acc",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "cls num list:\n",
      "[500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50]\n",
      "Epoch: [001] \t Loss 3.9900 \t Acc 15.70 \t AccHead 17.27 \t AccTail 0.00\n",
      "Epoch: [002] \t Loss 3.3789 \t Acc 22.13 \t AccHead 24.31 \t AccTail 0.44\n",
      "Epoch: [003] \t Loss 3.1052 \t Acc 24.58 \t AccHead 27.01 \t AccTail 0.24\n",
      "Epoch: [004] \t Loss 2.9250 \t Acc 26.19 \t AccHead 28.76 \t AccTail 0.52\n",
      "Epoch: [005] \t Loss 2.7731 \t Acc 32.07 \t AccHead 35.11 \t AccTail 1.65\n",
      "Epoch: [006] \t Loss 2.6449 \t Acc 31.82 \t AccHead 34.75 \t AccTail 2.57\n",
      "Epoch: [007] \t Loss 2.5182 \t Acc 32.07 \t AccHead 34.80 \t AccTail 4.77\n",
      "Epoch: [008] \t Loss 2.3992 \t Acc 38.11 \t AccHead 41.38 \t AccTail 5.46\n",
      "Epoch: [009] \t Loss 2.3156 \t Acc 40.74 \t AccHead 44.22 \t AccTail 5.98\n",
      "Epoch: [010] \t Loss 2.2423 \t Acc 41.22 \t AccHead 44.76 \t AccTail 5.82\n",
      "Epoch: [011] \t Loss 2.1667 \t Acc 43.01 \t AccHead 46.51 \t AccTail 8.04\n",
      "Epoch: [012] \t Loss 2.1060 \t Acc 43.19 \t AccHead 46.64 \t AccTail 8.78\n",
      "Epoch: [013] \t Loss 2.0585 \t Acc 45.17 \t AccHead 48.74 \t AccTail 9.47\n",
      "Epoch: [014] \t Loss 2.0079 \t Acc 44.66 \t AccHead 48.31 \t AccTail 8.09\n",
      "Epoch: [015] \t Loss 1.9619 \t Acc 47.25 \t AccHead 50.79 \t AccTail 11.91\n",
      "Epoch: [016] \t Loss 1.9276 \t Acc 48.93 \t AccHead 52.37 \t AccTail 14.45\n",
      "Epoch: [017] \t Loss 1.8880 \t Acc 48.58 \t AccHead 52.05 \t AccTail 13.87\n",
      "Epoch: [018] \t Loss 1.8499 \t Acc 47.70 \t AccHead 51.10 \t AccTail 13.76\n",
      "Epoch: [019] \t Loss 1.8214 \t Acc 51.66 \t AccHead 55.28 \t AccTail 15.45\n",
      "Epoch: [020] \t Loss 1.8105 \t Acc 51.14 \t AccHead 54.79 \t AccTail 14.64\n",
      "Epoch: [021] \t Loss 1.7787 \t Acc 50.52 \t AccHead 54.31 \t AccTail 12.61\n",
      "Epoch: [022] \t Loss 1.7590 \t Acc 54.52 \t AccHead 58.09 \t AccTail 18.76\n",
      "Epoch: [023] \t Loss 1.7248 \t Acc 51.11 \t AccHead 54.45 \t AccTail 17.79\n",
      "Epoch: [024] \t Loss 1.7004 \t Acc 55.45 \t AccHead 59.04 \t AccTail 19.70\n",
      "Epoch: [025] \t Loss 1.6769 \t Acc 53.29 \t AccHead 56.88 \t AccTail 17.40\n",
      "Epoch: [026] \t Loss 1.6487 \t Acc 53.06 \t AccHead 56.97 \t AccTail 14.03\n",
      "Epoch: [027] \t Loss 1.6478 \t Acc 53.55 \t AccHead 57.25 \t AccTail 16.57\n",
      "Epoch: [028] \t Loss 1.6247 \t Acc 53.91 \t AccHead 57.10 \t AccTail 22.00\n",
      "Epoch: [029] \t Loss 1.6048 \t Acc 55.83 \t AccHead 59.52 \t AccTail 18.95\n",
      "Epoch: [030] \t Loss 1.5936 \t Acc 53.38 \t AccHead 56.80 \t AccTail 19.10\n",
      "Epoch: [031] \t Loss 1.5854 \t Acc 56.63 \t AccHead 60.27 \t AccTail 20.17\n",
      "Epoch: [032] \t Loss 1.5638 \t Acc 56.70 \t AccHead 59.93 \t AccTail 24.40\n",
      "Epoch: [033] \t Loss 1.5547 \t Acc 55.56 \t AccHead 59.17 \t AccTail 19.54\n",
      "Epoch: [034] \t Loss 1.5350 \t Acc 57.13 \t AccHead 60.71 \t AccTail 21.33\n",
      "Epoch: [035] \t Loss 1.5071 \t Acc 55.37 \t AccHead 58.42 \t AccTail 24.87\n",
      "Epoch: [036] \t Loss 1.5098 \t Acc 57.34 \t AccHead 60.97 \t AccTail 21.13\n",
      "Epoch: [037] \t Loss 1.5088 \t Acc 56.83 \t AccHead 60.04 \t AccTail 24.66\n",
      "Epoch: [038] \t Loss 1.4813 \t Acc 60.04 \t AccHead 63.72 \t AccTail 23.26\n",
      "Epoch: [039] \t Loss 1.4777 \t Acc 60.32 \t AccHead 63.85 \t AccTail 25.02\n",
      "Epoch: [040] \t Loss 1.4576 \t Acc 57.71 \t AccHead 61.18 \t AccTail 22.98\n",
      "Epoch: [041] \t Loss 1.4400 \t Acc 58.92 \t AccHead 62.46 \t AccTail 23.42\n",
      "Epoch: [042] \t Loss 1.4535 \t Acc 57.84 \t AccHead 61.34 \t AccTail 22.91\n",
      "Epoch: [043] \t Loss 1.4487 \t Acc 59.40 \t AccHead 62.78 \t AccTail 25.61\n",
      "Epoch: [044] \t Loss 1.4112 \t Acc 56.11 \t AccHead 59.21 \t AccTail 25.22\n",
      "Epoch: [045] \t Loss 1.4041 \t Acc 57.96 \t AccHead 61.42 \t AccTail 23.33\n",
      "Epoch: [046] \t Loss 1.4141 \t Acc 60.22 \t AccHead 63.87 \t AccTail 23.85\n",
      "Epoch: [047] \t Loss 1.3914 \t Acc 61.73 \t AccHead 64.94 \t AccTail 29.63\n",
      "Epoch: [048] \t Loss 1.3813 \t Acc 61.94 \t AccHead 65.35 \t AccTail 27.74\n",
      "Epoch: [049] \t Loss 1.3835 \t Acc 62.12 \t AccHead 65.23 \t AccTail 30.98\n",
      "Epoch: [050] \t Loss 1.3719 \t Acc 62.36 \t AccHead 65.74 \t AccTail 28.48\n",
      "Epoch: [051] \t Loss 1.3784 \t Acc 57.97 \t AccHead 61.17 \t AccTail 26.04\n",
      "Epoch: [052] \t Loss 1.3522 \t Acc 59.03 \t AccHead 62.04 \t AccTail 28.93\n",
      "Epoch: [053] \t Loss 1.3502 \t Acc 62.24 \t AccHead 65.39 \t AccTail 30.71\n",
      "Epoch: [054] \t Loss 1.3439 \t Acc 61.56 \t AccHead 64.87 \t AccTail 28.42\n",
      "Epoch: [055] \t Loss 1.3309 \t Acc 60.43 \t AccHead 63.66 \t AccTail 28.08\n",
      "Epoch: [056] \t Loss 1.3129 \t Acc 63.13 \t AccHead 66.23 \t AccTail 32.06\n",
      "Epoch: [057] \t Loss 1.3161 \t Acc 62.52 \t AccHead 65.37 \t AccTail 34.07\n",
      "Epoch: [058] \t Loss 1.3211 \t Acc 63.39 \t AccHead 66.30 \t AccTail 34.20\n",
      "Epoch: [059] \t Loss 1.3252 \t Acc 61.16 \t AccHead 64.70 \t AccTail 25.70\n",
      "Epoch: [060] \t Loss 1.3037 \t Acc 62.03 \t AccHead 65.37 \t AccTail 28.57\n",
      "Epoch: [061] \t Loss 1.3017 \t Acc 65.48 \t AccHead 69.01 \t AccTail 30.20\n",
      "Epoch: [062] \t Loss 1.2858 \t Acc 63.23 \t AccHead 66.13 \t AccTail 34.30\n",
      "Epoch: [063] \t Loss 1.2717 \t Acc 63.55 \t AccHead 66.36 \t AccTail 35.39\n",
      "Epoch: [064] \t Loss 1.2800 \t Acc 62.05 \t AccHead 65.01 \t AccTail 32.44\n",
      "Epoch: [065] \t Loss 1.2808 \t Acc 63.76 \t AccHead 66.94 \t AccTail 31.90\n",
      "Epoch: [066] \t Loss 1.2741 \t Acc 63.06 \t AccHead 66.30 \t AccTail 30.61\n",
      "Epoch: [067] \t Loss 1.2525 \t Acc 64.26 \t AccHead 67.11 \t AccTail 35.81\n",
      "Epoch: [068] \t Loss 1.2718 \t Acc 58.09 \t AccHead 60.88 \t AccTail 30.16\n",
      "Epoch: [069] \t Loss 1.2516 \t Acc 60.24 \t AccHead 63.11 \t AccTail 31.52\n",
      "Epoch: [070] \t Loss 1.2514 \t Acc 63.46 \t AccHead 66.33 \t AccTail 34.71\n",
      "Epoch: [071] \t Loss 1.2511 \t Acc 61.57 \t AccHead 64.43 \t AccTail 32.93\n",
      "Epoch: [072] \t Loss 1.2394 \t Acc 62.42 \t AccHead 65.11 \t AccTail 35.48\n",
      "Epoch: [073] \t Loss 1.2279 \t Acc 61.84 \t AccHead 65.19 \t AccTail 28.35\n",
      "Epoch: [074] \t Loss 1.2256 \t Acc 62.23 \t AccHead 64.78 \t AccTail 36.85\n",
      "Epoch: [075] \t Loss 1.2374 \t Acc 65.52 \t AccHead 68.54 \t AccTail 35.29\n",
      "Epoch: [076] \t Loss 1.2100 \t Acc 64.80 \t AccHead 67.17 \t AccTail 41.12\n",
      "Epoch: [077] \t Loss 1.2268 \t Acc 65.55 \t AccHead 68.60 \t AccTail 35.06\n",
      "Epoch: [078] \t Loss 1.2193 \t Acc 64.27 \t AccHead 67.17 \t AccTail 35.27\n",
      "Epoch: [079] \t Loss 1.1990 \t Acc 65.83 \t AccHead 68.50 \t AccTail 39.08\n",
      "Epoch: [080] \t Loss 1.2003 \t Acc 63.35 \t AccHead 66.28 \t AccTail 34.14\n",
      "Epoch: [081] \t Loss 1.2047 \t Acc 65.13 \t AccHead 67.78 \t AccTail 38.62\n",
      "Epoch: [082] \t Loss 1.1989 \t Acc 63.37 \t AccHead 66.10 \t AccTail 36.10\n",
      "Epoch: [083] \t Loss 1.1914 \t Acc 66.03 \t AccHead 68.92 \t AccTail 37.21\n",
      "Epoch: [084] \t Loss 1.1827 \t Acc 66.18 \t AccHead 68.60 \t AccTail 41.95\n",
      "Epoch: [085] \t Loss 1.1836 \t Acc 66.64 \t AccHead 69.25 \t AccTail 40.60\n",
      "Epoch: [086] \t Loss 1.1753 \t Acc 63.83 \t AccHead 66.40 \t AccTail 38.12\n",
      "Epoch: [087] \t Loss 1.1693 \t Acc 66.13 \t AccHead 68.79 \t AccTail 39.46\n",
      "Epoch: [088] \t Loss 1.1692 \t Acc 64.30 \t AccHead 66.78 \t AccTail 39.47\n",
      "Epoch: [089] \t Loss 1.1706 \t Acc 63.05 \t AccHead 65.81 \t AccTail 35.42\n",
      "Epoch: [090] \t Loss 1.1751 \t Acc 65.51 \t AccHead 68.58 \t AccTail 34.78\n",
      "Epoch: [091] \t Loss 1.1712 \t Acc 65.97 \t AccHead 68.79 \t AccTail 37.70\n",
      "Epoch: [092] \t Loss 1.1762 \t Acc 68.62 \t AccHead 71.22 \t AccTail 42.63\n",
      "Epoch: [093] \t Loss 1.1547 \t Acc 64.97 \t AccHead 67.65 \t AccTail 38.15\n",
      "Epoch: [094] \t Loss 1.1583 \t Acc 67.48 \t AccHead 69.48 \t AccTail 47.41\n",
      "Epoch: [095] \t Loss 1.1458 \t Acc 66.14 \t AccHead 69.25 \t AccTail 35.02\n",
      "Epoch: [096] \t Loss 1.1459 \t Acc 63.82 \t AccHead 66.43 \t AccTail 37.72\n",
      "Epoch: [097] \t Loss 1.1451 \t Acc 67.21 \t AccHead 69.56 \t AccTail 43.73\n",
      "Epoch: [098] \t Loss 1.1469 \t Acc 67.22 \t AccHead 69.75 \t AccTail 41.89\n",
      "Epoch: [099] \t Loss 1.1408 \t Acc 65.93 \t AccHead 68.67 \t AccTail 38.59\n",
      "Epoch: [100] \t Loss 1.1340 \t Acc 63.96 \t AccHead 66.18 \t AccTail 41.73\n",
      "Epoch: [101] \t Loss 1.1405 \t Acc 67.35 \t AccHead 69.92 \t AccTail 41.68\n",
      "Epoch: [102] \t Loss 1.1373 \t Acc 67.64 \t AccHead 70.45 \t AccTail 39.43\n",
      "Epoch: [103] \t Loss 1.1308 \t Acc 67.45 \t AccHead 69.85 \t AccTail 43.40\n",
      "Epoch: [104] \t Loss 1.1250 \t Acc 67.55 \t AccHead 70.18 \t AccTail 41.21\n",
      "Epoch: [105] \t Loss 1.1082 \t Acc 63.53 \t AccHead 65.90 \t AccTail 39.78\n",
      "Epoch: [106] \t Loss 1.1188 \t Acc 64.09 \t AccHead 66.68 \t AccTail 38.25\n",
      "Epoch: [107] \t Loss 1.1285 \t Acc 66.37 \t AccHead 68.85 \t AccTail 41.59\n",
      "Epoch: [108] \t Loss 1.1258 \t Acc 66.99 \t AccHead 69.09 \t AccTail 46.00\n",
      "Epoch: [109] \t Loss 1.1327 \t Acc 66.23 \t AccHead 69.26 \t AccTail 35.84\n",
      "Epoch: [110] \t Loss 1.1137 \t Acc 68.80 \t AccHead 71.17 \t AccTail 45.11\n",
      "Epoch: [111] \t Loss 1.0985 \t Acc 67.64 \t AccHead 70.33 \t AccTail 40.79\n",
      "Epoch: [112] \t Loss 1.1059 \t Acc 67.29 \t AccHead 69.65 \t AccTail 43.80\n",
      "Epoch: [113] \t Loss 1.0965 \t Acc 70.19 \t AccHead 72.68 \t AccTail 45.23\n",
      "Epoch: [114] \t Loss 1.1112 \t Acc 69.07 \t AccHead 71.53 \t AccTail 44.48\n",
      "Epoch: [115] \t Loss 1.1019 \t Acc 65.53 \t AccHead 68.27 \t AccTail 38.14\n",
      "Epoch: [116] \t Loss 1.0885 \t Acc 68.94 \t AccHead 71.73 \t AccTail 41.14\n",
      "Epoch: [117] \t Loss 1.0894 \t Acc 67.74 \t AccHead 70.73 \t AccTail 37.81\n",
      "Epoch: [118] \t Loss 1.0986 \t Acc 68.59 \t AccHead 70.66 \t AccTail 47.87\n",
      "Epoch: [119] \t Loss 1.0918 \t Acc 65.79 \t AccHead 68.51 \t AccTail 38.58\n",
      "Epoch: [120] \t Loss 1.0998 \t Acc 59.27 \t AccHead 61.80 \t AccTail 33.92\n",
      "Epoch: [121] \t Loss 1.0946 \t Acc 66.27 \t AccHead 68.68 \t AccTail 42.23\n",
      "Epoch: [122] \t Loss 1.0952 \t Acc 69.06 \t AccHead 71.29 \t AccTail 46.81\n",
      "Epoch: [123] \t Loss 1.0951 \t Acc 67.22 \t AccHead 69.58 \t AccTail 43.57\n",
      "Epoch: [124] \t Loss 1.0659 \t Acc 69.92 \t AccHead 72.30 \t AccTail 46.12\n",
      "Epoch: [125] \t Loss 1.0769 \t Acc 62.90 \t AccHead 65.18 \t AccTail 40.10\n",
      "Epoch: [126] \t Loss 1.0690 \t Acc 68.26 \t AccHead 70.39 \t AccTail 47.01\n",
      "Epoch: [127] \t Loss 1.0838 \t Acc 68.47 \t AccHead 70.73 \t AccTail 45.92\n",
      "Epoch: [128] \t Loss 1.0715 \t Acc 66.86 \t AccHead 69.27 \t AccTail 42.75\n",
      "Epoch: [129] \t Loss 1.0683 \t Acc 70.92 \t AccHead 73.46 \t AccTail 45.57\n",
      "Epoch: [130] \t Loss 1.0615 \t Acc 67.17 \t AccHead 69.48 \t AccTail 44.02\n",
      "Epoch: [131] \t Loss 1.0704 \t Acc 68.03 \t AccHead 70.10 \t AccTail 47.37\n",
      "Epoch: [132] \t Loss 1.0728 \t Acc 69.79 \t AccHead 71.89 \t AccTail 48.77\n",
      "Epoch: [133] \t Loss 1.0585 \t Acc 69.49 \t AccHead 72.03 \t AccTail 44.02\n",
      "Epoch: [134] \t Loss 1.0704 \t Acc 68.49 \t AccHead 70.81 \t AccTail 45.34\n",
      "Epoch: [135] \t Loss 1.0621 \t Acc 68.25 \t AccHead 70.66 \t AccTail 44.15\n",
      "Epoch: [136] \t Loss 1.0639 \t Acc 68.41 \t AccHead 70.54 \t AccTail 47.15\n",
      "Epoch: [137] \t Loss 1.0758 \t Acc 65.08 \t AccHead 67.09 \t AccTail 45.02\n",
      "Epoch: [138] \t Loss 1.0528 \t Acc 66.37 \t AccHead 68.65 \t AccTail 43.51\n",
      "Epoch: [139] \t Loss 1.0491 \t Acc 68.73 \t AccHead 71.26 \t AccTail 43.44\n",
      "Epoch: [140] \t Loss 1.0359 \t Acc 69.61 \t AccHead 71.58 \t AccTail 49.92\n",
      "Epoch: [141] \t Loss 1.0630 \t Acc 68.18 \t AccHead 70.39 \t AccTail 46.10\n",
      "Epoch: [142] \t Loss 1.0468 \t Acc 71.29 \t AccHead 73.36 \t AccTail 50.66\n",
      "Epoch: [143] \t Loss 1.0598 \t Acc 69.34 \t AccHead 71.82 \t AccTail 44.63\n",
      "Epoch: [144] \t Loss 1.0400 \t Acc 70.16 \t AccHead 72.54 \t AccTail 46.43\n",
      "Epoch: [145] \t Loss 1.0470 \t Acc 66.01 \t AccHead 68.40 \t AccTail 42.07\n",
      "Epoch: [146] \t Loss 1.0606 \t Acc 68.98 \t AccHead 71.48 \t AccTail 44.03\n",
      "Epoch: [147] \t Loss 1.0372 \t Acc 69.49 \t AccHead 71.34 \t AccTail 51.04\n",
      "Epoch: [148] \t Loss 1.0573 \t Acc 69.18 \t AccHead 71.09 \t AccTail 50.02\n",
      "Epoch: [149] \t Loss 1.0378 \t Acc 68.19 \t AccHead 70.53 \t AccTail 44.83\n",
      "Epoch: [150] \t Loss 1.0461 \t Acc 61.24 \t AccHead 63.47 \t AccTail 38.84\n",
      "Epoch: [151] \t Loss 0.5940 \t Acc 89.89 \t AccHead 91.24 \t AccTail 76.48\n",
      "Epoch: [152] \t Loss 0.3831 \t Acc 93.18 \t AccHead 93.97 \t AccTail 85.21\n",
      "Epoch: [153] \t Loss 0.3097 \t Acc 94.49 \t AccHead 95.20 \t AccTail 87.43\n",
      "Epoch: [154] \t Loss 0.2556 \t Acc 95.43 \t AccHead 95.93 \t AccTail 90.38\n",
      "Epoch: [155] \t Loss 0.2168 \t Acc 96.46 \t AccHead 96.87 \t AccTail 92.30\n",
      "Epoch: [156] \t Loss 0.1904 \t Acc 96.93 \t AccHead 97.35 \t AccTail 92.68\n",
      "Epoch: [157] \t Loss 0.1675 \t Acc 97.55 \t AccHead 97.89 \t AccTail 94.22\n",
      "Epoch: [158] \t Loss 0.1490 \t Acc 97.87 \t AccHead 98.16 \t AccTail 94.97\n",
      "Epoch: [159] \t Loss 0.1322 \t Acc 98.05 \t AccHead 98.29 \t AccTail 95.70\n",
      "Epoch: [160] \t Loss 0.1204 \t Acc 98.55 \t AccHead 98.73 \t AccTail 96.83\n",
      "Epoch: [161] \t Loss 0.1084 \t Acc 98.71 \t AccHead 98.80 \t AccTail 97.87\n",
      "Epoch: [162] \t Loss 0.0985 \t Acc 98.80 \t AccHead 98.92 \t AccTail 97.63\n",
      "Epoch: [163] \t Loss 0.0905 \t Acc 98.97 \t AccHead 99.10 \t AccTail 97.67\n",
      "Epoch: [164] \t Loss 0.0836 \t Acc 99.14 \t AccHead 99.20 \t AccTail 98.51\n",
      "Epoch: [165] \t Loss 0.0783 \t Acc 99.11 \t AccHead 99.18 \t AccTail 98.39\n",
      "Epoch: [166] \t Loss 0.0728 \t Acc 99.22 \t AccHead 99.31 \t AccTail 98.27\n",
      "Epoch: [167] \t Loss 0.0711 \t Acc 99.38 \t AccHead 99.41 \t AccTail 99.08\n",
      "Epoch: [168] \t Loss 0.0655 \t Acc 99.38 \t AccHead 99.46 \t AccTail 98.59\n",
      "Epoch: [169] \t Loss 0.0611 \t Acc 99.40 \t AccHead 99.44 \t AccTail 99.03\n",
      "Epoch: [170] \t Loss 0.0591 \t Acc 99.40 \t AccHead 99.42 \t AccTail 99.24\n",
      "Epoch: [171] \t Loss 0.0537 \t Acc 99.51 \t AccHead 99.52 \t AccTail 99.40\n",
      "Epoch: [172] \t Loss 0.0516 \t Acc 99.47 \t AccHead 99.53 \t AccTail 98.92\n",
      "Epoch: [173] \t Loss 0.0517 \t Acc 99.53 \t AccHead 99.56 \t AccTail 99.20\n",
      "Epoch: [174] \t Loss 0.0475 \t Acc 99.66 \t AccHead 99.68 \t AccTail 99.48\n",
      "Epoch: [175] \t Loss 0.0465 \t Acc 99.55 \t AccHead 99.56 \t AccTail 99.52\n",
      "Epoch: [176] \t Loss 0.0428 \t Acc 99.66 \t AccHead 99.67 \t AccTail 99.56\n",
      "Epoch: [177] \t Loss 0.0439 \t Acc 99.64 \t AccHead 99.65 \t AccTail 99.52\n",
      "Epoch: [178] \t Loss 0.0428 \t Acc 99.63 \t AccHead 99.62 \t AccTail 99.76\n",
      "Epoch: [179] \t Loss 0.0401 \t Acc 99.63 \t AccHead 99.62 \t AccTail 99.68\n",
      "Epoch: [180] \t Loss 0.0390 \t Acc 99.63 \t AccHead 99.64 \t AccTail 99.56\n",
      "Epoch: [181] \t Loss 0.0396 \t Acc 99.70 \t AccHead 99.71 \t AccTail 99.60\n",
      "Epoch: [182] \t Loss 0.0361 \t Acc 99.73 \t AccHead 99.73 \t AccTail 99.72\n",
      "Epoch: [183] \t Loss 0.0377 \t Acc 99.70 \t AccHead 99.71 \t AccTail 99.60\n",
      "Epoch: [184] \t Loss 0.0366 \t Acc 99.73 \t AccHead 99.73 \t AccTail 99.76\n",
      "Epoch: [185] \t Loss 0.0343 \t Acc 99.73 \t AccHead 99.75 \t AccTail 99.48\n",
      "Epoch: [186] \t Loss 0.0360 \t Acc 99.59 \t AccHead 99.59 \t AccTail 99.60\n",
      "Epoch: [187] \t Loss 0.0353 \t Acc 99.73 \t AccHead 99.73 \t AccTail 99.80\n",
      "Epoch: [188] \t Loss 0.0333 \t Acc 99.70 \t AccHead 99.77 \t AccTail 98.95\n",
      "Epoch: [189] \t Loss 0.0321 \t Acc 99.76 \t AccHead 99.79 \t AccTail 99.40\n",
      "Epoch: [190] \t Loss 0.0344 \t Acc 99.71 \t AccHead 99.69 \t AccTail 99.88\n",
      "Epoch: [191] \t Loss 0.0342 \t Acc 99.76 \t AccHead 99.75 \t AccTail 99.80\n",
      "Epoch: [192] \t Loss 0.0345 \t Acc 99.69 \t AccHead 99.71 \t AccTail 99.48\n",
      "Epoch: [193] \t Loss 0.0359 \t Acc 99.74 \t AccHead 99.74 \t AccTail 99.68\n",
      "Epoch: [194] \t Loss 0.0329 \t Acc 99.72 \t AccHead 99.72 \t AccTail 99.72\n",
      "Epoch: [195] \t Loss 0.0333 \t Acc 99.72 \t AccHead 99.73 \t AccTail 99.60\n",
      "Epoch: [196] \t Loss 0.0367 \t Acc 99.66 \t AccHead 99.67 \t AccTail 99.60\n",
      "Epoch: [197] \t Loss 0.0323 \t Acc 99.60 \t AccHead 99.57 \t AccTail 99.84\n",
      "Epoch: [198] \t Loss 0.0366 \t Acc 99.65 \t AccHead 99.64 \t AccTail 99.72\n",
      "Epoch: [199] \t Loss 0.0339 \t Acc 99.64 \t AccHead 99.63 \t AccTail 99.68\n",
      "Epoch: [200] \t Loss 0.0379 \t Acc 99.66 \t AccHead 99.66 \t AccTail 99.72\n",
      "199\n",
      "Finished Training\n",
      "Acc 43.56 \t AccHead 63.20 \t AccTail 23.92\n",
      "----------------------------\n",
      "get the top-1 accuracy for each of the classes.\n",
      "Accuracy of apple : 87 %\n",
      "Accuracy of aquarium_fish : 77 %\n",
      "Accuracy of  baby : 48 %\n",
      "Accuracy of  bear : 46 %\n",
      "Accuracy of beaver : 53 %\n",
      "Accuracy of   bed : 64 %\n",
      "Accuracy of   bee : 62 %\n",
      "Accuracy of beetle : 65 %\n",
      "Accuracy of bicycle : 72 %\n",
      "Accuracy of bottle : 72 %\n",
      "Accuracy of  bowl : 50 %\n",
      "Accuracy of   boy : 45 %\n",
      "Accuracy of bridge : 60 %\n",
      "Accuracy of   bus : 68 %\n",
      "Accuracy of butterfly : 70 %\n",
      "Accuracy of camel : 53 %\n",
      "Accuracy of   can : 67 %\n",
      "Accuracy of castle : 79 %\n",
      "Accuracy of caterpillar : 49 %\n",
      "Accuracy of cattle : 51 %\n",
      "Accuracy of chair : 83 %\n",
      "Accuracy of chimpanzee : 77 %\n",
      "Accuracy of clock : 59 %\n",
      "Accuracy of cloud : 83 %\n",
      "Accuracy of cockroach : 85 %\n",
      "Accuracy of couch : 51 %\n",
      "Accuracy of  crab : 59 %\n",
      "Accuracy of crocodile : 40 %\n",
      "Accuracy of   cup : 68 %\n",
      "Accuracy of dinosaur : 53 %\n",
      "Accuracy of dolphin : 83 %\n",
      "Accuracy of elephant : 55 %\n",
      "Accuracy of flatfish : 58 %\n",
      "Accuracy of forest : 62 %\n",
      "Accuracy of   fox : 70 %\n",
      "Accuracy of  girl : 37 %\n",
      "Accuracy of hamster : 57 %\n",
      "Accuracy of house : 59 %\n",
      "Accuracy of kangaroo : 54 %\n",
      "Accuracy of keyboard : 75 %\n",
      "Accuracy of  lamp : 55 %\n",
      "Accuracy of lawn_mower : 79 %\n",
      "Accuracy of leopard : 64 %\n",
      "Accuracy of  lion : 70 %\n",
      "Accuracy of lizard : 42 %\n",
      "Accuracy of lobster : 45 %\n",
      "Accuracy of   man : 40 %\n",
      "Accuracy of maple_tree : 83 %\n",
      "Accuracy of motorcycle : 91 %\n",
      "Accuracy of mountain : 85 %\n",
      "Accuracy of mouse :  5 %\n",
      "Accuracy of mushroom : 21 %\n",
      "Accuracy of oak_tree : 25 %\n",
      "Accuracy of orange : 57 %\n",
      "Accuracy of orchid : 45 %\n",
      "Accuracy of otter :  3 %\n",
      "Accuracy of palm_tree : 45 %\n",
      "Accuracy of  pear : 31 %\n",
      "Accuracy of pickup_truck : 27 %\n",
      "Accuracy of pine_tree : 25 %\n",
      "Accuracy of plain : 57 %\n",
      "Accuracy of plate : 18 %\n",
      "Accuracy of poppy : 37 %\n",
      "Accuracy of porcupine : 16 %\n",
      "Accuracy of possum :  3 %\n",
      "Accuracy of rabbit :  8 %\n",
      "Accuracy of raccoon : 16 %\n",
      "Accuracy of   ray : 13 %\n",
      "Accuracy of  road : 62 %\n",
      "Accuracy of rocket : 40 %\n",
      "Accuracy of  rose : 39 %\n",
      "Accuracy of   sea : 29 %\n",
      "Accuracy of  seal :  6 %\n",
      "Accuracy of shark : 14 %\n",
      "Accuracy of shrew :  9 %\n",
      "Accuracy of skunk : 37 %\n",
      "Accuracy of skyscraper : 40 %\n",
      "Accuracy of snail : 10 %\n",
      "Accuracy of snake :  5 %\n",
      "Accuracy of spider : 24 %\n",
      "Accuracy of squirrel :  4 %\n",
      "Accuracy of streetcar : 13 %\n",
      "Accuracy of sunflower : 61 %\n",
      "Accuracy of sweet_pepper : 13 %\n",
      "Accuracy of table : 12 %\n",
      "Accuracy of  tank : 35 %\n",
      "Accuracy of telephone : 19 %\n",
      "Accuracy of television : 32 %\n",
      "Accuracy of tiger : 19 %\n",
      "Accuracy of tractor : 16 %\n",
      "Accuracy of train : 24 %\n",
      "Accuracy of trout : 31 %\n",
      "Accuracy of tulip : 19 %\n",
      "Accuracy of turtle :  5 %\n",
      "Accuracy of wardrobe : 58 %\n",
      "Accuracy of whale : 20 %\n",
      "Accuracy of willow_tree : 14 %\n",
      "Accuracy of  wolf : 13 %\n",
      "Accuracy of woman :  0 %\n",
      "Accuracy of  worm : 21 %\n",
      "0.1106\n",
      "Acc 47.81 \t AccHead 57.86 \t AccTail 37.76\n",
      "Accuracy of apple : 85 %\n",
      "Accuracy of aquarium_fish : 76 %\n",
      "Accuracy of  baby : 41 %\n",
      "Accuracy of  bear : 31 %\n",
      "Accuracy of beaver : 42 %\n",
      "Accuracy of   bed : 64 %\n",
      "Accuracy of   bee : 56 %\n",
      "Accuracy of beetle : 59 %\n",
      "Accuracy of bicycle : 72 %\n",
      "Accuracy of bottle : 70 %\n",
      "Accuracy of  bowl : 34 %\n",
      "Accuracy of   boy : 43 %\n",
      "Accuracy of bridge : 48 %\n",
      "Accuracy of   bus : 57 %\n",
      "Accuracy of butterfly : 57 %\n",
      "Accuracy of camel : 50 %\n",
      "Accuracy of   can : 61 %\n",
      "Accuracy of castle : 78 %\n",
      "Accuracy of caterpillar : 41 %\n",
      "Accuracy of cattle : 46 %\n",
      "Accuracy of chair : 85 %\n",
      "Accuracy of chimpanzee : 79 %\n",
      "Accuracy of clock : 53 %\n",
      "Accuracy of cloud : 79 %\n",
      "Accuracy of cockroach : 84 %\n",
      "Accuracy of couch : 43 %\n",
      "Accuracy of  crab : 53 %\n",
      "Accuracy of crocodile : 29 %\n",
      "Accuracy of   cup : 69 %\n",
      "Accuracy of dinosaur : 51 %\n",
      "Accuracy of dolphin : 73 %\n",
      "Accuracy of elephant : 57 %\n",
      "Accuracy of flatfish : 50 %\n",
      "Accuracy of forest : 50 %\n",
      "Accuracy of   fox : 63 %\n",
      "Accuracy of  girl : 32 %\n",
      "Accuracy of hamster : 56 %\n",
      "Accuracy of house : 51 %\n",
      "Accuracy of kangaroo : 49 %\n",
      "Accuracy of keyboard : 74 %\n",
      "Accuracy of  lamp : 49 %\n",
      "Accuracy of lawn_mower : 78 %\n",
      "Accuracy of leopard : 57 %\n",
      "Accuracy of  lion : 68 %\n",
      "Accuracy of lizard : 30 %\n",
      "Accuracy of lobster : 39 %\n",
      "Accuracy of   man : 36 %\n",
      "Accuracy of maple_tree : 71 %\n",
      "Accuracy of motorcycle : 91 %\n",
      "Accuracy of mountain : 83 %\n",
      "Accuracy of mouse :  9 %\n",
      "Accuracy of mushroom : 36 %\n",
      "Accuracy of oak_tree : 49 %\n",
      "Accuracy of orange : 77 %\n",
      "Accuracy of orchid : 56 %\n",
      "Accuracy of otter : 11 %\n",
      "Accuracy of palm_tree : 59 %\n",
      "Accuracy of  pear : 47 %\n",
      "Accuracy of pickup_truck : 47 %\n",
      "Accuracy of pine_tree : 33 %\n",
      "Accuracy of plain : 81 %\n",
      "Accuracy of plate : 43 %\n",
      "Accuracy of poppy : 52 %\n",
      "Accuracy of porcupine : 21 %\n",
      "Accuracy of possum :  6 %\n",
      "Accuracy of rabbit : 13 %\n",
      "Accuracy of raccoon : 28 %\n",
      "Accuracy of   ray : 34 %\n",
      "Accuracy of  road : 83 %\n",
      "Accuracy of rocket : 58 %\n",
      "Accuracy of  rose : 46 %\n",
      "Accuracy of   sea : 48 %\n",
      "Accuracy of  seal : 16 %\n",
      "Accuracy of shark : 28 %\n",
      "Accuracy of shrew : 19 %\n",
      "Accuracy of skunk : 63 %\n",
      "Accuracy of skyscraper : 69 %\n",
      "Accuracy of snail : 16 %\n",
      "Accuracy of snake : 15 %\n",
      "Accuracy of spider : 39 %\n",
      "Accuracy of squirrel : 12 %\n",
      "Accuracy of streetcar : 21 %\n",
      "Accuracy of sunflower : 72 %\n",
      "Accuracy of sweet_pepper : 25 %\n",
      "Accuracy of table : 21 %\n",
      "Accuracy of  tank : 48 %\n",
      "Accuracy of telephone : 31 %\n",
      "Accuracy of television : 48 %\n",
      "Accuracy of tiger : 35 %\n",
      "Accuracy of tractor : 36 %\n",
      "Accuracy of train : 36 %\n",
      "Accuracy of trout : 48 %\n",
      "Accuracy of tulip : 24 %\n",
      "Accuracy of turtle :  9 %\n",
      "Accuracy of wardrobe : 78 %\n",
      "Accuracy of whale : 36 %\n",
      "Accuracy of willow_tree : 30 %\n",
      "Accuracy of  wolf : 22 %\n",
      "Accuracy of woman : 12 %\n",
      "Accuracy of  worm : 42 %\n"
     ]
    }
   ],
   "source": [
    "train_dataset, train_loader, test_loader, num_classes = get_loaders()\n",
    "model = ResNet18(num_classes)\n",
    "model = model.cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=150, gamma=0.1)\n",
    "\n",
    "# Let's build our model\n",
    "train(EPOCHS)\n",
    "print('Finished Training')\n",
    "test() \n",
    "print('----------------------------')\n",
    "print('get the top-1 accuracy for each of the classes.')\n",
    "avg_accu = testEeahClass()\n",
    "\n",
    "print(tau_list[6])\n",
    "n2_norm = torch.norm(model.classifier.weight, p=2.0, dim=1)\n",
    "n2_norm *= n2_norm**tau_list[6]\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(num_classes):\n",
    "        model.classifier.weight[i] = model.classifier.weight[i].div_(n2_norm[i])\n",
    "        model.classifier.bias[i] = model.classifier.bias[i].div_(float(\"inf\"))\n",
    "\n",
    "test()\n",
    "avg_accu = testEeahClass()\n",
    "\n",
    "\n",
    "torch.save({\n",
    "    'model': model.state_dict(),\n",
    "    'optimizer': optimizer.state_dict(),\n",
    "    'epoch': EPOCHS},\n",
    "    osp.join(SAVE_DIR, 'tau-ep{:03d}.pth'.format(EPOCHS))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "258f19cc-156e-4904-9168-f54aad002a4d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-22T22:10:53.803541Z",
     "iopub.status.busy": "2022-06-22T22:10:53.803176Z",
     "iopub.status.idle": "2022-06-22T22:10:53.811373Z",
     "shell.execute_reply": "2022-06-22T22:10:53.810737Z",
     "shell.execute_reply.started": "2022-06-22T22:10:53.803489Z"
    },
    "id": "258f19cc-156e-4904-9168-f54aad002a4d",
    "tags": []
   },
   "outputs": [],
   "source": [
    " \n",
    "\n",
    "DATASET = 'CIFAR100' #['CIFAR10', 'CIFAR100']\n",
    "IMB_TYPE = 'step' #['exp', 'step']\n",
    "IMB_FACTOR = 0.01 #[0.1, 0.01]\n",
    "SAVE_DIR = 'logs/100-step-0.01' \n",
    "\n",
    "LR = 0.1\n",
    "BATCH_SIZE = 128\n",
    "MOMENTUM = 0.9\n",
    "WEIGHT_DECAY = 0.000670\n",
    "EPOCHS = 200\n",
    "\n",
    "os.makedirs(osp.join(SAVE_DIR), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "9934322c-2d1d-4619-a2a2-a69d2355e2a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-22T22:10:53.812712Z",
     "iopub.status.busy": "2022-06-22T22:10:53.812478Z",
     "iopub.status.idle": "2022-06-22T22:45:01.300757Z",
     "shell.execute_reply": "2022-06-22T22:45:01.299254Z",
     "shell.execute_reply.started": "2022-06-22T22:10:53.812685Z"
    },
    "id": "9934322c-2d1d-4619-a2a2-a69d2355e2a7",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "cls num list:\n",
      "[500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]\n",
      "Epoch: [001] \t Loss 3.7605 \t Acc 14.38 \t AccHead 14.53 \t AccTail 0.00\n",
      "Epoch: [002] \t Loss 3.1841 \t Acc 21.39 \t AccHead 21.61 \t AccTail 0.00\n",
      "Epoch: [003] \t Loss 2.9182 \t Acc 24.95 \t AccHead 25.20 \t AccTail 0.00\n",
      "Epoch: [004] \t Loss 2.7444 \t Acc 27.00 \t AccHead 27.27 \t AccTail 0.00\n",
      "Epoch: [005] \t Loss 2.5812 \t Acc 32.54 \t AccHead 32.86 \t AccTail 0.00\n",
      "Epoch: [006] \t Loss 2.4432 \t Acc 33.46 \t AccHead 33.79 \t AccTail 0.00\n",
      "Epoch: [007] \t Loss 2.3258 \t Acc 36.15 \t AccHead 36.51 \t AccTail 0.00\n",
      "Epoch: [008] \t Loss 2.2431 \t Acc 36.93 \t AccHead 37.30 \t AccTail 0.00\n",
      "Epoch: [009] \t Loss 2.1586 \t Acc 41.66 \t AccHead 42.08 \t AccTail 0.00\n",
      "Epoch: [010] \t Loss 2.0982 \t Acc 42.88 \t AccHead 43.31 \t AccTail 0.00\n",
      "Epoch: [011] \t Loss 2.0328 \t Acc 42.04 \t AccHead 42.46 \t AccTail 0.00\n",
      "Epoch: [012] \t Loss 1.9894 \t Acc 45.33 \t AccHead 45.78 \t AccTail 0.00\n",
      "Epoch: [013] \t Loss 1.9421 \t Acc 45.96 \t AccHead 46.42 \t AccTail 0.00\n",
      "Epoch: [014] \t Loss 1.9117 \t Acc 45.84 \t AccHead 46.30 \t AccTail 0.00\n",
      "Epoch: [015] \t Loss 1.8739 \t Acc 47.27 \t AccHead 47.74 \t AccTail 0.00\n",
      "Epoch: [016] \t Loss 1.8464 \t Acc 47.84 \t AccHead 48.32 \t AccTail 0.00\n",
      "Epoch: [017] \t Loss 1.8145 \t Acc 46.57 \t AccHead 47.03 \t AccTail 0.00\n",
      "Epoch: [018] \t Loss 1.7881 \t Acc 52.53 \t AccHead 53.06 \t AccTail 0.00\n",
      "Epoch: [019] \t Loss 1.7662 \t Acc 46.95 \t AccHead 47.41 \t AccTail 0.00\n",
      "Epoch: [020] \t Loss 1.7518 \t Acc 50.73 \t AccHead 51.24 \t AccTail 0.00\n",
      "Epoch: [021] \t Loss 1.7073 \t Acc 49.22 \t AccHead 49.71 \t AccTail 0.00\n",
      "Epoch: [022] \t Loss 1.6989 \t Acc 50.40 \t AccHead 50.91 \t AccTail 0.00\n",
      "Epoch: [023] \t Loss 1.6830 \t Acc 50.98 \t AccHead 51.48 \t AccTail 0.00\n",
      "Epoch: [024] \t Loss 1.6688 \t Acc 53.75 \t AccHead 54.29 \t AccTail 0.00\n",
      "Epoch: [025] \t Loss 1.6581 \t Acc 51.89 \t AccHead 52.41 \t AccTail 0.00\n",
      "Epoch: [026] \t Loss 1.6289 \t Acc 53.33 \t AccHead 53.87 \t AccTail 0.00\n",
      "Epoch: [027] \t Loss 1.6149 \t Acc 53.37 \t AccHead 53.91 \t AccTail 0.00\n",
      "Epoch: [028] \t Loss 1.5959 \t Acc 54.62 \t AccHead 55.16 \t AccTail 0.00\n",
      "Epoch: [029] \t Loss 1.5948 \t Acc 54.21 \t AccHead 54.75 \t AccTail 0.00\n",
      "Epoch: [030] \t Loss 1.5817 \t Acc 56.68 \t AccHead 57.25 \t AccTail 0.00\n",
      "Epoch: [031] \t Loss 1.5560 \t Acc 54.48 \t AccHead 55.02 \t AccTail 0.00\n",
      "Epoch: [032] \t Loss 1.5607 \t Acc 54.72 \t AccHead 55.27 \t AccTail 0.00\n",
      "Epoch: [033] \t Loss 1.5625 \t Acc 57.15 \t AccHead 57.72 \t AccTail 0.00\n",
      "Epoch: [034] \t Loss 1.5361 \t Acc 55.51 \t AccHead 56.06 \t AccTail 0.00\n",
      "Epoch: [035] \t Loss 1.5211 \t Acc 50.64 \t AccHead 51.15 \t AccTail 0.00\n",
      "Epoch: [036] \t Loss 1.5302 \t Acc 58.38 \t AccHead 58.96 \t AccTail 0.00\n",
      "Epoch: [037] \t Loss 1.4981 \t Acc 56.48 \t AccHead 57.04 \t AccTail 0.00\n",
      "Epoch: [038] \t Loss 1.4944 \t Acc 55.44 \t AccHead 56.00 \t AccTail 0.00\n",
      "Epoch: [039] \t Loss 1.5033 \t Acc 56.24 \t AccHead 56.80 \t AccTail 0.00\n",
      "Epoch: [040] \t Loss 1.4857 \t Acc 56.44 \t AccHead 57.01 \t AccTail 0.00\n",
      "Epoch: [041] \t Loss 1.4669 \t Acc 56.99 \t AccHead 57.56 \t AccTail 0.00\n",
      "Epoch: [042] \t Loss 1.4673 \t Acc 56.75 \t AccHead 57.32 \t AccTail 0.00\n",
      "Epoch: [043] \t Loss 1.4636 \t Acc 59.23 \t AccHead 59.82 \t AccTail 0.00\n",
      "Epoch: [044] \t Loss 1.4599 \t Acc 55.89 \t AccHead 56.45 \t AccTail 0.00\n",
      "Epoch: [045] \t Loss 1.4560 \t Acc 57.11 \t AccHead 57.68 \t AccTail 0.00\n",
      "Epoch: [046] \t Loss 1.4432 \t Acc 57.53 \t AccHead 58.11 \t AccTail 0.00\n",
      "Epoch: [047] \t Loss 1.4600 \t Acc 56.00 \t AccHead 56.56 \t AccTail 0.00\n",
      "Epoch: [048] \t Loss 1.4435 \t Acc 56.15 \t AccHead 56.71 \t AccTail 0.00\n",
      "Epoch: [049] \t Loss 1.4257 \t Acc 57.10 \t AccHead 57.67 \t AccTail 0.00\n",
      "Epoch: [050] \t Loss 1.4380 \t Acc 60.30 \t AccHead 60.90 \t AccTail 0.00\n",
      "Epoch: [051] \t Loss 1.4216 \t Acc 60.24 \t AccHead 60.84 \t AccTail 0.00\n",
      "Epoch: [052] \t Loss 1.4146 \t Acc 58.48 \t AccHead 59.07 \t AccTail 0.00\n",
      "Epoch: [053] \t Loss 1.4110 \t Acc 56.29 \t AccHead 56.85 \t AccTail 0.00\n",
      "Epoch: [054] \t Loss 1.4155 \t Acc 56.90 \t AccHead 57.47 \t AccTail 0.00\n",
      "Epoch: [055] \t Loss 1.4070 \t Acc 57.32 \t AccHead 57.89 \t AccTail 0.40\n",
      "Epoch: [056] \t Loss 1.4052 \t Acc 55.80 \t AccHead 56.36 \t AccTail 0.00\n",
      "Epoch: [057] \t Loss 1.4062 \t Acc 59.35 \t AccHead 59.95 \t AccTail 0.00\n",
      "Epoch: [058] \t Loss 1.3999 \t Acc 58.85 \t AccHead 59.44 \t AccTail 0.00\n",
      "Epoch: [059] \t Loss 1.3867 \t Acc 54.45 \t AccHead 55.00 \t AccTail 0.00\n",
      "Epoch: [060] \t Loss 1.3953 \t Acc 57.55 \t AccHead 58.13 \t AccTail 0.00\n",
      "Epoch: [061] \t Loss 1.3660 \t Acc 58.22 \t AccHead 58.80 \t AccTail 0.00\n",
      "Epoch: [062] \t Loss 1.3922 \t Acc 60.51 \t AccHead 61.12 \t AccTail 0.00\n",
      "Epoch: [063] \t Loss 1.3720 \t Acc 59.24 \t AccHead 59.83 \t AccTail 0.00\n",
      "Epoch: [064] \t Loss 1.3779 \t Acc 57.51 \t AccHead 58.07 \t AccTail 0.80\n",
      "Epoch: [065] \t Loss 1.3687 \t Acc 59.47 \t AccHead 60.06 \t AccTail 0.00\n",
      "Epoch: [066] \t Loss 1.3691 \t Acc 60.91 \t AccHead 61.52 \t AccTail 0.00\n",
      "Epoch: [067] \t Loss 1.3654 \t Acc 56.66 \t AccHead 57.23 \t AccTail 0.40\n",
      "Epoch: [068] \t Loss 1.3584 \t Acc 60.39 \t AccHead 60.99 \t AccTail 0.40\n",
      "Epoch: [069] \t Loss 1.3731 \t Acc 62.09 \t AccHead 62.71 \t AccTail 0.40\n",
      "Epoch: [070] \t Loss 1.3649 \t Acc 59.86 \t AccHead 60.45 \t AccTail 1.20\n",
      "Epoch: [071] \t Loss 1.3507 \t Acc 56.86 \t AccHead 57.43 \t AccTail 0.40\n",
      "Epoch: [072] \t Loss 1.3707 \t Acc 59.06 \t AccHead 59.65 \t AccTail 0.00\n",
      "Epoch: [073] \t Loss 1.3463 \t Acc 58.39 \t AccHead 58.98 \t AccTail 0.00\n",
      "Epoch: [074] \t Loss 1.3642 \t Acc 61.54 \t AccHead 62.15 \t AccTail 0.00\n",
      "Epoch: [075] \t Loss 1.3426 \t Acc 58.08 \t AccHead 58.66 \t AccTail 0.00\n",
      "Epoch: [076] \t Loss 1.3519 \t Acc 59.26 \t AccHead 59.85 \t AccTail 0.00\n",
      "Epoch: [077] \t Loss 1.3460 \t Acc 61.04 \t AccHead 61.65 \t AccTail 0.00\n",
      "Epoch: [078] \t Loss 1.3429 \t Acc 60.64 \t AccHead 61.24 \t AccTail 0.80\n",
      "Epoch: [079] \t Loss 1.3470 \t Acc 61.14 \t AccHead 61.75 \t AccTail 0.00\n",
      "Epoch: [080] \t Loss 1.3310 \t Acc 59.55 \t AccHead 60.15 \t AccTail 0.00\n",
      "Epoch: [081] \t Loss 1.3384 \t Acc 56.63 \t AccHead 57.19 \t AccTail 0.00\n",
      "Epoch: [082] \t Loss 1.3527 \t Acc 60.44 \t AccHead 61.04 \t AccTail 0.40\n",
      "Epoch: [083] \t Loss 1.3293 \t Acc 58.52 \t AccHead 59.10 \t AccTail 0.40\n",
      "Epoch: [084] \t Loss 1.3258 \t Acc 60.66 \t AccHead 61.26 \t AccTail 0.00\n",
      "Epoch: [085] \t Loss 1.3369 \t Acc 59.70 \t AccHead 60.29 \t AccTail 0.40\n",
      "Epoch: [086] \t Loss 1.3315 \t Acc 58.97 \t AccHead 59.56 \t AccTail 0.40\n",
      "Epoch: [087] \t Loss 1.3196 \t Acc 59.93 \t AccHead 60.53 \t AccTail 0.00\n",
      "Epoch: [088] \t Loss 1.3324 \t Acc 61.32 \t AccHead 61.94 \t AccTail 0.00\n",
      "Epoch: [089] \t Loss 1.3193 \t Acc 58.19 \t AccHead 58.77 \t AccTail 0.40\n",
      "Epoch: [090] \t Loss 1.3377 \t Acc 62.79 \t AccHead 63.41 \t AccTail 0.40\n",
      "Epoch: [091] \t Loss 1.3255 \t Acc 56.20 \t AccHead 56.77 \t AccTail 0.00\n",
      "Epoch: [092] \t Loss 1.3085 \t Acc 62.10 \t AccHead 62.73 \t AccTail 0.00\n",
      "Epoch: [093] \t Loss 1.3141 \t Acc 60.62 \t AccHead 61.22 \t AccTail 1.20\n",
      "Epoch: [094] \t Loss 1.3169 \t Acc 59.78 \t AccHead 60.37 \t AccTail 0.00\n",
      "Epoch: [095] \t Loss 1.3238 \t Acc 60.64 \t AccHead 61.24 \t AccTail 0.00\n",
      "Epoch: [096] \t Loss 1.3023 \t Acc 60.78 \t AccHead 61.39 \t AccTail 0.00\n",
      "Epoch: [097] \t Loss 1.3130 \t Acc 59.21 \t AccHead 59.80 \t AccTail 0.40\n",
      "Epoch: [098] \t Loss 1.3040 \t Acc 60.22 \t AccHead 60.81 \t AccTail 0.40\n",
      "Epoch: [099] \t Loss 1.3170 \t Acc 61.40 \t AccHead 62.01 \t AccTail 0.40\n",
      "Epoch: [100] \t Loss 1.3007 \t Acc 63.17 \t AccHead 63.79 \t AccTail 0.00\n",
      "Epoch: [101] \t Loss 1.3165 \t Acc 57.98 \t AccHead 58.55 \t AccTail 0.80\n",
      "Epoch: [102] \t Loss 1.3067 \t Acc 62.22 \t AccHead 62.84 \t AccTail 0.00\n",
      "Epoch: [103] \t Loss 1.3224 \t Acc 61.85 \t AccHead 62.46 \t AccTail 0.80\n",
      "Epoch: [104] \t Loss 1.3030 \t Acc 62.59 \t AccHead 63.21 \t AccTail 0.40\n",
      "Epoch: [105] \t Loss 1.3041 \t Acc 60.08 \t AccHead 60.68 \t AccTail 0.00\n",
      "Epoch: [106] \t Loss 1.3052 \t Acc 61.83 \t AccHead 62.44 \t AccTail 0.40\n",
      "Epoch: [107] \t Loss 1.3068 \t Acc 61.22 \t AccHead 61.83 \t AccTail 0.00\n",
      "Epoch: [108] \t Loss 1.3040 \t Acc 59.61 \t AccHead 60.21 \t AccTail 0.40\n",
      "Epoch: [109] \t Loss 1.3033 \t Acc 60.12 \t AccHead 60.72 \t AccTail 0.00\n",
      "Epoch: [110] \t Loss 1.2867 \t Acc 61.90 \t AccHead 62.50 \t AccTail 0.81\n",
      "Epoch: [111] \t Loss 1.3170 \t Acc 58.73 \t AccHead 59.31 \t AccTail 0.00\n",
      "Epoch: [112] \t Loss 1.3034 \t Acc 62.95 \t AccHead 63.58 \t AccTail 0.00\n",
      "Epoch: [113] \t Loss 1.2998 \t Acc 61.31 \t AccHead 61.92 \t AccTail 0.00\n",
      "Epoch: [114] \t Loss 1.3123 \t Acc 61.98 \t AccHead 62.59 \t AccTail 0.80\n",
      "Epoch: [115] \t Loss 1.2870 \t Acc 61.75 \t AccHead 62.37 \t AccTail 0.00\n",
      "Epoch: [116] \t Loss 1.3084 \t Acc 62.29 \t AccHead 62.90 \t AccTail 0.00\n",
      "Epoch: [117] \t Loss 1.3054 \t Acc 60.29 \t AccHead 60.89 \t AccTail 0.00\n",
      "Epoch: [118] \t Loss 1.2889 \t Acc 51.80 \t AccHead 52.32 \t AccTail 0.00\n",
      "Epoch: [119] \t Loss 1.3025 \t Acc 61.09 \t AccHead 61.70 \t AccTail 0.00\n",
      "Epoch: [120] \t Loss 1.2764 \t Acc 59.61 \t AccHead 60.21 \t AccTail 0.00\n",
      "Epoch: [121] \t Loss 1.3179 \t Acc 60.89 \t AccHead 61.48 \t AccTail 1.20\n",
      "Epoch: [122] \t Loss 1.2784 \t Acc 64.79 \t AccHead 65.44 \t AccTail 0.40\n",
      "Epoch: [123] \t Loss 1.2844 \t Acc 62.46 \t AccHead 63.07 \t AccTail 1.60\n",
      "Epoch: [124] \t Loss 1.2868 \t Acc 58.47 \t AccHead 59.06 \t AccTail 0.40\n",
      "Epoch: [125] \t Loss 1.3037 \t Acc 60.85 \t AccHead 61.46 \t AccTail 0.00\n",
      "Epoch: [126] \t Loss 1.2806 \t Acc 59.33 \t AccHead 59.90 \t AccTail 1.61\n",
      "Epoch: [127] \t Loss 1.2972 \t Acc 57.13 \t AccHead 57.70 \t AccTail 0.00\n",
      "Epoch: [128] \t Loss 1.2956 \t Acc 60.29 \t AccHead 60.89 \t AccTail 0.40\n",
      "Epoch: [129] \t Loss 1.2910 \t Acc 62.06 \t AccHead 62.67 \t AccTail 0.80\n",
      "Epoch: [130] \t Loss 1.2832 \t Acc 60.96 \t AccHead 61.56 \t AccTail 0.40\n",
      "Epoch: [131] \t Loss 1.3020 \t Acc 59.14 \t AccHead 59.74 \t AccTail 0.00\n",
      "Epoch: [132] \t Loss 1.2838 \t Acc 62.73 \t AccHead 63.36 \t AccTail 0.00\n",
      "Epoch: [133] \t Loss 1.2793 \t Acc 59.73 \t AccHead 60.33 \t AccTail 0.00\n",
      "Epoch: [134] \t Loss 1.2732 \t Acc 60.60 \t AccHead 61.21 \t AccTail 0.00\n",
      "Epoch: [135] \t Loss 1.2893 \t Acc 58.88 \t AccHead 59.47 \t AccTail 0.40\n",
      "Epoch: [136] \t Loss 1.2694 \t Acc 60.43 \t AccHead 61.03 \t AccTail 0.00\n",
      "Epoch: [137] \t Loss 1.2844 \t Acc 60.87 \t AccHead 61.47 \t AccTail 0.40\n",
      "Epoch: [138] \t Loss 1.2812 \t Acc 61.10 \t AccHead 61.69 \t AccTail 1.60\n",
      "Epoch: [139] \t Loss 1.2795 \t Acc 61.80 \t AccHead 62.42 \t AccTail 0.00\n",
      "Epoch: [140] \t Loss 1.2815 \t Acc 63.94 \t AccHead 64.57 \t AccTail 0.80\n",
      "Epoch: [141] \t Loss 1.2838 \t Acc 62.73 \t AccHead 63.34 \t AccTail 2.00\n",
      "Epoch: [142] \t Loss 1.2699 \t Acc 57.92 \t AccHead 58.50 \t AccTail 0.40\n",
      "Epoch: [143] \t Loss 1.2705 \t Acc 62.96 \t AccHead 63.59 \t AccTail 0.00\n",
      "Epoch: [144] \t Loss 1.2881 \t Acc 60.85 \t AccHead 61.45 \t AccTail 0.00\n",
      "Epoch: [145] \t Loss 1.2683 \t Acc 62.49 \t AccHead 63.12 \t AccTail 0.00\n",
      "Epoch: [146] \t Loss 1.2997 \t Acc 58.66 \t AccHead 59.24 \t AccTail 0.00\n",
      "Epoch: [147] \t Loss 1.2761 \t Acc 61.21 \t AccHead 61.82 \t AccTail 0.00\n",
      "Epoch: [148] \t Loss 1.2788 \t Acc 59.94 \t AccHead 60.53 \t AccTail 0.80\n",
      "Epoch: [149] \t Loss 1.2850 \t Acc 63.85 \t AccHead 64.48 \t AccTail 1.20\n",
      "Epoch: [150] \t Loss 1.2627 \t Acc 58.86 \t AccHead 59.45 \t AccTail 0.00\n",
      "Epoch: [151] \t Loss 0.8700 \t Acc 80.98 \t AccHead 81.78 \t AccTail 1.20\n",
      "Epoch: [152] \t Loss 0.6760 \t Acc 83.73 \t AccHead 84.56 \t AccTail 1.60\n",
      "Epoch: [153] \t Loss 0.5964 \t Acc 86.09 \t AccHead 86.92 \t AccTail 3.20\n",
      "Epoch: [154] \t Loss 0.5455 \t Acc 87.33 \t AccHead 88.16 \t AccTail 4.00\n",
      "Epoch: [155] \t Loss 0.4951 \t Acc 88.27 \t AccHead 89.10 \t AccTail 5.20\n",
      "Epoch: [156] \t Loss 0.4601 \t Acc 89.34 \t AccHead 90.17 \t AccTail 6.00\n",
      "Epoch: [157] \t Loss 0.4282 \t Acc 90.55 \t AccHead 91.39 \t AccTail 6.80\n",
      "Epoch: [158] \t Loss 0.3935 \t Acc 91.52 \t AccHead 92.33 \t AccTail 10.80\n",
      "Epoch: [159] \t Loss 0.3595 \t Acc 92.26 \t AccHead 93.07 \t AccTail 12.00\n",
      "Epoch: [160] \t Loss 0.3400 \t Acc 92.98 \t AccHead 93.82 \t AccTail 9.60\n",
      "Epoch: [161] \t Loss 0.3114 \t Acc 93.44 \t AccHead 94.26 \t AccTail 12.05\n",
      "Epoch: [162] \t Loss 0.3033 \t Acc 93.77 \t AccHead 94.57 \t AccTail 13.25\n",
      "Epoch: [163] \t Loss 0.2748 \t Acc 94.41 \t AccHead 95.21 \t AccTail 14.06\n",
      "Epoch: [164] \t Loss 0.2570 \t Acc 94.69 \t AccHead 95.48 \t AccTail 15.60\n",
      "Epoch: [165] \t Loss 0.2425 \t Acc 95.00 \t AccHead 95.76 \t AccTail 18.80\n",
      "Epoch: [166] \t Loss 0.2309 \t Acc 95.67 \t AccHead 96.40 \t AccTail 22.09\n",
      "Epoch: [167] \t Loss 0.2214 \t Acc 95.81 \t AccHead 96.54 \t AccTail 22.40\n",
      "Epoch: [168] \t Loss 0.2090 \t Acc 95.80 \t AccHead 96.52 \t AccTail 24.80\n",
      "Epoch: [169] \t Loss 0.2009 \t Acc 96.00 \t AccHead 96.68 \t AccTail 28.00\n",
      "Epoch: [170] \t Loss 0.1832 \t Acc 96.43 \t AccHead 97.12 \t AccTail 27.60\n",
      "Epoch: [171] \t Loss 0.1792 \t Acc 96.45 \t AccHead 97.10 \t AccTail 31.20\n",
      "Epoch: [172] \t Loss 0.1641 \t Acc 97.01 \t AccHead 97.64 \t AccTail 34.40\n",
      "Epoch: [173] \t Loss 0.1592 \t Acc 96.92 \t AccHead 97.47 \t AccTail 41.13\n",
      "Epoch: [174] \t Loss 0.1621 \t Acc 97.15 \t AccHead 97.68 \t AccTail 44.18\n",
      "Epoch: [175] \t Loss 0.1501 \t Acc 97.05 \t AccHead 97.58 \t AccTail 44.40\n",
      "Epoch: [176] \t Loss 0.1533 \t Acc 96.98 \t AccHead 97.54 \t AccTail 41.20\n",
      "Epoch: [177] \t Loss 0.1432 \t Acc 96.85 \t AccHead 97.37 \t AccTail 45.20\n",
      "Epoch: [178] \t Loss 0.1446 \t Acc 97.20 \t AccHead 97.73 \t AccTail 43.55\n",
      "Epoch: [179] \t Loss 0.1449 \t Acc 97.47 \t AccHead 97.94 \t AccTail 49.80\n",
      "Epoch: [180] \t Loss 0.1445 \t Acc 97.47 \t AccHead 97.94 \t AccTail 50.00\n",
      "Epoch: [181] \t Loss 0.1430 \t Acc 97.34 \t AccHead 97.78 \t AccTail 53.41\n",
      "Epoch: [182] \t Loss 0.1371 \t Acc 97.80 \t AccHead 98.19 \t AccTail 58.80\n",
      "Epoch: [183] \t Loss 0.1263 \t Acc 97.29 \t AccHead 97.69 \t AccTail 57.43\n",
      "Epoch: [184] \t Loss 0.1326 \t Acc 97.47 \t AccHead 97.86 \t AccTail 58.00\n",
      "Epoch: [185] \t Loss 0.1347 \t Acc 96.91 \t AccHead 97.31 \t AccTail 56.80\n",
      "Epoch: [186] \t Loss 0.1451 \t Acc 96.70 \t AccHead 97.08 \t AccTail 58.40\n",
      "Epoch: [187] \t Loss 0.1364 \t Acc 96.70 \t AccHead 97.03 \t AccTail 63.20\n",
      "Epoch: [188] \t Loss 0.1395 \t Acc 97.58 \t AccHead 97.89 \t AccTail 66.40\n",
      "Epoch: [189] \t Loss 0.1404 \t Acc 96.86 \t AccHead 97.21 \t AccTail 61.85\n",
      "Epoch: [190] \t Loss 0.1318 \t Acc 97.51 \t AccHead 97.83 \t AccTail 65.06\n",
      "Epoch: [191] \t Loss 0.1411 \t Acc 97.18 \t AccHead 97.52 \t AccTail 63.60\n",
      "Epoch: [192] \t Loss 0.1371 \t Acc 97.25 \t AccHead 97.56 \t AccTail 66.27\n",
      "Epoch: [193] \t Loss 0.1418 \t Acc 97.13 \t AccHead 97.40 \t AccTail 70.28\n",
      "Epoch: [194] \t Loss 0.1574 \t Acc 96.67 \t AccHead 96.95 \t AccTail 68.80\n",
      "Epoch: [195] \t Loss 0.1426 \t Acc 97.13 \t AccHead 97.39 \t AccTail 70.85\n",
      "Epoch: [196] \t Loss 0.1393 \t Acc 97.03 \t AccHead 97.30 \t AccTail 70.40\n",
      "Epoch: [197] \t Loss 0.1514 \t Acc 96.52 \t AccHead 96.74 \t AccTail 74.40\n",
      "Epoch: [198] \t Loss 0.1458 \t Acc 96.87 \t AccHead 97.12 \t AccTail 72.29\n",
      "Epoch: [199] \t Loss 0.1524 \t Acc 96.76 \t AccHead 97.00 \t AccTail 72.80\n",
      "Epoch: [200] \t Loss 0.1528 \t Acc 96.39 \t AccHead 96.64 \t AccTail 70.80\n",
      "199\n",
      "Finished Training\n",
      "Acc 32.42 \t AccHead 63.80 \t AccTail 1.04\n",
      "----------------------------\n",
      "get the top-1 accuracy for each of the classes.\n",
      "Accuracy of apple : 87 %\n",
      "Accuracy of aquarium_fish : 84 %\n",
      "Accuracy of  baby : 47 %\n",
      "Accuracy of  bear : 35 %\n",
      "Accuracy of beaver : 65 %\n",
      "Accuracy of   bed : 64 %\n",
      "Accuracy of   bee : 69 %\n",
      "Accuracy of beetle : 64 %\n",
      "Accuracy of bicycle : 67 %\n",
      "Accuracy of bottle : 68 %\n",
      "Accuracy of  bowl : 53 %\n",
      "Accuracy of   boy : 52 %\n",
      "Accuracy of bridge : 80 %\n",
      "Accuracy of   bus : 76 %\n",
      "Accuracy of butterfly : 63 %\n",
      "Accuracy of camel : 55 %\n",
      "Accuracy of   can : 63 %\n",
      "Accuracy of castle : 72 %\n",
      "Accuracy of caterpillar : 64 %\n",
      "Accuracy of cattle : 52 %\n",
      "Accuracy of chair : 80 %\n",
      "Accuracy of chimpanzee : 77 %\n",
      "Accuracy of clock : 52 %\n",
      "Accuracy of cloud : 77 %\n",
      "Accuracy of cockroach : 79 %\n",
      "Accuracy of couch : 48 %\n",
      "Accuracy of  crab : 63 %\n",
      "Accuracy of crocodile : 51 %\n",
      "Accuracy of   cup : 68 %\n",
      "Accuracy of dinosaur : 58 %\n",
      "Accuracy of dolphin : 82 %\n",
      "Accuracy of elephant : 47 %\n",
      "Accuracy of flatfish : 57 %\n",
      "Accuracy of forest : 60 %\n",
      "Accuracy of   fox : 74 %\n",
      "Accuracy of  girl : 32 %\n",
      "Accuracy of hamster : 62 %\n",
      "Accuracy of house : 68 %\n",
      "Accuracy of kangaroo : 47 %\n",
      "Accuracy of keyboard : 70 %\n",
      "Accuracy of  lamp : 60 %\n",
      "Accuracy of lawn_mower : 79 %\n",
      "Accuracy of leopard : 72 %\n",
      "Accuracy of  lion : 71 %\n",
      "Accuracy of lizard : 34 %\n",
      "Accuracy of lobster : 54 %\n",
      "Accuracy of   man : 30 %\n",
      "Accuracy of maple_tree : 86 %\n",
      "Accuracy of motorcycle : 88 %\n",
      "Accuracy of mountain : 84 %\n",
      "Accuracy of mouse :  0 %\n",
      "Accuracy of mushroom :  0 %\n",
      "Accuracy of oak_tree :  1 %\n",
      "Accuracy of orange :  8 %\n",
      "Accuracy of orchid :  0 %\n",
      "Accuracy of otter :  0 %\n",
      "Accuracy of palm_tree :  0 %\n",
      "Accuracy of  pear :  4 %\n",
      "Accuracy of pickup_truck :  0 %\n",
      "Accuracy of pine_tree :  1 %\n",
      "Accuracy of plain :  1 %\n",
      "Accuracy of plate :  0 %\n",
      "Accuracy of poppy :  3 %\n",
      "Accuracy of porcupine :  0 %\n",
      "Accuracy of possum :  0 %\n",
      "Accuracy of rabbit :  0 %\n",
      "Accuracy of raccoon :  0 %\n",
      "Accuracy of   ray :  2 %\n",
      "Accuracy of  road :  2 %\n",
      "Accuracy of rocket :  1 %\n",
      "Accuracy of  rose :  6 %\n",
      "Accuracy of   sea :  2 %\n",
      "Accuracy of  seal :  0 %\n",
      "Accuracy of shark :  0 %\n",
      "Accuracy of shrew :  0 %\n",
      "Accuracy of skunk :  1 %\n",
      "Accuracy of skyscraper :  1 %\n",
      "Accuracy of snail :  0 %\n",
      "Accuracy of snake :  0 %\n",
      "Accuracy of spider :  0 %\n",
      "Accuracy of squirrel :  0 %\n",
      "Accuracy of streetcar :  0 %\n",
      "Accuracy of sunflower :  9 %\n",
      "Accuracy of sweet_pepper :  0 %\n",
      "Accuracy of table :  0 %\n",
      "Accuracy of  tank :  0 %\n",
      "Accuracy of telephone :  0 %\n",
      "Accuracy of television :  0 %\n",
      "Accuracy of tiger :  1 %\n",
      "Accuracy of tractor :  1 %\n",
      "Accuracy of train :  0 %\n",
      "Accuracy of trout :  0 %\n",
      "Accuracy of tulip :  0 %\n",
      "Accuracy of turtle :  0 %\n",
      "Accuracy of wardrobe :  2 %\n",
      "Accuracy of whale :  3 %\n",
      "Accuracy of willow_tree :  2 %\n",
      "Accuracy of  wolf :  0 %\n",
      "Accuracy of woman :  0 %\n",
      "Accuracy of  worm :  1 %\n",
      "0.40980000000000005\n",
      "Acc 28.45 \t AccHead 33.68 \t AccTail 23.22\n",
      "Accuracy of apple : 73 %\n",
      "Accuracy of aquarium_fish : 50 %\n",
      "Accuracy of  baby :  8 %\n",
      "Accuracy of  bear :  4 %\n",
      "Accuracy of beaver : 11 %\n",
      "Accuracy of   bed : 50 %\n",
      "Accuracy of   bee : 46 %\n",
      "Accuracy of beetle : 21 %\n",
      "Accuracy of bicycle : 64 %\n",
      "Accuracy of bottle : 52 %\n",
      "Accuracy of  bowl : 17 %\n",
      "Accuracy of   boy : 13 %\n",
      "Accuracy of bridge : 26 %\n",
      "Accuracy of   bus :  2 %\n",
      "Accuracy of butterfly : 40 %\n",
      "Accuracy of camel : 23 %\n",
      "Accuracy of   can : 36 %\n",
      "Accuracy of castle : 35 %\n",
      "Accuracy of caterpillar : 36 %\n",
      "Accuracy of cattle : 26 %\n",
      "Accuracy of chair : 81 %\n",
      "Accuracy of chimpanzee : 43 %\n",
      "Accuracy of clock :  9 %\n",
      "Accuracy of cloud : 54 %\n",
      "Accuracy of cockroach : 63 %\n",
      "Accuracy of couch : 27 %\n",
      "Accuracy of  crab : 45 %\n",
      "Accuracy of crocodile :  9 %\n",
      "Accuracy of   cup : 58 %\n",
      "Accuracy of dinosaur : 34 %\n",
      "Accuracy of dolphin : 30 %\n",
      "Accuracy of elephant : 26 %\n",
      "Accuracy of flatfish : 23 %\n",
      "Accuracy of forest : 30 %\n",
      "Accuracy of   fox : 37 %\n",
      "Accuracy of  girl :  1 %\n",
      "Accuracy of hamster : 40 %\n",
      "Accuracy of house : 17 %\n",
      "Accuracy of kangaroo :  7 %\n",
      "Accuracy of keyboard : 47 %\n",
      "Accuracy of  lamp : 30 %\n",
      "Accuracy of lawn_mower : 62 %\n",
      "Accuracy of leopard : 20 %\n",
      "Accuracy of  lion : 42 %\n",
      "Accuracy of lizard : 14 %\n",
      "Accuracy of lobster : 39 %\n",
      "Accuracy of   man : 10 %\n",
      "Accuracy of maple_tree : 42 %\n",
      "Accuracy of motorcycle : 68 %\n",
      "Accuracy of mountain : 43 %\n",
      "Accuracy of mouse : 26 %\n",
      "Accuracy of mushroom : 22 %\n",
      "Accuracy of oak_tree : 53 %\n",
      "Accuracy of orange : 19 %\n",
      "Accuracy of orchid : 23 %\n",
      "Accuracy of otter : 10 %\n",
      "Accuracy of palm_tree : 20 %\n",
      "Accuracy of  pear : 27 %\n",
      "Accuracy of pickup_truck : 10 %\n",
      "Accuracy of pine_tree : 17 %\n",
      "Accuracy of plain : 14 %\n",
      "Accuracy of plate : 37 %\n",
      "Accuracy of poppy : 14 %\n",
      "Accuracy of porcupine : 16 %\n",
      "Accuracy of possum :  2 %\n",
      "Accuracy of rabbit : 14 %\n",
      "Accuracy of raccoon : 12 %\n",
      "Accuracy of   ray :  5 %\n",
      "Accuracy of  road : 54 %\n",
      "Accuracy of rocket : 33 %\n",
      "Accuracy of  rose : 16 %\n",
      "Accuracy of   sea : 62 %\n",
      "Accuracy of  seal : 13 %\n",
      "Accuracy of shark : 18 %\n",
      "Accuracy of shrew :  4 %\n",
      "Accuracy of skunk : 28 %\n",
      "Accuracy of skyscraper : 16 %\n",
      "Accuracy of snail : 15 %\n",
      "Accuracy of snake :  6 %\n",
      "Accuracy of spider : 31 %\n",
      "Accuracy of squirrel : 11 %\n",
      "Accuracy of streetcar : 42 %\n",
      "Accuracy of sunflower : 33 %\n",
      "Accuracy of sweet_pepper : 10 %\n",
      "Accuracy of table : 12 %\n",
      "Accuracy of  tank : 63 %\n",
      "Accuracy of telephone :  7 %\n",
      "Accuracy of television : 46 %\n",
      "Accuracy of tiger : 34 %\n",
      "Accuracy of tractor : 11 %\n",
      "Accuracy of train : 19 %\n",
      "Accuracy of trout : 31 %\n",
      "Accuracy of tulip : 15 %\n",
      "Accuracy of turtle : 10 %\n",
      "Accuracy of wardrobe : 32 %\n",
      "Accuracy of whale : 44 %\n",
      "Accuracy of willow_tree : 15 %\n",
      "Accuracy of  wolf : 26 %\n",
      "Accuracy of woman : 57 %\n",
      "Accuracy of  worm :  6 %\n"
     ]
    }
   ],
   "source": [
    "train_dataset, train_loader, test_loader, num_classes = get_loaders()\n",
    "model = ResNet18(num_classes)\n",
    "model = model.cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=150, gamma=0.1)\n",
    "\n",
    "# Let's build our model\n",
    "train(EPOCHS)\n",
    "print('Finished Training')\n",
    "test() \n",
    "print('----------------------------')\n",
    "print('get the top-1 accuracy for each of the classes.')\n",
    "avg_accu = testEeahClass()\n",
    "\n",
    "print(tau_list[7])\n",
    "n2_norm = torch.norm(model.classifier.weight, p=2.0, dim=1)\n",
    "n2_norm *= n2_norm**tau_list[7]\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(num_classes):\n",
    "        model.classifier.weight[i] = model.classifier.weight[i].div_(n2_norm[i])\n",
    "        model.classifier.bias[i] = model.classifier.bias[i].div_(float(\"inf\"))\n",
    "\n",
    "test()\n",
    "avg_accu = testEeahClass()\n",
    "\n",
    "\n",
    "torch.save({\n",
    "    'model': model.state_dict(),\n",
    "    'optimizer': optimizer.state_dict(),\n",
    "    'epoch': EPOCHS},\n",
    "    osp.join(SAVE_DIR, 'tau-ep{:03d}.pth'.format(EPOCHS))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2641deac-f588-4070-8ae2-46566966d076",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DLLAB_project (2).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "torch1.9.0-py3.8-cuda11.1",
   "language": "python",
   "name": "torch1.9.0-py3.8-cuda11.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
